{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10de0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284d3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_436946/3252503702.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_fof = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.csv', skiprows=28)\n",
      "/tmp/ipykernel_436946/3252503702.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_subhalo = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.csv', skiprows=37)\n"
     ]
    }
   ],
   "source": [
    "data_fof = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.csv', skiprows=28)\n",
    "data_subhalo = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.csv', skiprows=37)  \n",
    "data_subhalo['GalaxyID'] = data_subhalo['GalaxyID'].astype(str)\n",
    "data_fof['GroupID'] = data_subhalo['GalaxyID'].astype(str)\n",
    "\n",
    "data_subhalo.to_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.feather')\n",
    "data_fof.to_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fof = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.feather')  \n",
    "data_fof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subhalo = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.feather')  \n",
    "data_subhalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c752d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_fof \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data_subhalo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m data_fof\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "same_features = set(data_fof.columns).intersection(set(data_subhalo.columns))\n",
    "same_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fof_unique = data_fof.drop(columns=same_features)\n",
    "fof_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2da83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subhalo_unique = data_subhalo.drop(columns=same_features)\n",
    "subhalo_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3db738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1: Index(['GroupID', 'SnapNum', 'Redshift', 'RandomNumber', 'GroupMass',\n",
      "       'GroupCentreOfPotential_x', 'GroupCentreOfPotential_y',\n",
      "       'GroupCentreOfPotential_z', 'NumOfSubhalos', 'Group_M_Crit200',\n",
      "       'Group_R_Crit200', 'Group_M_Mean200', 'Group_R_Mean200',\n",
      "       'Group_M_TopHat200', 'Group_R_TopHat200', 'Group_M_Crit500',\n",
      "       'Group_R_Crit500', 'Group_M_Mean500', 'Group_R_Mean500',\n",
      "       'Group_M_Crit2500', 'Group_R_Crit2500', 'Group_R_Mean2500',\n",
      "       'Group_M_Mean2500', 'GalaxyID', 'DescendantID', 'LastProgID',\n",
      "       'TopLeafID', 'GroupNumber', 'SubGroupNumber', 'CentreOfMass_x',\n",
      "       'CentreOfMass_y', 'CentreOfMass_z', 'CentreOfPotential_x',\n",
      "       'CentreOfPotential_y', 'CentreOfPotential_z', 'Velocity_x',\n",
      "       'Velocity_y', 'Velocity_z', 'KineticEnergy', 'MechanicalEnergy',\n",
      "       'TotalEnergy', 'Vmax', 'VmaxRadius', 'Mass', 'MassType_DM',\n",
      "       'HalfMassProjRad_DM', 'HalfMassRad_DM', 'nodeIndex', 'PosInFile',\n",
      "       'FileNum', 'Image_ID'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1236622 entries, 0 to 1236621\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Non-Null Count    Dtype  \n",
      "---  ------                    --------------    -----  \n",
      " 0   GroupID                   1084535 non-null  float64\n",
      " 1   SnapNum                   1084535 non-null  float64\n",
      " 2   Redshift                  1084535 non-null  float64\n",
      " 3   RandomNumber              1084535 non-null  float64\n",
      " 4   GroupMass                 1084535 non-null  float64\n",
      " 5   GroupCentreOfPotential_x  1084535 non-null  float64\n",
      " 6   GroupCentreOfPotential_y  1084535 non-null  float64\n",
      " 7   GroupCentreOfPotential_z  1084535 non-null  float64\n",
      " 8   NumOfSubhalos             1084535 non-null  float64\n",
      " 9   Group_M_Crit200           1084535 non-null  float64\n",
      " 10  Group_R_Crit200           1084535 non-null  float64\n",
      " 11  Group_M_Mean200           1084535 non-null  float64\n",
      " 12  Group_R_Mean200           1084535 non-null  float64\n",
      " 13  Group_M_TopHat200         1084535 non-null  float64\n",
      " 14  Group_R_TopHat200         1084535 non-null  float64\n",
      " 15  Group_M_Crit500           1084535 non-null  float64\n",
      " 16  Group_R_Crit500           1084535 non-null  float64\n",
      " 17  Group_M_Mean500           1084535 non-null  float64\n",
      " 18  Group_R_Mean500           1084535 non-null  float64\n",
      " 19  Group_M_Crit2500          1084535 non-null  float64\n",
      " 20  Group_R_Crit2500          1084535 non-null  float64\n",
      " 21  Group_R_Mean2500          1084535 non-null  float64\n",
      " 22  Group_M_Mean2500          1084535 non-null  float64\n",
      " 23  GalaxyID                  1236622 non-null  int64  \n",
      " 24  DescendantID              1236622 non-null  int64  \n",
      " 25  LastProgID                1236622 non-null  int64  \n",
      " 26  TopLeafID                 1236622 non-null  int64  \n",
      " 27  GroupNumber               1236622 non-null  int64  \n",
      " 28  SubGroupNumber            1236622 non-null  int64  \n",
      " 29  CentreOfMass_x            1236622 non-null  float64\n",
      " 30  CentreOfMass_y            1236622 non-null  float64\n",
      " 31  CentreOfMass_z            1236622 non-null  float64\n",
      " 32  CentreOfPotential_x       1236622 non-null  float64\n",
      " 33  CentreOfPotential_y       1236622 non-null  float64\n",
      " 34  CentreOfPotential_z       1236622 non-null  float64\n",
      " 35  Velocity_x                1236622 non-null  float64\n",
      " 36  Velocity_y                1236622 non-null  float64\n",
      " 37  Velocity_z                1236622 non-null  float64\n",
      " 38  KineticEnergy             1236622 non-null  float64\n",
      " 39  MechanicalEnergy          1236622 non-null  float64\n",
      " 40  TotalEnergy               1236622 non-null  float64\n",
      " 41  Vmax                      1236622 non-null  float64\n",
      " 42  VmaxRadius                1236622 non-null  float64\n",
      " 43  Mass                      1236622 non-null  float64\n",
      " 44  MassType_DM               1236622 non-null  float64\n",
      " 45  HalfMassProjRad_DM        1236622 non-null  float64\n",
      " 46  HalfMassRad_DM            1236622 non-null  float64\n",
      " 47  nodeIndex                 1236622 non-null  int64  \n",
      " 48  PosInFile                 1236622 non-null  int64  \n",
      " 49  FileNum                   1236622 non-null  int64  \n",
      " 50  Image_ID                  1236622 non-null  int64  \n",
      "dtypes: float64(41), int64(10)\n",
      "memory usage: 481.2 MB\n",
      "None\n",
      "\n",
      "Data 2: Index(['GroupID', 'SnapNum', 'Redshift', 'RandomNumber', 'GroupMass',\n",
      "       'GroupCentreOfPotential_x', 'GroupCentreOfPotential_y',\n",
      "       'GroupCentreOfPotential_z', 'NumOfSubhalos', 'Group_M_Crit200',\n",
      "       'Group_R_Crit200', 'Group_M_Mean200', 'Group_R_Mean200',\n",
      "       'Group_M_TopHat200', 'Group_R_TopHat200', 'Group_M_Crit500',\n",
      "       'Group_R_Crit500', 'Group_M_Mean500', 'Group_R_Mean500',\n",
      "       'Group_M_Crit2500', 'Group_R_Crit2500', 'Group_R_Mean2500',\n",
      "       'Group_M_Mean2500', 'GalaxyID', 'DescendantID', 'LastProgID',\n",
      "       'TopLeafID', 'GroupNumber', 'SubGroupNumber', 'CentreOfMass_x',\n",
      "       'CentreOfMass_y', 'CentreOfMass_z', 'CentreOfPotential_x',\n",
      "       'CentreOfPotential_y', 'CentreOfPotential_z', 'Velocity_x',\n",
      "       'Velocity_y', 'Velocity_z', 'KineticEnergy', 'MechanicalEnergy',\n",
      "       'TotalEnergy', 'Vmax', 'VmaxRadius', 'Mass', 'MassType_DM',\n",
      "       'HalfMassProjRad_DM', 'HalfMassRad_DM', 'nodeIndex', 'PosInFile',\n",
      "       'FileNum', 'Image_ID'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8058645 entries, 0 to 8058644\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   GroupID                   int64  \n",
      " 1   SnapNum                   int64  \n",
      " 2   Redshift                  float64\n",
      " 3   RandomNumber              float64\n",
      " 4   GroupMass                 float64\n",
      " 5   GroupCentreOfPotential_x  float64\n",
      " 6   GroupCentreOfPotential_y  float64\n",
      " 7   GroupCentreOfPotential_z  float64\n",
      " 8   NumOfSubhalos             int64  \n",
      " 9   Group_M_Crit200           float64\n",
      " 10  Group_R_Crit200           float64\n",
      " 11  Group_M_Mean200           float64\n",
      " 12  Group_R_Mean200           float64\n",
      " 13  Group_M_TopHat200         float64\n",
      " 14  Group_R_TopHat200         float64\n",
      " 15  Group_M_Crit500           float64\n",
      " 16  Group_R_Crit500           float64\n",
      " 17  Group_M_Mean500           float64\n",
      " 18  Group_R_Mean500           float64\n",
      " 19  Group_M_Crit2500          float64\n",
      " 20  Group_R_Crit2500          float64\n",
      " 21  Group_R_Mean2500          float64\n",
      " 22  Group_M_Mean2500          float64\n",
      " 23  GalaxyID                  float64\n",
      " 24  DescendantID              float64\n",
      " 25  LastProgID                float64\n",
      " 26  TopLeafID                 float64\n",
      " 27  GroupNumber               float64\n",
      " 28  SubGroupNumber            float64\n",
      " 29  CentreOfMass_x            float64\n",
      " 30  CentreOfMass_y            float64\n",
      " 31  CentreOfMass_z            float64\n",
      " 32  CentreOfPotential_x       float64\n",
      " 33  CentreOfPotential_y       float64\n",
      " 34  CentreOfPotential_z       float64\n",
      " 35  Velocity_x                float64\n",
      " 36  Velocity_y                float64\n",
      " 37  Velocity_z                float64\n",
      " 38  KineticEnergy             float64\n",
      " 39  MechanicalEnergy          float64\n",
      " 40  TotalEnergy               float64\n",
      " 41  Vmax                      float64\n",
      " 42  VmaxRadius                float64\n",
      " 43  Mass                      float64\n",
      " 44  MassType_DM               float64\n",
      " 45  HalfMassProjRad_DM        float64\n",
      " 46  HalfMassRad_DM            float64\n",
      " 47  nodeIndex                 float64\n",
      " 48  PosInFile                 float64\n",
      " 49  FileNum                   float64\n",
      " 50  Image_ID                  float64\n",
      "dtypes: float64(48), int64(3)\n",
      "memory usage: 3.1 GB\n",
      "None\n",
      "\n",
      "Data 3: Index(['GroupID', 'SnapNum', 'Redshift', 'RandomNumber', 'GroupMass',\n",
      "       'GroupCentreOfPotential_x', 'GroupCentreOfPotential_y',\n",
      "       'GroupCentreOfPotential_z', 'NumOfSubhalos', 'Group_M_Crit200',\n",
      "       'Group_R_Crit200', 'Group_M_Mean200', 'Group_R_Mean200',\n",
      "       'Group_M_TopHat200', 'Group_R_TopHat200', 'Group_M_Crit500',\n",
      "       'Group_R_Crit500', 'Group_M_Mean500', 'Group_R_Mean500',\n",
      "       'Group_M_Crit2500', 'Group_R_Crit2500', 'Group_R_Mean2500',\n",
      "       'Group_M_Mean2500', 'GalaxyID', 'DescendantID', 'LastProgID',\n",
      "       'TopLeafID', 'GroupNumber', 'SubGroupNumber', 'CentreOfMass_x',\n",
      "       'CentreOfMass_y', 'CentreOfMass_z', 'CentreOfPotential_x',\n",
      "       'CentreOfPotential_y', 'CentreOfPotential_z', 'Velocity_x',\n",
      "       'Velocity_y', 'Velocity_z', 'KineticEnergy', 'MechanicalEnergy',\n",
      "       'TotalEnergy', 'Vmax', 'VmaxRadius', 'Mass', 'MassType_DM',\n",
      "       'HalfMassProjRad_DM', 'HalfMassRad_DM', 'nodeIndex', 'PosInFile',\n",
      "       'FileNum', 'Image_ID'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22109109 entries, 0 to 22109108\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   GroupID                   int64  \n",
      " 1   SnapNum                   float64\n",
      " 2   Redshift                  float64\n",
      " 3   RandomNumber              float64\n",
      " 4   GroupMass                 float64\n",
      " 5   GroupCentreOfPotential_x  float64\n",
      " 6   GroupCentreOfPotential_y  float64\n",
      " 7   GroupCentreOfPotential_z  float64\n",
      " 8   NumOfSubhalos             float64\n",
      " 9   Group_M_Crit200           float64\n",
      " 10  Group_R_Crit200           float64\n",
      " 11  Group_M_Mean200           float64\n",
      " 12  Group_R_Mean200           float64\n",
      " 13  Group_M_TopHat200         float64\n",
      " 14  Group_R_TopHat200         float64\n",
      " 15  Group_M_Crit500           float64\n",
      " 16  Group_R_Crit500           float64\n",
      " 17  Group_M_Mean500           float64\n",
      " 18  Group_R_Mean500           float64\n",
      " 19  Group_M_Crit2500          float64\n",
      " 20  Group_R_Crit2500          float64\n",
      " 21  Group_R_Mean2500          float64\n",
      " 22  Group_M_Mean2500          float64\n",
      " 23  GalaxyID                  int64  \n",
      " 24  DescendantID              float64\n",
      " 25  LastProgID                float64\n",
      " 26  TopLeafID                 float64\n",
      " 27  GroupNumber               float64\n",
      " 28  SubGroupNumber            float64\n",
      " 29  CentreOfMass_x            float64\n",
      " 30  CentreOfMass_y            float64\n",
      " 31  CentreOfMass_z            float64\n",
      " 32  CentreOfPotential_x       float64\n",
      " 33  CentreOfPotential_y       float64\n",
      " 34  CentreOfPotential_z       float64\n",
      " 35  Velocity_x                float64\n",
      " 36  Velocity_y                float64\n",
      " 37  Velocity_z                float64\n",
      " 38  KineticEnergy             float64\n",
      " 39  MechanicalEnergy          float64\n",
      " 40  TotalEnergy               float64\n",
      " 41  Vmax                      float64\n",
      " 42  VmaxRadius                float64\n",
      " 43  Mass                      float64\n",
      " 44  MassType_DM               float64\n",
      " 45  HalfMassProjRad_DM        float64\n",
      " 46  HalfMassRad_DM            float64\n",
      " 47  nodeIndex                 float64\n",
      " 48  PosInFile                 float64\n",
      " 49  FileNum                   float64\n",
      " 50  Image_ID                  float64\n",
      "dtypes: float64(49), int64(2)\n",
      "memory usage: 8.4 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_1 = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/merge_L0025N0376.feather')\n",
    "data_2 = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/merge_L0025N0752.feather')  \n",
    "data_3 = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/merge_L0100N1504.feather')\n",
    "\n",
    "print(\"Data 1:\", data_1.columns)\n",
    "print(data_1.info())\n",
    "print(\"\\nData 2:\", data_2.columns)\n",
    "print(data_2.info())\n",
    "print(\"\\nData 3:\", data_3.columns)\n",
    "print(data_3.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns with all NaNs:\", [col for col in numeric_cols if self.data[col].isna().all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4121e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimized PyTorch Training Code\n",
    "- Improved data loading\n",
    "- Enhanced model architecture\n",
    "- Optimized training loop\n",
    "\"\"\"\n",
    "\n",
    "################ MODEL OPTIMIZATION ################\n",
    "\n",
    "class DMHaloMatterRNN(tcn.Module):\n",
    "    def __init__(self, \n",
    "                input_size=None, \n",
    "                hidden_size=None, \n",
    "                output_size=None, \n",
    "                num_layers=2, \n",
    "                dropout_rate=0.2,\n",
    "                activation='gelu', \n",
    "                bidirectional=False, \n",
    "                feature_dim=None,\n",
    "                use_layer_norm=False, \n",
    "                return_hidden=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size if input_size is not None else 64\n",
    "        self.hidden_size = hidden_size if hidden_size is not None else 128\n",
    "        self.output_size = output_size if output_size is not None else 1\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.feature_dim = feature_dim or min(self.hidden_size, self.input_size)\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.return_hidden = return_hidden\n",
    "        \n",
    "        # Get activation function\n",
    "        self.activation = self.__get_activation_layer(activation)\n",
    "        \n",
    "        # Pre-compute bidirectional factor\n",
    "        self.output_factor = 2 if self.bidirectional else 1\n",
    "        \n",
    "        # Streamlined feature extraction\n",
    "        self.feature_layer = tcn.Linear(self.input_size, self.feature_dim)\n",
    "        \n",
    "        # Optional batch normalization - don't create if not needed\n",
    "        self.batch_norm = tcn.BatchNorm1d(self.feature_dim)\n",
    "        \n",
    "        # Simplified output projection\n",
    "        intermediate_size = max(4, self.hidden_size // 2)\n",
    "        self.output_proj1 = tcn.Linear(self.hidden_size * self.output_factor, intermediate_size)\n",
    "        self.output_proj2 = tcn.Linear(intermediate_size, self.output_size)\n",
    "        \n",
    "        # Optional layer normalization\n",
    "        if self.use_layer_norm:\n",
    "            self.norm_after_rnn = tcn.LayerNorm(self.hidden_size * self.output_factor)\n",
    "    \n",
    "    def __get_activation_layer(self, activation):\n",
    "        activation_name = activation.lower()\n",
    "        \n",
    "        # Simplified activation map with most common activations\n",
    "        activation_map = {\n",
    "            'relu': tcn.ReLU(inplace=True),\n",
    "            'leaky_relu': tcn.LeakyReLU(inplace=True),\n",
    "            'gelu': tcn.GELU(),\n",
    "            'swish': tcn.SiLU(inplace=True),\n",
    "            'silu': tcn.SiLU(inplace=True),\n",
    "        }\n",
    "        \n",
    "        return activation_map.get(activation_name, tcn.GELU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass (not implemented in base class).\"\"\"\n",
    "        raise NotImplementedError(\"Forward is Not Implemented\")\n",
    "    \n",
    "    @tc.jit.ignore\n",
    "    def process_input(self, x):\n",
    "        \"\"\"Optimized input processing\"\"\"\n",
    "        # Handle 2D (single step) vs 3D (sequence) input\n",
    "        is_sequence = x.dim() == 3\n",
    "        \n",
    "        if is_sequence:\n",
    "            # Batch processing for sequences\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x = x.reshape(-1, self.input_size)\n",
    "            x = self.feature_layer(x)\n",
    "            x = self.activation(x)\n",
    "            \n",
    "            # Apply batch norm efficiently\n",
    "            x = self.batch_norm(x)\n",
    "            \n",
    "            # Return to sequence form\n",
    "            return x.reshape(batch_size, seq_len, self.feature_dim)\n",
    "        else:\n",
    "            # Process single step\n",
    "            x = self.feature_layer(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.batch_norm(x)\n",
    "            return x.unsqueeze(1)  # Add sequence dimension\n",
    "    \n",
    "    def apply_weight_init(self):\n",
    "        \"\"\"Optimized weight initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, tcn.Linear):\n",
    "                tcn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    tcn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, tcn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        tcn.init.xavier_uniform_(param)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        tcn.init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        tcn.init.zeros_(param)\n",
    "\n",
    "\n",
    "class GRUDMHaloMapper(DMHaloMatterRNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create GRU layer\n",
    "        self.gru = tcn.GRU(\n",
    "            self.feature_dim,\n",
    "            self.hidden_size,\n",
    "            self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_rate if self.num_layers > 1 else 0,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # Apply optimized weight initialization\n",
    "        self.apply_weight_init()\n",
    "        \n",
    "        # Enable cudnn benchmarking for optimal kernels\n",
    "        if tc.cuda.is_available():\n",
    "            tc.backends.cudnn.benchmark = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input\n",
    "        x = self.process_input(x)\n",
    "        \n",
    "        # Run through GRU\n",
    "        features, h_n = self.gru(x)\n",
    "        \n",
    "        # Get only the last output efficiently\n",
    "        features = features[:, -1]\n",
    "        \n",
    "        # Apply layer normalization if needed\n",
    "        if self.use_layer_norm:\n",
    "            features = self.norm_after_rnn(features)\n",
    "        \n",
    "        # Apply functional dropout during training\n",
    "        if self.training:\n",
    "            features = F.dropout(features, p=self.dropout_rate, training=True, inplace=True)\n",
    "        \n",
    "        # Simplified output projection\n",
    "        features = self.output_proj1(features)\n",
    "        features = self.activation(features)\n",
    "        \n",
    "        # Final output projection\n",
    "        output = self.output_proj2(features)\n",
    "        \n",
    "        return (output, h_n) if self.return_hidden else output\n",
    "\n",
    "\n",
    "################ TRAINER OPTIMIZATION ################\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model=None, criterion=None, optimizer=None, patience=10, device=None, save_dir=None):\n",
    "        # Model setup\n",
    "        self.model = model if model is not None else RNNDMHaloMapper()\n",
    "        self.device = device if device else ('cuda' if tc.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Criterion and optimizer with better defaults\n",
    "        self.criterion = criterion or tc.nn.MSELoss()\n",
    "        self.optimizer = optimizer or tc.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=0.001, \n",
    "            weight_decay=0.0001,\n",
    "            eps=1e-8\n",
    "        )\n",
    "\n",
    "        # Save and tracking\n",
    "        self.save_dir = save_dir\n",
    "        self.metrics_calculator = CustomMetrics()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_metrics': [],\n",
    "            'val_metrics': []\n",
    "        }\n",
    "\n",
    "        # Configure for performance\n",
    "        self.__configure_for_performance()\n",
    "\n",
    "    def __configure_for_performance(self):\n",
    "        \"\"\"Configure settings for optimal performance\"\"\"\n",
    "        # Data loaders\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "\n",
    "        # Mixed precision settings\n",
    "        self.use_amp = self.device == 'cuda'\n",
    "        if self.use_amp:\n",
    "            self.scaler = GradScaler()\n",
    "        \n",
    "        # Optimize settings for device\n",
    "        if self.device == 'cuda':\n",
    "            # Set optimal CUDA settings\n",
    "            tc.backends.cudnn.benchmark = True\n",
    "            tc.backends.cudnn.deterministic = False\n",
    "            # Increase batch size for GPU\n",
    "            self.log_interval = 25\n",
    "            self.accumulation_steps = 1\n",
    "        else:\n",
    "            # CPU settings\n",
    "            self.log_interval = 100\n",
    "            self.accumulation_steps = 1\n",
    "        \n",
    "        # Use JIT compilation for CPU if possible\n",
    "        if self.device == 'cpu' and hasattr(tc, 'compile'):\n",
    "            try:\n",
    "                self.model = tc.compile(self.model)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def __optimize_batch_parameters(self):\n",
    "        \"\"\"Optimize batch processing parameters\"\"\"\n",
    "        # Auto-tune batching parameters based on the device and model\n",
    "        if self.device == 'cuda':\n",
    "            # Find optimal batch size based on model size\n",
    "            param_count = sum(p.numel() for p in self.model.parameters())\n",
    "            gpu_mem = tc.cuda.get_device_properties(0).total_memory\n",
    "            \n",
    "            if param_count < 1e6:  # Small model\n",
    "                self.log_interval = 10\n",
    "            elif param_count < 1e7:  # Medium model\n",
    "                self.log_interval = 25\n",
    "            else:  # Large model\n",
    "                self.log_interval = 50\n",
    "                \n",
    "            # Calculate optimal number of workers based on CPU cores\n",
    "            self.train_loader.num_workers = min(8, os.cpu_count() or 4)\n",
    "            \n",
    "            # Set pinned memory for faster data transfer\n",
    "            if hasattr(self.train_loader, 'pin_memory'):\n",
    "                self.train_loader.pin_memory = True\n",
    "                \n",
    "        else:  # CPU optimizations\n",
    "            self.log_interval = 100\n",
    "            if hasattr(self.train_loader, 'num_workers'):\n",
    "                self.train_loader.num_workers = 0  # Avoid overhead on CPU\n",
    "\n",
    "    def __train_epoch(self):\n",
    "        \"\"\"Optimized training epoch\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Simplified metrics tracking\n",
    "        epoch_metrics = {\n",
    "            'mse': 0, 'rmse': 0, 'mae': 0, 'r2': 0\n",
    "        }\n",
    "        \n",
    "        # Gradient tracking\n",
    "        grad_norms = []\n",
    "        \n",
    "        # Streamlined progress bar\n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        \n",
    "        # Track if metrics need update\n",
    "        metrics_update_counter = 0\n",
    "        \n",
    "        for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "            try:\n",
    "                # Move data to device efficiently\n",
    "                batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision training\n",
    "                if self.use_amp:\n",
    "                    with autocast(device_type=self.device):\n",
    "                        outputs = self.model(batch_features)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        loss = self.criterion(outputs, batch_targets)\n",
    "                        loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Scale gradients\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights with scaler\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)  # More efficient\n",
    "                else:\n",
    "                    # Standard CPU training\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Optional gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Accumulate loss for reporting (unscaled)\n",
    "                train_loss += loss.item() * self.accumulation_steps\n",
    "                \n",
    "                # Update metrics less frequently\n",
    "                if batch_idx % self.log_interval == 0:\n",
    "                    metrics_update_counter += 1\n",
    "                    with tc.no_grad():\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets.detach(), outputs.detach()\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    if batch_idx > 0:\n",
    "                        pbar.set_postfix({'loss': f\"{loss.item() * self.accumulation_steps:.5f}\"})\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                logger.error(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = train_loss / len(self.train_loader)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics, grad_norms\n",
    "\n",
    "    def __validate_epoch(self):\n",
    "        \"\"\"Optimized validation epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Simplified metrics tracking\n",
    "        epoch_metrics = {\n",
    "            'mse': 0, 'rmse': 0, 'mae': 0, 'r2': 0\n",
    "        }\n",
    "        \n",
    "        # Metrics update tracking\n",
    "        metrics_update_counter = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "        \n",
    "        with tc.no_grad():\n",
    "            for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    # Move data to device efficiently\n",
    "                    batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                    batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                    \n",
    "                    # Forward pass (simpler without autocast for validation)\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    \n",
    "                    # Track loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    # Update metrics less frequently\n",
    "                    if batch_idx % self.log_interval == 0:\n",
    "                        metrics_update_counter += 1\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets, outputs\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        if batch_idx > 0:\n",
    "                            pbar.set_postfix({'loss': f\"{loss.item():.5f}\"})\n",
    "                \n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = val_loss / max(1, val_batches)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics\n",
    "\n",
    "    def __create_save_directory(self, model_name):\n",
    "        \"\"\"Create directory for saving model artifacts\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(self.save_dir, f\"{model_name}_{timestamp}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        best_model_path = os.path.join(model_dir, f\"best_{model_name}.pt\")\n",
    "        train_history_path = os.path.join(model_dir, f\"{model_name}_history.npy\")\n",
    "        train_history_file = os.path.join(model_dir, f\"{model_name}_history.csv\")\n",
    "        \n",
    "        return {\n",
    "            'model_dir': model_dir,\n",
    "            'best_model_path': best_model_path,\n",
    "            'train_history_path': train_history_path,\n",
    "            'train_history_file': train_history_file\n",
    "        }\n",
    "\n",
    "    def __save_model_checkpoint(self, path):\n",
    "        \"\"\"Save model checkpoint efficiently\"\"\"\n",
    "        # Use torch.save with _use_new_zipfile_serialization=True for faster saving\n",
    "        tc.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path, _use_new_zipfile_serialization=True)\n",
    "\n",
    "    def __save_training_progress(self, epoch, train_loss, val_loss, history_file):\n",
    "        \"\"\"Save training progress to CSV file\"\"\"\n",
    "        if epoch == 0:\n",
    "            with open(history_file, 'w') as f:\n",
    "                f.write('epoch,train_loss,val_loss\\n')\n",
    "        \n",
    "        with open(history_file, 'a') as f:\n",
    "            f.write(f'{epoch},{train_loss},{val_loss}\\n')\n",
    "\n",
    "    def __log_metrics(self, train_metrics, val_metrics):\n",
    "        \"\"\"Log metrics efficiently\"\"\"\n",
    "        # Only log essential metrics\n",
    "        essential_metrics = ['mse', 'rmse', 'r2']\n",
    "        \n",
    "        train_line = \"  \".join([f\"{m}: {train_metrics[m]:.5E}\" for m in essential_metrics if m in train_metrics])\n",
    "        val_line = \"  \".join([f\"{m}: {val_metrics[m]:.5E}\" for m in essential_metrics if m in val_metrics])\n",
    "        \n",
    "        logger.info(f\"Train: {train_line}\")\n",
    "        logger.info(f\"Valid: {val_line}\")\n",
    "\n",
    "    def set_loaders(self, X, y):\n",
    "        \"\"\"Set up optimized data loaders\"\"\"\n",
    "        # Create dataset and split\n",
    "        dataset = TensorDataset(tc.FloatTensor(X), tc.FloatTensor(y))\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create optimized data loaders\n",
    "        num_workers = min(8, os.cpu_count() or 4) if self.device == 'cuda' else 0\n",
    "        pin_memory = self.device == 'cuda'\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=512, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=1024,  # Larger batches for validation\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "\n",
    "    def train(self, num_epochs, model_name, train_loader=None, val_loader=None, X=None, y=None):\n",
    "        \"\"\"Optimized training function\"\"\"\n",
    "        # Set up data loaders\n",
    "        if train_loader and val_loader:\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "        else:\n",
    "            self.set_loaders(X, y)\n",
    "        \n",
    "        # Create save directory\n",
    "        save_paths = self.__create_save_directory(model_name)\n",
    "        \n",
    "        # Initialize learning rate scheduler with better defaults\n",
    "        scheduler = tc.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Slightly more patience\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training variables\n",
    "        grad_norms = []\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Optimize batch parameters\n",
    "        self.__optimize_batch_parameters()\n",
    "        \n",
    "        # Track time and throughput\n",
    "        start_time = time.time()\n",
    "        samples_processed = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{num_epochs}] | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_metrics, epoch_grad_norms = self.__train_epoch()\n",
    "            grad_norms.extend(epoch_grad_norms)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_metrics = self.__validate_epoch()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Check for LR changes\n",
    "            new_lr = self.optimizer.param_groups[0]['lr']\n",
    "            if new_lr != current_lr:\n",
    "                print(f\"Learning rate updated: {current_lr:.2e} → {new_lr:.2e}\")\n",
    "                current_lr = new_lr\n",
    "            \n",
    "            # Save training progress\n",
    "            self.__save_training_progress(epoch, train_loss, val_loss, save_paths['train_history_file'])\n",
    "            \n",
    "            # Track gradient statistics\n",
    "            if grad_norms:\n",
    "                avg_grad_norm = sum(grad_norms[-min(len(grad_norms), len(self.train_loader)):]) / min(len(grad_norms), len(self.train_loader))\n",
    "                self.history.setdefault('grad_norms', []).append(avg_grad_norm)\n",
    "            \n",
    "            # Track samples for throughput calculation\n",
    "            samples_processed += len(self.train_loader) * self.train_loader.batch_size\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.__save_model_checkpoint(save_paths['best_model_path'])\n",
    "                print(f\"New best model saved (Val Loss: {val_loss:.5E})\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_metrics'].append(train_metrics)\n",
    "            self.history['val_metrics'].append(val_metrics)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            samples_per_second = len(self.train_loader) * self.train_loader.batch_size / epoch_time\n",
    "            \n",
    "            # Print summary\n",
    "            epoch_summary = (\n",
    "                f\"Train Loss: {train_loss:.5E}, Val Loss: {val_loss:.5E}, \"\n",
    "                f\"Throughput: {samples_per_second:.1f} it/s, \"\n",
    "                f\"Time: {epoch_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "            print(epoch_summary)\n",
    "            \n",
    "            # Log metrics occasionally\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.__log_metrics(train_metrics, val_metrics)\n",
    "        \n",
    "        # Final performance summary\n",
    "        total_time = time.time() - start_time\n",
    "        avg_throughput = samples_processed / total_time\n",
    "        print(f\"Training complete. Total time: {total_time:.2f}s, Avg throughput: {avg_throughput:.1f} it/s\")\n",
    "        \n",
    "        # Save final history\n",
    "        np.save(save_paths['train_history_path'], self.history)\n",
    "        \n",
    "        return self.history, save_paths['model_dir']\n",
    "\n",
    "\n",
    "################ HYPERPARAMETER OPTIMIZATION ################\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    \"\"\"Optimized hyperparameters based on target hardware\"\"\"\n",
    "    # Detect device capabilities\n",
    "    device = 'cuda' if tc.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Get system info\n",
    "    cpu_count = os.cpu_count() or 4\n",
    "    \n",
    "    # Optimize based on hardware\n",
    "    if device == 'cuda':\n",
    "        # GPU configuration\n",
    "        gpu_mem = tc.cuda.get_device_properties(0).total_memory\n",
    "        gpu_mem_gb = gpu_mem / (1024**3)\n",
    "        \n",
    "        # Scale batch size based on GPU memory\n",
    "        if gpu_mem_gb > 16:  # High-end GPU\n",
    "            batch_size = 2048\n",
    "            hidden_size = 128\n",
    "        elif gpu_mem_gb > 8:  # Mid-range GPU\n",
    "            batch_size = 1024\n",
    "            hidden_size = 64\n",
    "        else:  # Entry-level GPU\n",
    "            batch_size = 512\n",
    "            hidden_size = 32\n",
    "        \n",
    "        # Workers based on CPU cores, with limit for GPU data transfer\n",
    "        num_workers = min(8, cpu_count)\n",
    "    else:\n",
    "        # CPU configuration\n",
    "        batch_size = 256  # Smaller batches for CPU\n",
    "        hidden_size = 32  # Smaller model for CPU\n",
    "        num_workers = 0  # Avoid worker overhead on CPU\n",
    "    \n",
    "    # Return optimized config\n",
    "    return {\n",
    "        'MODEL_TYPE': 'GRU',\n",
    "        'BATCH_SIZE': batch_size,\n",
    "        'HIDDEN_SIZE': hidden_size,\n",
    "        'NUM_WORKERS': num_workers,\n",
    "        'DEVICE': device,\n",
    "        'NUM_LAYERS': 1,  # Simpler model is often faster\n",
    "        'DROPOUT_RATE': 0.1,  # Lower dropout for speed\n",
    "        'ACTIVATION': 'swish',\n",
    "        'USE_LAYER_NORM': False,  # Skip layer norm for speed\n",
    "        'LEARNING_RATE': 0.001,\n",
    "        'WEIGHT_DECAY': 0.001,\n",
    "        'OPTIMIZER': 'adamw'  # AdamW often works better than Adam\n",
    "    }\n",
    "\n",
    "\n",
    "################ USAGE EXAMPLE ################\n",
    "\n",
    "def train_optimized_model():\n",
    "    \"\"\"Example of using the optimized code\"\"\"\n",
    "    # Get optimized hyperparameters\n",
    "    config = optimize_hyperparameters()\n",
    "    \n",
    "    # Initialize model with optimized configuration\n",
    "    model = GRUDMHaloMapper(\n",
    "        input_size=X_train.shape[1],\n",
    "        hidden_size=config['HIDDEN_SIZE'],\n",
    "        output_size=1,\n",
    "        num_layers=config['NUM_LAYERS'],\n",
    "        dropout_rate=config['DROPOUT_RATE'],\n",
    "        activation=config['ACTIVATION'],\n",
    "        bidirectional=False,\n",
    "        use_layer_norm=config['USE_LAYER_NORM'],\n",
    "        return_hidden=True\n",
    "    ).to(config['DEVICE'])\n",
    "    \n",
    "    # Create optimizer - AdamW usually better than Adam\n",
    "    optimizer = tc.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['LEARNING_RATE'],\n",
    "        weight_decay=config['WEIGHT_DECAY'],\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        criterion=tc.nn.MSELoss(),\n",
    "        optimizer=optimizer,\n",
    "        device=config['DEVICE'],\n",
    "        save_dir=path_manager.model_path\n",
    "    )\n",
    "    \n",
    "    # Create data loaders with optimized batch size\n",
    "    train_dataset = TensorDataset(tc.FloatTensor(X_train), tc.FloatTensor(y_train))\n",
    "    val_dataset = TensorDataset(tc.FloatTensor(X_val), tc.FloatTensor(y_val))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['BATCH_SIZE'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['NUM_WORKERS'],\n",
    "        pin_memory=config['DEVICE'] == 'cuda',\n",
    "        persistent_workers=config['NUM_WORKERS'] > 0,\n",
    "        prefetch_factor=2 if config['NUM_WORKERS'] > 0 else None\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['BATCH_SIZE'] * 2,  # Larger batches for validation\n",
    "        shuffle=False,\n",
    "        num_workers=config['NUM_WORKERS'],\n",
    "        pin_memory=config['DEVICE'] == 'cuda',\n",
    "        persistent_workers=config['NUM_WORKERS'] > 0,\n",
    "        prefetch_factor=2 if config['NUM_WORKERS'] > 0 else None\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history, model_dir = trainer.train(\n",
    "        num_epochs=30,\n",
    "        model_name=config['MODEL_TYPE'],\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader\n",
    "    )\n",
    "    \n",
    "    return model, history, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeeb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        metrics_update_counter = 0\n",
    "        \n",
    "        for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "            try:\n",
    "                # Move data to device efficiently\n",
    "                batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision training\n",
    "                if self.use_amp:\n",
    "                    with autocast(device_type=self.device):\n",
    "                        outputs = self.model(batch_features)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        loss = self.criterion(outputs, batch_targets)\n",
    "                        loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Scale gradients\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights with scaler\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)  # More efficient\n",
    "                else:\n",
    "                    # Standard CPU training\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Optional gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Accumulate loss for reporting (unscaled)\n",
    "                train_loss += loss.item() * self.accumulation_steps\n",
    "                \n",
    "                # Update metrics less frequently\n",
    "                if batch_idx % self.log_interval == 0:\n",
    "                    metrics_update_counter += 1\n",
    "                    with tc.no_grad():\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets.detach(), outputs.detach()\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    if batch_idx > 0:\n",
    "                        pbar.set_postfix({'loss': f\"{loss.item() * self.accumulation_steps:.5f}\"})\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                logger.error(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = train_loss / len(self.train_loader)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics, grad_norms\n",
    "\n",
    "    def __validate_epoch(self):\n",
    "        \"\"\"Optimized validation epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Simplified metrics tracking\n",
    "        epoch_metrics = {\n",
    "            'mse': 0, 'rmse': 0, 'mae': 0, 'r2': 0\n",
    "        }\n",
    "        \n",
    "        # Metrics update tracking\n",
    "        metrics_update_counter = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "        \n",
    "        with tc.no_grad():\n",
    "            for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    # Move data to device efficiently\n",
    "                    batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                    batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                    \n",
    "                    # Forward pass (simpler without autocast for validation)\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    \n",
    "                    # Track loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    # Update metrics less frequently\n",
    "                    if batch_idx % self.log_interval == 0:\n",
    "                        metrics_update_counter += 1\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets, outputs\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        if batch_idx > 0:\n",
    "                            pbar.set_postfix({'loss': f\"{loss.item():.5f}\"})\n",
    "                \n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = val_loss / max(1, val_batches)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics\n",
    "\n",
    "    def __create_save_directory(self, model_name):\n",
    "        \"\"\"Create directory for saving model artifacts\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(self.save_dir, f\"{model_name}_{timestamp}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        best_model_path = os.path.join(model_dir, f\"best_{model_name}.pt\")\n",
    "        train_history_path = os.path.join(model_dir, f\"{model_name}_history.npy\")\n",
    "        train_history_file = os.path.join(model_dir, f\"{model_name}_history.csv\")\n",
    "        \n",
    "        return {\n",
    "            'model_dir': model_dir,\n",
    "            'best_model_path': best_model_path,\n",
    "            'train_history_path': train_history_path,\n",
    "            'train_history_file': train_history_file\n",
    "        }\n",
    "\n",
    "    def __save_model_checkpoint(self, path):\n",
    "        \"\"\"Save model checkpoint efficiently\"\"\"\n",
    "        # Use torch.save with _use_new_zipfile_serialization=True for faster saving\n",
    "        tc.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path, _use_new_zipfile_serialization=True)\n",
    "\n",
    "    def __save_training_progress(self, epoch, train_loss, val_loss, history_file):\n",
    "        \"\"\"Save training progress to CSV file\"\"\"\n",
    "        if epoch == 0:\n",
    "            with open(history_file, 'w') as f:\n",
    "                f.write('epoch,train_loss,val_loss\\n')\n",
    "        \n",
    "        with open(history_file, 'a') as f:\n",
    "            f.write(f'{epoch},{train_loss},{val_loss}\\n')\n",
    "\n",
    "    def __log_metrics(self, train_metrics, val_metrics):\n",
    "        \"\"\"Log metrics efficiently\"\"\"\n",
    "        # Only log essential metrics\n",
    "        essential_metrics = ['mse', 'rmse', 'r2']\n",
    "        \n",
    "        train_line = \"  \".join([f\"{m}: {train_metrics[m]:.5E}\" for m in essential_metrics if m in train_metrics])\n",
    "        val_line = \"  \".join([f\"{m}: {val_metrics[m]:.5E}\" for m in essential_metrics if m in val_metrics])\n",
    "        \n",
    "        logger.info(f\"Train: {train_line}\")\n",
    "        logger.info(f\"Valid: {val_line}\")\n",
    "\n",
    "    def set_loaders(self, X, y):\n",
    "        \"\"\"Set up optimized data loaders\"\"\"\n",
    "        # Create dataset and split\n",
    "        dataset = TensorDataset(tc.FloatTensor(X), tc.FloatTensor(y))\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create optimized data loaders\n",
    "        num_workers = min(8, os.cpu_count() or 4) if self.device == 'cuda' else 0\n",
    "        pin_memory = self.device == 'cuda'\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=512, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=1024,  # Larger batches for validation\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "\n",
    "    def train(self, num_epochs, model_name, train_loader=None, val_loader=None, X=None, y=None):\n",
    "        \"\"\"Optimized training function\"\"\"\n",
    "        # Set up data loaders\n",
    "        if train_loader and val_loader:\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "        else:\n",
    "            self.set_loaders(X, y)\n",
    "        \n",
    "        # Create save directory\n",
    "        save_paths = self.__create_save_directory(model_name)\n",
    "        \n",
    "        # Initialize learning rate scheduler with better defaults\n",
    "        scheduler = tc.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Slightly more patience\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training variables\n",
    "        grad_norms = []\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Optimize batch parameters\n",
    "        self.__optimize_batch_parameters()\n",
    "        \n",
    "        # Track time and throughput\n",
    "        start_time = time.time()\n",
    "        samples_processed = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{num_epochs}] | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_metrics, epoch_grad_norms = self.__train_epoch()\n",
    "            grad_norms.extend(epoch_grad_norms)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_metrics = self.__validate_epoch()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Check for LR changes\n",
    "            new_lr = self.optimizer.param_groups[0]['lr']\n",
    "            if new_lr != current_lr:\n",
    "                print(f\"Learning rate updated: {current_lr:.2e} → {new_lr:.2e}\")\n",
    "                current_lr = new_lr\n",
    "            \n",
    "            # Save training progress\n",
    "            self.__save_training_progress(epoch, train_loss, val_loss, save_paths['train_history_file'])\n",
    "            \n",
    "            # Track gradient statistics\n",
    "            if grad_norms:\n",
    "                avg_grad_norm = sum(grad_norms[-min(len(grad_norms), len(self.train_loader)):]) / min(len(grad_norms), len(self.train_loader))\n",
    "                self.history.setdefault('grad_norms', []).append(avg_grad_norm)\n",
    "            \n",
    "            # Track samples for throughput calculation\n",
    "            samples_processed += len(self.train_loader) * self.train_loader.batch_size\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.__save_model_checkpoint(save_paths['best_model_path'])\n",
    "                print(f\"New best model saved (Val Loss: {val_loss:.5E})\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_metrics'].append(train_metrics)\n",
    "            self.history['val_metrics'].append(val_metrics)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            samples_per_second = len(self.train_loader) * self.train_loader.batch_size / epoch_time\n",
    "            \n",
    "            # Print summary\n",
    "            epoch_summary = (\n",
    "                f\"Train Loss: {train_loss:.5E}, Val Loss: {val_loss:.5E}, \"\n",
    "                f\"Throughput: {samples_per_second:.1f} it/s, \"\n",
    "                f\"Time: {epoch_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "            print(epoch_summary)\n",
    "            \n",
    "            # Log metrics occasionally\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.__log_metrics(train_metrics, val_metrics)\n",
    "        \n",
    "        # Final performance summary\n",
    "        total_time = time.time() - start_time\n",
    "        avg_throughput = samples_processed / total_time\n",
    "        print(f\"Training complete. Total time: {total_time:.2f}s, Avg throughput: {avg_throughput:.1f} it/s\")\n",
    "        \n",
    "        # Save final history\n",
    "        np.save(save_paths['train_history_path'], self.history)\n",
    "        \n",
    "        return self.history, save_paths['model_dir']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c477c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import kaleido  # For PDF export\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelVisualizer:\n",
    "    def __init__(self, predictions, true_values, spatial_coords, redshifts=None, cosmological_time=None, subhalo_coords=None, save_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the ModelVisualizer.\n",
    "\n",
    "        Args:\n",
    "            predictions (numpy.ndarray): Model predictions.\n",
    "            true_values (numpy.ndarray): True target values.\n",
    "            spatial_coords (numpy.ndarray): Spatial coordinates of the data points.\n",
    "            redshifts (numpy.ndarray, optional): Redshift values for each data point. Defaults to None.\n",
    "            cosmological_time (numpy.ndarray, optional): Cosmological time values. Defaults to None.\n",
    "            subhalo_coords (numpy.ndarray, optioanl): Spatial coordinates of subhalos. Defaults to None.\n",
    "            save_dir (str, optional): Directory to save visualizations. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.predictions = np.asarray(predictions).reshape(-1)  # Reshape predictions\n",
    "        self.true_values = np.asarray(true_values).reshape(-1)  # Reshape true_values\n",
    "        self.spatial_coords = np.asarray(spatial_coords)\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Handle length mismatches\n",
    "        pred_len = len(self.predictions)\n",
    "        true_len = len(self.true_values)\n",
    "        spatial_len = len(self.spatial_coords)\n",
    "        \n",
    "        if pred_len != true_len or pred_len != spatial_len:\n",
    "            logger.warning(f\"Length mismatch detected: predictions={pred_len}, true_values={true_len}, spatial_coords={spatial_len}\")\n",
    "            \n",
    "            # Find minimum length to trim all arrays\n",
    "            min_len = min(pred_len, true_len, spatial_len)\n",
    "            logger.info(f\"Trimming all arrays to minimum length: {min_len}\")\n",
    "            \n",
    "            # Trim arrays to the same length\n",
    "            self.predictions = self.predictions[:min_len]\n",
    "            self.true_values = self.true_values[:min_len]\n",
    "            self.spatial_coords = self.spatial_coords[:min_len]\n",
    "            \n",
    "            logger.info(f\"Arrays trimmed successfully. New lengths: predictions={len(self.predictions)}, \" \n",
    "                       f\"true_values={len(self.true_values)}, spatial_coords={len(self.spatial_coords)}\")\n",
    "        \n",
    "            \n",
    "        if self.spatial_coords.shape[1] != 3:\n",
    "            logger.error(\"Spatial coordinates must be 3-dimensional\")\n",
    "            raise ValueError(\"Spatial coordinates must be 3-dimensional\")\n",
    "            \n",
    "        # Process optional arrays, ensuring they match the trimmed length if provided\n",
    "        if redshifts is not None:\n",
    "            self.redshifts = np.asarray(redshifts)\n",
    "            if len(self.redshifts) != len(self.predictions):\n",
    "                logger.warning(f\"Redshifts length ({len(self.redshifts)}) doesn't match predictions ({len(self.predictions)}). Trimming.\")\n",
    "                self.redshifts = self.redshifts[:len(self.predictions)]\n",
    "        else:\n",
    "            self.redshifts = None\n",
    "            \n",
    "        if cosmological_time is not None:\n",
    "            self.cosmological_time = np.asarray(cosmological_time)\n",
    "            if len(self.cosmological_time) != len(self.predictions):\n",
    "                logger.warning(f\"Cosmological time length ({len(self.cosmological_time)}) doesn't match predictions ({len(self.predictions)}). Trimming.\")\n",
    "                self.cosmological_time = self.cosmological_time[:len(self.predictions)]\n",
    "        else:\n",
    "            self.cosmological_time = None\n",
    "            \n",
    "        if subhalo_coords is not None:\n",
    "            self.subhalo_coords = np.asarray(subhalo_coords)\n",
    "            if len(self.subhalo_coords) != len(self.predictions):\n",
    "                logger.warning(f\"Subhalo coordinates length ({len(self.subhalo_coords)}) doesn't match predictions ({len(self.predictions)}). Trimming.\")\n",
    "                self.subhalo_coords = self.subhalo_coords[:len(self.predictions)]\n",
    "        else:\n",
    "            self.subhalo_coords = None\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Calculate min/max radius\n",
    "        radii = np.linalg.norm(self.spatial_coords, axis=1)\n",
    "        self.min_radius = np.min(radii[radii > 0])  # Avoid zero radius\n",
    "        self.max_radius = np.max(radii)\n",
    "        \n",
    "        # Calculate spatial limits for consistent visualization\n",
    "        self.x_limits = [np.min(self.spatial_coords[:, 0]), np.max(self.spatial_coords[:, 0])]\n",
    "        self.y_limits = [np.min(self.spatial_coords[:, 1]), np.max(self.spatial_coords[:, 1])]\n",
    "        self.z_limits = [np.min(self.spatial_coords[:, 2]), np.max(self.spatial_coords[:, 2])]\n",
    "        \n",
    "        # Expands limits by 10% for better visualization\n",
    "        self.x_range = self.x_limits[1] - self.x_limits[0]\n",
    "        self.y_range = self.y_limits[1] - self.y_limits[0]\n",
    "        self.z_range = self.z_limits[1] - self.z_limits[0]\n",
    "        \n",
    "        self.x_limits = [self.x_limits[0] - 0.1 * self.x_range, self.x_limits[1] + 0.1 * self.x_range]\n",
    "        self.y_limits = [self.y_limits[0] - 0.1 * self.y_range, self.y_limits[1] + 0.1 * self.y_range]\n",
    "        self.z_limits = [self.z_limits[0] - 0.1 * self.z_range, self.z_limits[1] + 0.1 * self.z_range]\n",
    "\n",
    "    def _create_save_directory(self, subdir=None):\n",
    "        \"\"\"Create save directory if it doesn't exist.\"\"\"\n",
    "        if self.save_dir:\n",
    "            if subdir:\n",
    "                full_dir = os.path.join(self.save_dir, subdir)\n",
    "            else:\n",
    "                full_dir = self.save_dir\n",
    "            Path(full_dir).mkdir(parents=True, exist_ok=True)\n",
    "            return full_dir\n",
    "        return None\n",
    "\n",
    "    def _get_bw_style(self, index):\n",
    "        \"\"\"Get black and white styling for different data series.\"\"\"\n",
    "        styles = [\n",
    "            {'color': 'black', 'dash': 'solid', 'symbol': 'circle'},\n",
    "            {'color': 'black', 'dash': 'dash', 'symbol': 'square'},\n",
    "            {'color': 'black', 'dash': 'dot', 'symbol': 'diamond'},\n",
    "            {'color': 'black', 'dash': 'dashdot', 'symbol': 'triangle-up'},\n",
    "            {'color': 'gray', 'dash': 'solid', 'symbol': 'x'},\n",
    "            {'color': 'gray', 'dash': 'dash', 'symbol': 'cross'},\n",
    "        ]\n",
    "        return styles[index % len(styles)]\n",
    "\n",
    "    def plot_prediction_vs_true_2d(self, save_formats=['html', 'pdf']):\n",
    "        \"\"\"Create black and white 2D scatter plot of predictions vs true values.\"\"\"\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(np.min(self.predictions), np.min(self.true_values))\n",
    "        max_val = max(np.max(self.predictions), np.max(self.true_values))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            line=dict(color='gray', dash='dash', width=2),\n",
    "            name='Perfect Prediction',\n",
    "            showlegend=True\n",
    "        ))\n",
    "        \n",
    "        # Actual data points\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.true_values,\n",
    "            y=self.predictions,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='black',\n",
    "                symbol='circle',\n",
    "                size=6,\n",
    "                line=dict(color='gray', width=1)\n",
    "            ),\n",
    "            name='Predictions vs True',\n",
    "            showlegend=True\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            template='plotly_white',\n",
    "            title='Model Predictions vs True Values',\n",
    "            xaxis_title='True Values',\n",
    "            yaxis_title='Predictions',\n",
    "            width=800,\n",
    "            height=600,\n",
    "            font=dict(color='black'),\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        # Save in different formats\n",
    "        save_dir = self._create_save_directory('2d_plots')\n",
    "        if save_dir:\n",
    "            for fmt in save_formats:\n",
    "                if fmt == 'html':\n",
    "                    fig.write_html(os.path.join(save_dir, 'predictions_vs_true.html'))\n",
    "                elif fmt == 'pdf':\n",
    "                    fig.write_image(os.path.join(save_dir, 'predictions_vs_true.pdf'))\n",
    "                elif fmt == 'png':\n",
    "                    fig.write_image(os.path.join(save_dir, 'predictions_vs_true.png'))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def plot_residuals_2d(self, save_formats=['html', 'pdf']):\n",
    "        \"\"\"Create black and white 2D residuals plot.\"\"\"\n",
    "        residuals = self.predictions - self.true_values\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=('Residuals vs True Values', 'Residuals Distribution'),\n",
    "            vertical_spacing=0.12\n",
    "        )\n",
    "        \n",
    "        # Residuals vs true values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=self.true_values,\n",
    "                y=residuals,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color='black',\n",
    "                    symbol='circle',\n",
    "                    size=5,\n",
    "                    line=dict(color='gray', width=0.5)\n",
    "                ),\n",
    "                name='Residuals',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Zero line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[np.min(self.true_values), np.max(self.true_values)],\n",
    "                y=[0, 0],\n",
    "                mode='lines',\n",
    "                line=dict(color='gray', dash='dash', width=2),\n",
    "                name='Zero Line',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Residuals histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=residuals,\n",
    "                nbinsx=30,\n",
    "                marker=dict(color='lightgray', line=dict(color='black', width=1)),\n",
    "                name='Distribution',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            template='plotly_white',\n",
    "            title='Residuals Analysis',\n",
    "            height=800,\n",
    "            font=dict(color='black'),\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(title_text='True Values', row=1, col=1)\n",
    "        fig.update_yaxes(title_text='Residuals', row=1, col=1)\n",
    "        fig.update_xaxes(title_text='Residuals', row=2, col=1)\n",
    "        fig.update_yaxes(title_text='Frequency', row=2, col=1)\n",
    "        \n",
    "        # Save in different formats\n",
    "        save_dir = self._create_save_directory('2d_plots')\n",
    "        if save_dir:\n",
    "            for fmt in save_formats:\n",
    "                if fmt == 'html':\n",
    "                    fig.write_html(os.path.join(save_dir, 'residuals_analysis.html'))\n",
    "                elif fmt == 'pdf':\n",
    "                    fig.write_image(os.path.join(save_dir, 'residuals_analysis.pdf'))\n",
    "                elif fmt == 'png':\n",
    "                    fig.write_image(os.path.join(save_dir, 'residuals_analysis.png'))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def plot_spatial_distribution_3d(self, save_formats=['html']):\n",
    "        \"\"\"Create 3D spatial distribution with non-overlapping colorbars.\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Predictions', 'True Values', 'Residuals', 'Spatial Coordinates'),\n",
    "            specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}],\n",
    "                   [{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "            horizontal_spacing=0.05,\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        residuals = self.predictions - self.true_values\n",
    "        \n",
    "        # Configure colorbar positions to avoid overlap\n",
    "        colorbar_positions = [\n",
    "            {'x': 0.23, 'y': 0.85, 'len': 0.35},  # Top left\n",
    "            {'x': 0.77, 'y': 0.85, 'len': 0.35},  # Top right\n",
    "            {'x': 0.23, 'y': 0.15, 'len': 0.35},  # Bottom left\n",
    "            {'x': 0.77, 'y': 0.15, 'len': 0.35}   # Bottom right\n",
    "        ]\n",
    "        \n",
    "        # Predictions\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=self.spatial_coords[:, 0],\n",
    "                y=self.spatial_coords[:, 1],\n",
    "                z=self.spatial_coords[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=4,\n",
    "                    color=self.predictions,\n",
    "                    colorscale='Viridis',\n",
    "                    colorbar=dict(\n",
    "                        title='Predictions',\n",
    "                        titleside='right',\n",
    "                        **colorbar_positions[0]\n",
    "                    ),\n",
    "                    showscale=True\n",
    "                ),\n",
    "                name='Predictions',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # True Values\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=self.spatial_coords[:, 0],\n",
    "                y=self.spatial_coords[:, 1],\n",
    "                z=self.spatial_coords[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=4,\n",
    "                    color=self.true_values,\n",
    "                    colorscale='Plasma',\n",
    "                    colorbar=dict(\n",
    "                        title='True Values',\n",
    "                        titleside='right',\n",
    "                        **colorbar_positions[1]\n",
    "                    ),\n",
    "                    showscale=True\n",
    "                ),\n",
    "                name='True Values',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Residuals\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=self.spatial_coords[:, 0],\n",
    "                y=self.spatial_coords[:, 1],\n",
    "                z=self.spatial_coords[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=4,\n",
    "                    color=residuals,\n",
    "                    colorscale='RdBu',\n",
    "                    colorbar=dict(\n",
    "                        title='Residuals',\n",
    "                        titleside='right',\n",
    "                        **colorbar_positions[2]\n",
    "                    ),\n",
    "                    showscale=True\n",
    "                ),\n",
    "                name='Residuals',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Spatial coordinates (colored by distance from origin)\n",
    "        distances = np.linalg.norm(self.spatial_coords, axis=1)\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=self.spatial_coords[:, 0],\n",
    "                y=self.spatial_coords[:, 1],\n",
    "                z=self.spatial_coords[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=4,\n",
    "                    color=distances,\n",
    "                    colorscale='Cividis',\n",
    "                    colorbar=dict(\n",
    "                        title='Distance',\n",
    "                        titleside='right',\n",
    "                        **colorbar_positions[3]\n",
    "                    ),\n",
    "                    showscale=True\n",
    "                ),\n",
    "                name='Distance',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            template='plotly_white',\n",
    "            title='3D Spatial Distribution Analysis',\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            font=dict(color='black'),\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            ),\n",
    "            scene2=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            ),\n",
    "            scene3=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            ),\n",
    "            scene4=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Save with frame extraction\n",
    "        save_dir = self._create_save_directory('3d_plots')\n",
    "        if save_dir:\n",
    "            for fmt in save_formats:\n",
    "                if fmt == 'html':\n",
    "                    fig.write_html(os.path.join(save_dir, 'spatial_distribution_3d.html'))\n",
    "                elif fmt == 'pdf':\n",
    "                    fig.write_image(os.path.join(save_dir, 'spatial_distribution_3d.pdf'))\n",
    "        \n",
    "        # Create frame snapshots with different zoom levels\n",
    "        self._create_3d_snapshots(fig, 'spatial_distribution_3d', save_dir)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def plot_redshift_evolution(self, save_formats=['html', 'pdf']):\n",
    "        \"\"\"Create standalone redshift evolution visualization.\"\"\"\n",
    "        if self.redshifts is None:\n",
    "            logger.warning(\"No redshift data available for evolution plot\")\n",
    "            return None\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Predictions vs Redshift', 'True Values vs Redshift', \n",
    "                          'Residuals vs Redshift', 'Redshift Distribution'),\n",
    "            specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "                   [{'secondary_y': False}, {'secondary_y': False}]]\n",
    "        )\n",
    "        \n",
    "        residuals = self.predictions - self.true_values\n",
    "        \n",
    "        # Sort by redshift for line plots\n",
    "        sort_idx = np.argsort(self.redshifts)\n",
    "        sorted_z = self.redshifts[sort_idx]\n",
    "        sorted_pred = self.predictions[sort_idx]\n",
    "        sorted_true = self.true_values[sort_idx]\n",
    "        sorted_res = residuals[sort_idx]\n",
    "        \n",
    "        # Predictions vs Redshift\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=sorted_z,\n",
    "                y=sorted_pred,\n",
    "                mode='markers+lines',\n",
    "                line=dict(color='black', dash='solid', width=2),\n",
    "                marker=dict(color='black', symbol='circle', size=4),\n",
    "                name='Predictions',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # True Values vs Redshift\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=sorted_z,\n",
    "                y=sorted_true,\n",
    "                mode='markers+lines',\n",
    "                line=dict(color='black', dash='dash', width=2),\n",
    "                marker=dict(color='black', symbol='square', size=4),\n",
    "                name='True Values',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Residuals vs Redshift\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=sorted_z,\n",
    "                y=sorted_res,\n",
    "                mode='markers',\n",
    "                marker=dict(color='black', symbol='diamond', size=4),\n",
    "                name='Residuals',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Zero line for residuals\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[np.min(sorted_z), np.max(sorted_z)],\n",
    "                y=[0, 0],\n",
    "                mode='lines',\n",
    "                line=dict(color='gray', dash='dash', width=1),\n",
    "                name='Zero Line',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Redshift Distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=self.redshifts,\n",
    "                nbinsx=30,\n",
    "                marker=dict(color='lightgray', line=dict(color='black', width=1)),\n",
    "                name='Distribution',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            template='plotly_white',\n",
    "            title='Redshift Evolution Analysis',\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            font=dict(color='black'),\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig.update_xaxes(title_text='Redshift', row=1, col=1)\n",
    "        fig.update_yaxes(title_text='Predictions', row=1, col=1)\n",
    "        fig.update_xaxes(title_text='Redshift', row=1, col=2)\n",
    "        fig.update_yaxes(title_text='True Values', row=1, col=2)\n",
    "        fig.update_xaxes(title_text='Redshift', row=2, col=1)\n",
    "        fig.update_yaxes(title_text='Residuals', row=2, col=1)\n",
    "        fig.update_xaxes(title_text='Redshift', row=2, col=2)\n",
    "        fig.update_yaxes(title_text='Frequency', row=2, col=2)\n",
    "        \n",
    "        # Save in different formats\n",
    "        save_dir = self._create_save_directory('redshift_evolution')\n",
    "        if save_dir:\n",
    "            for fmt in save_formats:\n",
    "                if fmt == 'html':\n",
    "                    fig.write_html(os.path.join(save_dir, 'redshift_evolution.html'))\n",
    "                elif fmt == 'pdf':\n",
    "                    fig.write_image(os.path.join(save_dir, 'redshift_evolution.pdf'))\n",
    "                elif fmt == 'png':\n",
    "                    fig.write_image(os.path.join(save_dir, 'redshift_evolution.png'))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def _create_3d_snapshots(self, fig, base_name, save_dir):\n",
    "        \"\"\"Create snapshots of 3D plots with different camera angles and zoom levels.\"\"\"\n",
    "        if not save_dir:\n",
    "            return\n",
    "        \n",
    "        # Create frames directory\n",
    "        frames_dir = os.path.join(save_dir, f'{base_name}_frames')\n",
    "        Path(frames_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Camera configurations\n",
    "        camera_configs = {\n",
    "            'default': dict(),\n",
    "            'zoom_2x': dict(\n",
    "                eye=dict(x=0.75, y=0.75, z=0.75)\n",
    "            ),\n",
    "            'zoom_5x': dict(\n",
    "                eye=dict(x=0.3, y=0.3, z=0.3)\n",
    "            ),\n",
    "            'zoom_10x': dict(\n",
    "                eye=dict(x=0.15, y=0.15, z=0.15)\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Multiple viewing angles\n",
    "        angles = {\n",
    "            'front': dict(eye=dict(x=1.5, y=0, z=0)),\n",
    "            'side': dict(eye=dict(x=0, y=1.5, z=0)),\n",
    "            'top': dict(eye=dict(x=0, y=0, z=1.5)),\n",
    "            'diagonal': dict(eye=dict(x=1.25, y=1.25, z=1.25))\n",
    "        }\n",
    "        \n",
    "        for zoom_name, zoom_config in camera_configs.items():\n",
    "            for angle_name, angle_config in angles.items():\n",
    "                # Combine zoom and angle configurations\n",
    "                camera = {**zoom_config, **angle_config}\n",
    "                \n",
    "                # Update figure with camera settings\n",
    "                fig_copy = fig\n",
    "                fig_copy.update_layout(\n",
    "                    scene_camera=camera,\n",
    "                    scene2_camera=camera,\n",
    "                    scene3_camera=camera,\n",
    "                    scene4_camera=camera\n",
    "                )\n",
    "                \n",
    "                # Save as PNG and PDF\n",
    "                filename_base = f'{base_name}_{zoom_name}_{angle_name}'\n",
    "                fig_copy.write_image(os.path.join(frames_dir, f'{filename_base}.png'))\n",
    "                fig_copy.write_image(os.path.join(frames_dir, f'{filename_base}.pdf'))\n",
    "\n",
    "    def create_comprehensive_report(self):\n",
    "        \"\"\"Generate all visualizations and create a comprehensive report.\"\"\"\n",
    "        logger.info(\"Creating comprehensive visualization report...\")\n",
    "        \n",
    "        # Create all visualizations\n",
    "        plots = {}\n",
    "        \n",
    "        # 2D Plots\n",
    "        plots['prediction_vs_true'] = self.plot_prediction_vs_true_2d()\n",
    "        plots['residuals'] = self.plot_residuals_2d()\n",
    "        \n",
    "        # 3D Plots\n",
    "        plots['spatial_3d'] = self.plot_spatial_distribution_3d()\n",
    "        \n",
    "        # Redshift evolution (if available)\n",
    "        if self.redshifts is not None:\n",
    "            plots['redshift_evolution'] = self.plot_redshift_evolution()\n",
    "        \n",
    "        # Create summary statistics\n",
    "        self._create_summary_statistics()\n",
    "        \n",
    "        logger.info(\"Comprehensive report created successfully!\")\n",
    "        return plots\n",
    "\n",
    "    def _create_summary_statistics(self):\n",
    "        \"\"\"Create a summary statistics file.\"\"\"\n",
    "        if not self.save_dir:\n",
    "            return\n",
    "        \n",
    "        stats = {\n",
    "            'Dataset Size': len(self.predictions),\n",
    "            'Spatial Dimensions': self.spatial_coords.shape,\n",
    "            'Prediction Range': [float(np.min(self.predictions)), float(np.max(self.predictions))],\n",
    "            'True Values Range': [float(np.min(self.true_values)), float(np.max(self.true_values))],\n",
    "            'Mean Absolute Error': float(np.mean(np.abs(self.predictions - self.true_values))),\n",
    "            'Root Mean Square Error': float(np.sqrt(np.mean((self.predictions - self.true_values)**2))),\n",
    "            'R² Score': float(1 - (np.sum((self.true_values - self.predictions)**2) / \n",
    "                            np.sum((self.true_values - np.mean(self.true_values))**2))),\n",
    "            'Spatial Extents': {\n",
    "                'X': [float(self.x_limits[0]), float(self.x_limits[1])],\n",
    "                'Y': [float(self.y_limits[0]), float(self.y_limits[1])],\n",
    "                'Z': [float(self.z_limits[0]), float(self.z_limits[1])]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.redshifts is not None:\n",
    "            stats['Redshift Range'] = [float(np.min(self.redshifts)), float(np.max(self.redshifts))]\n",
    "        \n",
    "        # Save statistics as text file\n",
    "        save_dir = self._create_save_directory()\n",
    "        if save_dir:\n",
    "            with open(os.path.join(save_dir, 'summary_statistics.txt'), 'w') as f:\n",
    "                for key, value in stats.items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data generation (replace with your actual data)\n",
    "    n_points = 1000\n",
    "    spatial_coords = np.random.randn(n_points, 3) * 10\n",
    "    true_values = np.random.randn(n_points)\n",
    "    predictions = true_values + np.random.randn(n_points) * 0.1  # Add some noise\n",
    "    redshifts = np.random.uniform(0, 2, n_points)\n",
    "    \n",
    "    # Initialize visualizer\n",
    "    visualizer = ModelVisualizer(\n",
    "        predictions=predictions,\n",
    "        true_values=true_values,\n",
    "        spatial_coords=spatial_coords,\n",
    "        redshifts=redshifts,\n",
    "        save_dir='visualization_output'\n",
    "    )\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    plots = visualizer.create_comprehensive_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import io\n",
    "from directory_manager import PathManager\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "path_manager = PathManager()\n",
    "logger = path_manager.get_logger()\n",
    "\n",
    "class ModelVisualizer:\n",
    "    def __init__(self, predictions, true_values, spatial_coords, redshifts=None, cosmological_time=None, subhalo_coords=None, save_dir=None):\n",
    "        \"\"\"Initialize the ModelVisualizer.\"\"\"\n",
    "        # [Keep all your existing __init__ code exactly as is]\n",
    "        # ... [previous __init__ code remains unchanged] ...\n",
    "        \n",
    "        # Add new attributes for visualization settings\n",
    "        self.bw_line_styles = ['solid', 'dot', 'dash', 'longdash', 'dashdot', 'longdashdot']\n",
    "        self.bw_symbols = ['circle', 'square', 'diamond', 'cross', 'x', 'triangle-up']\n",
    "        \n",
    "    # [Keep all your existing methods until __plot_density_profile]\n",
    "    \n",
    "    def __plot_density_profile(self, r_bins: np.ndarray, profile: np.ndarray, title: str, yaxis_title: str) -> go.Figure:\n",
    "        \"\"\"Modified to produce black and white plots with different line styles.\"\"\"\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=r_bins,\n",
    "                y=profile,\n",
    "                mode='lines',\n",
    "                name=r\"$\\text{\" + title + r\"}$\",\n",
    "                line=dict(\n",
    "                    width=2,\n",
    "                    color='black',\n",
    "                    dash=self.bw_line_styles[0]  # First line style\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=r\"$\\text{\" + title + r\"}$\",\n",
    "            xaxis_title=r\"$\\text{Radius (Mpc)}$\",\n",
    "            yaxis_title=r\"$\\text{\" + yaxis_title + r\"}$\",\n",
    "            xaxis=dict(type=\"log\", title_font=dict(size=16)),\n",
    "            yaxis=dict(type=\"log\", title_font=dict(size=16)),\n",
    "            template=\"plotly_white\",\n",
    "            font=dict(family=\"Arial, sans-serif\", size=14),\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def __plot_all_density_profiles(self, r_bins, profiles):\n",
    "        \"\"\"Modified for black and white with different line styles.\"\"\"\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        for i, (name, profile) in enumerate(profiles.items()):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=r_bins,\n",
    "                    y=profile,\n",
    "                    mode='lines',\n",
    "                    name=f\"$\\\\text{{{name}}}$\",\n",
    "                    line=dict(\n",
    "                        width=2,\n",
    "                        color='black',\n",
    "                        dash=self.bw_line_styles[i % len(self.bw_line_styles)]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=r\"$\\text{Dark Matter Halos Density Profiles}$\",\n",
    "            xaxis_title=r\"$\\text{Radius (Mpc)}$\",\n",
    "            yaxis_title=r\"$\\text{Density (M_{\\odot} / Mpc^{3})}$\",\n",
    "            xaxis=dict(type=\"log\", title_font=dict(size=16)),\n",
    "            yaxis=dict(type=\"log\", title_font=dict(size=16)),\n",
    "            template=\"plotly_white\",\n",
    "            font=dict(family=\"Arial, sans-serif\", size=14),\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def visualize_dm_distribution(self, a=1.0, b=0.8, c=0.6):\n",
    "        \"\"\"Modified to prevent color bar overlap.\"\"\"\n",
    "        try:\n",
    "            # [Previous code until figure creation]\n",
    "            \n",
    "            # Create subplots with adjusted spacing for color bars\n",
    "            fig = sp.make_subplots(\n",
    "                rows=1,\n",
    "                cols=3,\n",
    "                specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}]],\n",
    "                subplot_titles=[\n",
    "                    r\"$\\text{Dark Matter Distribution}$\", \n",
    "                    r\"$\\text{Triaxial Model}$\", \n",
    "                    r\"$\\text{Redshift Evolution}$\"\n",
    "                ],\n",
    "                horizontal_spacing=0.1,  # Increased spacing\n",
    "                vertical_spacing=0.1\n",
    "            )\n",
    "            \n",
    "            # [Previous trace creation code]\n",
    "            \n",
    "            # Modified color bar positions\n",
    "            fig.update_traces(\n",
    "                marker_colorbar_x=0.28,  # First color bar\n",
    "                selector=dict(name=r\"$\\text{DM Halos}$\")\n",
    "            )\n",
    "            \n",
    "            fig.update_traces(\n",
    "                marker_colorbar_x=0.85,  # Second color bar\n",
    "                selector=dict(name=r\"$\\text{Predictions}$\")\n",
    "            )\n",
    "            \n",
    "            # [Rest of the previous code]\n",
    "            \n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in visualization: {e}\", exc_info=True)\n",
    "            return None\n",
    "    \n",
    "    def export_animation_frames(self, fig, frames, base_name):\n",
    "        \"\"\"Export animation frames as PNG and PDF with different zoom levels.\"\"\"\n",
    "        try:\n",
    "            if not self.save_dir:\n",
    "                logger.warning(\"No save directory specified for frame export\")\n",
    "                return\n",
    "            \n",
    "            # Create subdirectory for frames\n",
    "            frame_dir = os.path.join(self.save_dir, f\"{base_name}_frames\")\n",
    "            os.makedirs(frame_dir, exist_ok=True)\n",
    "            \n",
    "            # Camera views to export\n",
    "            camera_views = {\n",
    "                'default': None,\n",
    "                'zoom2x': dict(eye=dict(x=0.5, y=0.5, z=0.5)),\n",
    "                'zoom5x': dict(eye=dict(x=0.2, y=0.2, z=0.2)),\n",
    "                'zoom10x': dict(eye=dict(x=0.1, y=0.1, z=0.1))\n",
    "            }\n",
    "            \n",
    "            for i, frame in enumerate(frames):\n",
    "                # Create a copy of the figure\n",
    "                fig_copy = go.Figure(fig)\n",
    "                \n",
    "                # Update with frame data\n",
    "                for j, trace in enumerate(frame.data):\n",
    "                    if j < len(fig_copy.data):\n",
    "                        fig_copy.data[j].update(trace)\n",
    "                \n",
    "                # Update title if present\n",
    "                if hasattr(frame, 'name') and frame.name:\n",
    "                    current_title = fig_copy.layout.title.text\n",
    "                    if current_title:\n",
    "                        base_title = current_title.split(\" at\")[0]\n",
    "                        fig_copy.update_layout(title=f\"{base_title} at {frame.name}\")\n",
    "                \n",
    "                # Export for each camera view\n",
    "                for view_name, camera in camera_views.items():\n",
    "                    if camera:\n",
    "                        fig_copy.update_layout(scene_camera=camera)\n",
    "                    \n",
    "                    # Save as PNG\n",
    "                    png_path = os.path.join(frame_dir, f\"frame_{i:03d}_{view_name}.png\")\n",
    "                    fig_copy.write_image(png_path, width=1200, height=800)\n",
    "                    \n",
    "                    # Convert to PDF using matplotlib\n",
    "                    img = Image.open(png_path)\n",
    "                    pdf_path = os.path.join(frame_dir, f\"frame_{i:03d}_{view_name}.pdf\")\n",
    "                    img.save(pdf_path, \"PDF\", resolution=100.0)\n",
    "            \n",
    "            logger.info(f\"Exported frames to {frame_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error exporting frames: {e}\", exc_info=True)\n",
    "    \n",
    "    def visualize_redshift_evolution_standalone(self):\n",
    "        \"\"\"Standalone redshift evolution visualization.\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'redshifts'):\n",
    "                logger.error(\"Redshift data not available\")\n",
    "                return None\n",
    "                \n",
    "            # Get unique redshifts (high to low)\n",
    "            unique_redshifts = np.sort(np.unique(self.redshifts))[::-1]\n",
    "            \n",
    "            # Create figure with initial frame (highest redshift)\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Calculate normalized densities\n",
    "            denom = np.max(self.predictions) - np.min(self.predictions)\n",
    "            if denom == 0:\n",
    "                norm_predictions = np.zeros_like(self.predictions)\n",
    "            else:\n",
    "                norm_predictions = (self.predictions - np.min(self.predictions)) / denom\n",
    "            \n",
    "            # Add initial trace\n",
    "            initial_mask = (self.redshifts == unique_redshifts[0])\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=self.spatial_coords[initial_mask, 0],\n",
    "                    y=self.spatial_coords[initial_mask, 1],\n",
    "                    z=self.spatial_coords[initial_mask, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=3,\n",
    "                        color=norm_predictions[initial_mask],\n",
    "                        colorscale='Viridis',\n",
    "                        opacity=0.8,\n",
    "                        colorbar=dict(title=\"Normalized Density\", x=0.85)\n",
    "                    ),\n",
    "                    name=\"Dark Matter\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Create animation frames\n",
    "            frames = []\n",
    "            for z in unique_redshifts:\n",
    "                mask = (self.redshifts == z)\n",
    "                if not np.any(mask):\n",
    "                    continue\n",
    "                    \n",
    "                frames.append(go.Frame(\n",
    "                    data=[go.Scatter3d(\n",
    "                        x=self.spatial_coords[mask, 0],\n",
    "                        y=self.spatial_coords[mask, 1],\n",
    "                        z=self.spatial_coords[mask, 2],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=3,\n",
    "                            color=norm_predictions[mask],\n",
    "                            colorscale='Viridis',\n",
    "                            opacity=0.8\n",
    "                        )\n",
    "                    )],\n",
    "                    name=f\"z={z:.5E}\",\n",
    "                    traces=[0]\n",
    "                ))\n",
    "            \n",
    "            # Configure layout\n",
    "            fig.update_layout(\n",
    "                title=\"Standalone Redshift Evolution\",\n",
    "                scene=dict(\n",
    "                    xaxis_title=r\"$X \\, (\\text{Mpc})$\",\n",
    "                    yaxis_title=r\"$Y \\, (\\text{Mpc})$\",\n",
    "                    zaxis_title=r\"$Z \\, (\\text{Mpc})$\",\n",
    "                    aspectmode='cube',\n",
    "                    xaxis=dict(range=self.x_limits),\n",
    "                    yaxis=dict(range=self.y_limits),\n",
    "                    zaxis=dict(range=self.z_limits)\n",
    "                ),\n",
    "                updatemenus=[{\n",
    "                    'type': 'buttons',\n",
    "                    'buttons': [\n",
    "                        {\n",
    "                            'args': [None, {\n",
    "                                'frame': {'duration': 500, 'redraw': True},\n",
    "                                'fromcurrent': True,\n",
    "                                'transition': {'duration': 0}\n",
    "                            }],\n",
    "                            'label': 'Play',\n",
    "                            'method': 'animate'\n",
    "                        },\n",
    "                        {\n",
    "                            'args': [[None], {\n",
    "                                'frame': {'duration': 0},\n",
    "                                'mode': 'immediate'\n",
    "                            }],\n",
    "                            'label': 'Pause',\n",
    "                            'method': 'animate'\n",
    "                        }\n",
    "                    ],\n",
    "                    'x': 0.1,\n",
    "                    'y': 0,\n",
    "                    'xanchor': 'right',\n",
    "                    'yanchor': 'top'\n",
    "                }],\n",
    "                sliders=[{\n",
    "                    'active': 0,\n",
    "                    'currentvalue': {'prefix': 'Redshift: '},\n",
    "                    'steps': [{\n",
    "                        'args': [[frame.name], {\n",
    "                            'frame': {'duration': 300, 'redraw': True},\n",
    "                            'mode': 'immediate'\n",
    "                        }],\n",
    "                        'label': frame.name,\n",
    "                        'method': 'animate'\n",
    "                    } for frame in frames]\n",
    "                }],\n",
    "                width=1000,\n",
    "                height=800,\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            \n",
    "            fig.frames = frames\n",
    "            \n",
    "            # Export frames if save_dir exists\n",
    "            if self.save_dir:\n",
    "                self.export_animation_frames(fig, frames, 'redshift_evolution')\n",
    "                \n",
    "                # Save HTML\n",
    "                html_path = os.path.join(self.save_dir, 'standalone_redshift_evolution.html')\n",
    "                fig.write_html(html_path, include_mathjax='cdn')\n",
    "                logger.info(f\"Saved standalone visualization to {html_path}\")\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in standalone redshift visualization: {e}\", exc_info=True)\n",
    "            return None\n",
    "    \n",
    "    # [Add this new method to your existing class]\n",
    "    def save_2d_plots_as_pdf(self, fig, filename):\n",
    "        \"\"\"Save 2D plots as PDF with black and white styling.\"\"\"\n",
    "        try:\n",
    "            if not self.save_dir:\n",
    "                logger.warning(\"No save directory specified for PDF export\")\n",
    "                return\n",
    "                \n",
    "            # Convert to static image first\n",
    "            img_bytes = fig.to_image(format=\"png\")\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            \n",
    "            # Save as PDF\n",
    "            pdf_path = os.path.join(self.save_dir, filename)\n",
    "            img.save(pdf_path, \"PDF\", resolution=100.0)\n",
    "            logger.info(f\"Saved 2D plot as PDF to {pdf_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving 2D plot as PDF: {e}\", exc_info=True)\n",
    "\n",
    "    # [Modify all methods that create 2D plots to call save_2d_plots_as_pdf]\n",
    "    # For example, modify density_profile() like this:\n",
    "    def density_profile(self):\n",
    "        try:\n",
    "            r_bins, density_profile = self.__calculate_density_profile()\n",
    "            fig = self.__plot_density_profile(r_bins, density_profile, \n",
    "                                            'Dark Matter Halos Density Profile', \n",
    "                                            r'Density (Msun/Mpc^{3})')\n",
    "            \n",
    "            if fig and self.save_dir:\n",
    "                self.save_2d_plots_as_pdf(fig, 'density_profile.pdf')\n",
    "                \n",
    "            return fig\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in density profile calculation: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # [Similarly modify all other 2D plotting methods]\n",
    "    \n",
    "    # [Keep all your other existing methods]\n",
    "    # ... [rest of your existing code remains unchanged] ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requqira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
