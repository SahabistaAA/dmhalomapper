{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy.stats import skew, kurtosis, sigmaclip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import torch.utils.data as tcud\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data = None\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load dataset from a CSV file.\"\"\"\n",
    "        try:\n",
    "            if self.data_path is None:\n",
    "                raise ValueError(\"Data path is not provided.\")\n",
    "            #self.data = pd.read_csv(self.data_path).drop(['Unnamed: 0'], axis=1)\n",
    "            self.data = pd.read_feather(self.data_path)\n",
    "            print(\"Data loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "        \n",
    "    def data_info(self):\n",
    "        if self.data is not None:\n",
    "            return self.data.info()\n",
    "        else:\n",
    "            return 'Data is not loaded'\n",
    "    \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Compute basic statistics for numeric columns.\"\"\"\n",
    "        if self.data is not None:\n",
    "            stats = self.data.describe().T\n",
    "            stats[\"Skewness\"] = self.data.skew()\n",
    "            stats[\"Kurtosis\"] = self.data.kurt()\n",
    "            print(\"\\nData statistics\")\n",
    "            print(stats)\n",
    "            return stats\n",
    "        else:\n",
    "            raise ValueError\n",
    "    \n",
    "    def non_category_features(self):\n",
    "        if self.data is not None:\n",
    "            self.non_categorical_features = self.data.select_dtypes(exclude=['object']).columns\n",
    "            return self\n",
    "        else:\n",
    "            return 'Data is not loaded'\n",
    "    \n",
    "    def missing_values(self):\n",
    "        \"\"\"Check for missing values in the dataset.\"\"\"\n",
    "        if self.data is not None:\n",
    "            return self.data.isnull().sum()\n",
    "        else:\n",
    "            print(\"Data is not loaded.\")\n",
    "            return None\n",
    "    \n",
    "    def duplicated_values(self):\n",
    "        if self.data is not None:\n",
    "            return self.data.duplicated().sum()\n",
    "        else:\n",
    "            return 'Data is not loaded'\n",
    "    \n",
    "    def data_columns(self):\n",
    "        if self.data is not None:\n",
    "            return self.data.columns\n",
    "        else:\n",
    "            return 'Data is not loaded'\n",
    "    \n",
    "    def outlier_analysis(self):\n",
    "        self.Q1 = self.data[self.data.columns].quantile(0.25)\n",
    "        self.Q3 = self.data[self.data.columns].quantile(0.75)\n",
    "        self.IQR = self.Q3 - self.Q1\n",
    "\n",
    "        # define the boundary\n",
    "        self.lower_bound = self.Q1 - 1.5 * self.IQR\n",
    "        self.upper_bound = self.Q3 + 1.5 * self.IQR\n",
    "\n",
    "        # checking outliers\n",
    "        self.outliers = self.data[(self.data[self.data.columns] < self.lower_bound) | (self.data[self.data.columns] > self.upper_bound)]\n",
    "\n",
    "        # number of outliers\n",
    "        self.n_outliers = self.outliers.shape[0]\n",
    "\n",
    "        # percentage outlier\n",
    "        self.pct_outliers = (self.n_outliers / self.data.shape[0]) * 100\n",
    "\n",
    "        return self.n_outliers, self.pct_outliers, self.Q1, self.Q3, self.IQR, self.lower_bound, self.upper_bound\n",
    "\n",
    "    def analyze_outliers(self):\n",
    "        outlier_summary = []\n",
    "        for column in self.non_categorical_features:\n",
    "            n_outliers, pct_outliers, Q1, Q3, IQR, lower, upper_bound = self.outlier_analysis()\n",
    "            if n_outliers > 0:\n",
    "                outlier_summary.append({\n",
    "                    'Feature': column,\n",
    "                    'Number of Outliers': n_outliers,\n",
    "                    'Percentage (%)': pct_outliers,\n",
    "                    'Q1': Q1,\n",
    "                    'Q3': Q3,\n",
    "                    'IQR': IQR,\n",
    "                    'Lower Bound': lower,\n",
    "                    'Upper Bound': upper_bound,\n",
    "                })\n",
    "        return pd.DataFrame(outlier_summary)\n",
    "\n",
    "    def plot_distribution(self, features, output_dir, title_prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Plot and save distribution histograms for the given features.\n",
    "        Args:\n",
    "            features (list): List of feature names to plot.\n",
    "            output_dir (str): Directory to save the plots.\n",
    "            title_prefix (str): Prefix for the plot titles (optional).\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for feature in features:\n",
    "            try:\n",
    "                plt.figure(figsize=(10, 6), dpi=300)\n",
    "                sns.histplot(self.data[feature], bins=50, kde=True, color='skyblue', edgecolor='black')\n",
    "                plt.title(f\"{title_prefix}Histogram of {feature}\", fontsize=14)\n",
    "                plt.xlabel(feature, fontsize=12)\n",
    "                plt.ylabel(\"Frequency\", fontsize=12)\n",
    "                plt.xticks(fontsize=10)\n",
    "                plt.yticks(fontsize=10)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save the plot\n",
    "                file_path = os.path.join(output_dir, f\"{feature}.png\")\n",
    "                plt.savefig(file_path, dpi=600.00)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting feature {feature}: {e}\")\n",
    "    \n",
    "    def data_distribution(self):\n",
    "        \"\"\"\n",
    "        Generate and save histograms for the non-categorical features.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Data is not loaded.\")\n",
    "            return\n",
    "        \n",
    "        output_dir = \"../output/plots/distribution\"\n",
    "        non_categorical_features = self.data.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "        self.plot_distribution(non_categorical_features, output_dir, title_prefix=\"Original Data: \")\n",
    "        print(\"\\nHistograms for original data saved successfully.\")\n",
    "\n",
    "    \n",
    "    def correlation_heatmap(self):\n",
    "        \"\"\"Create a correlation matrix heatmap.\"\"\"\n",
    "        corr_path = '../output/plots/correlation'\n",
    "        os.makedirs(corr_path, exist_ok=True)\n",
    "        \n",
    "        corr_matrix = self.data.drop(['Image_ID'], axis=1)\n",
    "        corr_matrix = corr_matrix.corr(numeric_only=True)\n",
    "        plt.figure(figsize=(16,10), dpi=600.00)\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='inferno', square=True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fname=f'{corr_path}/corr_matrix.png', dpi=600.00)\n",
    "        plt.close()\n",
    "        print('\\nCorrelation matrix saved successfully')\n",
    "        \n",
    "    \n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Split the data into training and validation sets.\"\"\"\n",
    "        try:\n",
    "            self.train_set, self.val_set = train_test_split(self.data, test_size=test_size, random_state=random_state)\n",
    "            print(\"\\nData split into training and validation sets.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting data: {e}\")\n",
    "    \n",
    "    def handle_missing_values(self, strategy=\"mean\"):\n",
    "        \"\"\"Handle missing values using the specified strategy.\"\"\"\n",
    "        if self.data is not None:\n",
    "            imputer = SimpleImputer(strategy=strategy)\n",
    "            numeric_cols = self.data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "            self.data[numeric_cols] = imputer.fit_transform(self.data[numeric_cols])\n",
    "            print(\"\\nMissing values handled.\")\n",
    "        else:\n",
    "            print(\"Data is not loaded.\")\n",
    "\n",
    "    def detect_outliers(self, columns=None, iqr_factor=1.5):\n",
    "        \"\"\"Detect outliers using the IQR method.\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "        \n",
    "        outliers = {}\n",
    "        Q1 = self.data[columns].quantile(0.25)\n",
    "        Q3 = self.data[columns].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_factor * IQR\n",
    "        upper_bound = Q3 + iqr_factor * IQR\n",
    "        \n",
    "        for column in columns:\n",
    "            outliers[column] = {\n",
    "                \"lower_bound\": lower_bound[column],\n",
    "                \"upper_bound\": upper_bound[column],\n",
    "                \"outliers\": self.data[column][(self.data[column] < lower_bound[column]) | (self.data[column] > upper_bound[column])].count()\n",
    "            }\n",
    "        \n",
    "        return outliers\n",
    "\n",
    "    def detect_skewed_cols(self, columns=None, skew_threshold=0.5):\n",
    "        \"\"\"Detect skewed columns based on the skew threshold.\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "        skewness = self.data[columns].skew()\n",
    "        return skewness[skewness > skew_threshold]\n",
    "\n",
    "    def detect_clip_cols(self, columns, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        \"\"\"Detect columns needing clipping based on percentiles.\"\"\"\n",
    "        lower_limits = self.data[columns].quantile(lower_percentile)\n",
    "        upper_limits = self.data[columns].quantile(upper_percentile)\n",
    "\n",
    "        clip_cols = {}\n",
    "        for col in columns:\n",
    "            outliers = self.data[col][(self.data[col] < lower_limits[col]) | (self.data[col] > upper_limits[col])].count()\n",
    "            clip_cols[col] = {\n",
    "                \"lower_limit\": lower_limits[col],\n",
    "                \"upper_limit\": upper_limits[col],\n",
    "                \"outliers\": outliers\n",
    "            }\n",
    "        return clip_cols\n",
    "\n",
    "    def auto_handle_outliers(self, numerical_columns):\n",
    "        \"\"\"Automatically handle outliers in the dataset.\"\"\"\n",
    "        outliers = self.detect_outliers(columns=numerical_columns)\n",
    "        cols_to_impute = [col for col, stats in outliers.items() if stats[\"outliers\"] > 0]\n",
    "\n",
    "        skewed_cols = self.detect_skewed_cols(columns=numerical_columns)\n",
    "        cols_to_transform = skewed_cols.index.tolist()\n",
    "\n",
    "        clip_cols = self.detect_clip_cols(numerical_columns)\n",
    "        cols_to_clip = [col for col, stats in clip_cols.items() if stats[\"outliers\"] > 0]\n",
    "\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        self.data[cols_to_impute] = imputer.fit_transform(self.data[cols_to_impute])\n",
    "\n",
    "        for col in cols_to_transform:\n",
    "            self.data[col] = np.log1p(self.data[col])\n",
    "\n",
    "        for col in cols_to_clip:\n",
    "            lower_limit = self.data[col].quantile(0.01)\n",
    "            upper_limit = self.data[col].quantile(0.99)\n",
    "            self.data[col] = self.data[col].clip(lower=lower_limit, upper=upper_limit)\n",
    "\n",
    "        return cols_to_impute, cols_to_transform, cols_to_clip\n",
    "\n",
    "    def separate_obs_data(self, data):\n",
    "        \"\"\"Separate observation and feature data.\"\"\"\n",
    "        self.obs_data_columns = [\n",
    "            \"GroupID\", \"SnapNum\", \"Redshift\", \"RandomNumber\", \"GalaxyID\", \"DescendantID\",\n",
    "            \"LastProgID\", \"TopLeafID\", \"GroupNumber\", \"SubGroupNumber\", \"nodeIndex\",\n",
    "            \"PosInFile\", \"FileNum\", \"Image_ID\"\n",
    "        ]\n",
    "        self.obs_data = data[self.obs_data_columns]\n",
    "        self.features = data.drop(columns=self.obs_data_columns)\n",
    "        return self.obs_data, self.features\n",
    "\n",
    "    def scale_data(self, scaler_type=\"standard\"):\n",
    "        \"\"\"Scale numeric features using the specified scaler type.\"\"\"\n",
    "        scaler = {\n",
    "            \"standard\": StandardScaler(),\n",
    "            \"robust\": RobustScaler(),\n",
    "            \"minmax\": MinMaxScaler()\n",
    "        }.get(scaler_type)\n",
    "        if scaler is None:\n",
    "            raise ValueError(f\"Invalid scaler type: {scaler_type}. Choose from 'standard', 'robust', or 'minmax'.\")\n",
    "        \n",
    "        self.train_data_sc, self.train_features = self.separate_obs_data(self.train_set)\n",
    "        self.val_data_sc, self.val_features = self.separate_obs_data(self.val_set)\n",
    "        \n",
    "        self.train_data_sc = pd.DataFrame(scaler.fit_transform(self.train_features), columns=self.train_features.columns)\n",
    "        self.val_data_sc = pd.DataFrame(scaler.transform(self.val_features), columns=self.val_features.columns)\n",
    "\n",
    "        print(f\"\\nData scaled using {scaler_type} scaler.\")\n",
    "        return self.train_data_sc, self.val_data_sc\n",
    "\n",
    "    def normalize_data(self):\n",
    "        normalizer =  Normalizer()\n",
    "        self.train_data_sc_dn = pd.DataFrame(normalizer.fit_transform(self.train_data_sc), columns=self.train_features.columns)\n",
    "        self.val_data_sc_dn = pd.DataFrame(normalizer.transform(self.val_data_sc), columns=self.val_features.columns)\n",
    "        print(\"\\nData have been successfully normalized.\")\n",
    "        return self.train_data_sc_dn, self.val_data_sc_dn\n",
    "    \n",
    "    def integrate_data(self):\n",
    "        \"\"\"Integrate observation data and processed features.\"\"\"\n",
    "        # Extract observation data for training\n",
    "        try:\n",
    "            train_obs_data = self.train_set[self.obs_data_columns]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Missing columns in train_data: {e}\")\n",
    "\n",
    "        # Combine observation data with scaled and normalized features for training\n",
    "        self.train_data_integrate = pd.concat(\n",
    "            [\n",
    "                train_obs_data.reset_index(drop=True),\n",
    "                pd.DataFrame(self.train_data_sc_dn[self.train_features.columns].values, columns=self.train_features.columns)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Extract observation data for validation\n",
    "        try:\n",
    "            val_obs_data = self.val_set[self.obs_data_columns]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Missing columns in val_data: {e}\")\n",
    "\n",
    "        # Combine observation data with scaled and normalized features for validation\n",
    "        self.val_data_integrate = pd.concat(\n",
    "            [\n",
    "                val_obs_data.reset_index(drop=True),\n",
    "                pd.DataFrame(self.val_data_sc_dn[self.val_features.columns].values, columns=self.val_features.columns)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.train_data = self.train_data_integrate\n",
    "        self.val_data = self.val_data_integrate\n",
    "\n",
    "        return self.train_data, self.val_data\n",
    "\n",
    "    '''def final_clip(self, coverage=0.85):\n",
    "        \"\"\"Clip data to remove extreme outliers using sigma clipping.\"\"\"\n",
    "        # Compute tail probabilities for clipping\n",
    "        tail_prob = (1 - coverage) / 2\n",
    "        lower_percentile = tail_prob\n",
    "        upper_percentile = 1 - tail_prob\n",
    "\n",
    "        # Iterate over each feature to clip\n",
    "        for col in self.train_features.columns:\n",
    "            # Perform sigma clipping for train and validation data\n",
    "            clipped_train, _, _ = sigmaclip(self.train_data_sc_dn[col], lower_percentile, upper_percentile)\n",
    "            clipped_val, _, _ = sigmaclip(self.val_data_sc_dn[col], lower_percentile, upper_percentile)\n",
    "\n",
    "            # Ensure the clipped data is not None\n",
    "            if clipped_train is None or clipped_val is None:\n",
    "                raise ValueError(f\"Sigma clipping failed for column: {col}\")\n",
    "\n",
    "            # Update the columns in the train and validation data\n",
    "            self.train_data_sc_dn[col] = pd.Series(\n",
    "                clipped_train, index=self.train_data_sc_dn.index[:len(clipped_train)]\n",
    "            )\n",
    "            self.val_data_sc_dn[col] = pd.Series(\n",
    "                clipped_val, index=self.val_data_sc_dn.index[:len(clipped_val)]\n",
    "            )\n",
    "\n",
    "        self.train_data = self.train_data_sc_dn\n",
    "        self.val_data = self.val_data_sc_dn\n",
    "        \n",
    "        print(f\"Data have been successfully clipped by {coverage*100}%.\")\n",
    "        return self.train_data, self.val_data'''\n",
    "\n",
    "\n",
    "    def preprocess_data(self, scaler_type=\"standard\", coverage=0.85):\n",
    "        \"\"\"Run the full preprocessing pipeline.\"\"\"\n",
    "        self.handle_missing_values()\n",
    "        non_categorical_features = self.data.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "        self.auto_handle_outliers(non_categorical_features)\n",
    "        self.split_data()\n",
    "        self.scale_data(scaler_type)\n",
    "        self.normalize_data()\n",
    "        self.integrate_data()\n",
    "        #self.final_clip(coverage)\n",
    "        print('\\nData have been preprocessed')\n",
    "        return self.train_data, self.val_data\n",
    "    \n",
    "    def preprocessed_distribution(self):\n",
    "        \"\"\"\n",
    "        Generate and save histograms for preprocessed training data.\n",
    "        \"\"\"\n",
    "        if self.train_data is None:\n",
    "            print(\"Training data is not available.\")\n",
    "            return\n",
    "        \n",
    "        output_dir = \"../output/plots/preprocessed\"\n",
    "        numeric_features = self.train_data.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "        self.plot_distribution(numeric_features, output_dir, title_prefix=\"Preprocessed Data: \")\n",
    "        print(\"\\nHistograms for preprocessed data saved successfully.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "\n",
      "Data statistics\n",
      "                               count          mean           std  \\\n",
      "GroupID                   22109109.0  1.109590e+13  4.515858e+12   \n",
      "SnapNum                   22109109.0  1.109115e+01  4.516415e+00   \n",
      "Redshift                  22109109.0  3.864350e+00  2.210902e+00   \n",
      "RandomNumber              22109109.0  5.002389e-01  2.886437e-01   \n",
      "GroupMass                 22109109.0  5.599906e+11  4.779501e+12   \n",
      "GroupCentreOfPotential_x  22109109.0  5.138570e+01  2.903008e+01   \n",
      "GroupCentreOfPotential_y  22109109.0  5.234821e+01  2.848542e+01   \n",
      "GroupCentreOfPotential_z  22109109.0  4.884773e+01  2.709691e+01   \n",
      "NumOfSubhalos             22109109.0  2.183267e+01  1.572136e+02   \n",
      "Group_M_Crit200           22109109.0  4.501973e+11  3.809841e+12   \n",
      "Group_R_Crit200           22109109.0  2.144501e+01  5.143914e+01   \n",
      "Group_M_Mean200           22109109.0  4.774301e+11  4.058466e+12   \n",
      "Group_R_Mean200           22109109.0  2.268119e+01  5.591853e+01   \n",
      "Group_M_TopHat200         22109109.0  4.823987e+11  4.071832e+12   \n",
      "Group_R_TopHat200         22109109.0  2.328414e+01  5.644981e+01   \n",
      "Group_M_Crit500           22109109.0  3.047619e+11  2.705036e+12   \n",
      "Group_R_Crit500           22109109.0  1.361948e+01  3.328524e+01   \n",
      "Group_M_Mean500           22109109.0  3.306447e+11  2.962525e+12   \n",
      "Group_R_Mean500           22109109.0  1.447645e+01  3.646680e+01   \n",
      "Group_M_Crit2500          22109109.0  1.105710e+11  9.956899e+11   \n",
      "Group_R_Crit2500          22109109.0  5.420620e+00  1.408208e+01   \n",
      "Group_R_Mean2500          22109109.0  5.842958e+00  1.564683e+01   \n",
      "Group_M_Mean2500          22109109.0  1.257631e+11  1.186810e+12   \n",
      "GalaxyID                  22109109.0  1.469286e+07  8.352476e+06   \n",
      "DescendantID              22109109.0  1.466984e+07  8.391431e+06   \n",
      "LastProgID                22109109.0  1.469286e+07  8.352477e+06   \n",
      "TopLeafID                 22109109.0  1.469286e+07  8.352477e+06   \n",
      "GroupNumber               22109109.0  9.590249e+05  8.821989e+05   \n",
      "SubGroupNumber            22109109.0  1.042967e+01  9.103253e+01   \n",
      "CentreOfMass_x            22109109.0  5.138519e+01  2.902999e+01   \n",
      "CentreOfMass_y            22109109.0  5.234957e+01  2.848493e+01   \n",
      "CentreOfMass_z            22109109.0  4.884899e+01  2.709676e+01   \n",
      "CentreOfPotential_x       22109109.0  5.138531e+01  2.902999e+01   \n",
      "CentreOfPotential_y       22109109.0  5.234965e+01  2.848492e+01   \n",
      "CentreOfPotential_z       22109109.0  4.884906e+01  2.709677e+01   \n",
      "Velocity_x                22109109.0  2.328064e-01  1.645607e+02   \n",
      "Velocity_y                22109109.0  5.234919e-02  1.640220e+02   \n",
      "Velocity_z                22109109.0  1.515064e-01  1.773089e+02   \n",
      "KineticEnergy             22109109.0  1.021710e+14  2.346293e+16   \n",
      "MechanicalEnergy          22109109.0 -4.656770e+13  8.539926e+15   \n",
      "TotalEnergy               22109109.0 -4.656770e+13  8.539926e+15   \n",
      "Vmax                      22109109.0  2.635207e+01  1.483264e+01   \n",
      "VmaxRadius                22109109.0  4.158438e+00  3.249476e+00   \n",
      "Mass                      22109109.0  3.669193e+09  9.381179e+10   \n",
      "MassType_DM               22109109.0  3.669193e+09  9.381179e+10   \n",
      "HalfMassProjRad_DM        22109109.0  3.033581e+00  2.201281e+00   \n",
      "HalfMassRad_DM            22109109.0  4.048366e+00  2.971064e+00   \n",
      "nodeIndex                 22109109.0  1.109685e+13  4.516109e+12   \n",
      "PosInFile                 22109109.0  1.145447e+04  7.513777e+03   \n",
      "FileNum                   22109109.0  5.702631e+01  3.905164e+01   \n",
      "Image_ID                  22109109.0 -1.000000e+00  0.000000e+00   \n",
      "\n",
      "                                   min           25%           50%  \\\n",
      "GroupID                   0.000000e+00  8.000000e+12  1.100550e+13   \n",
      "SnapNum                   0.000000e+00  8.000000e+00  1.100000e+01   \n",
      "Redshift                  1.004122e+00  2.012410e+00  3.527976e+00   \n",
      "RandomNumber              2.250078e-07  2.504168e-01  5.006754e-01   \n",
      "GroupMass                 3.680960e+08  5.521440e+08  1.058276e+09   \n",
      "GroupCentreOfPotential_x  5.741259e-06  2.504735e+01  5.349097e+01   \n",
      "GroupCentreOfPotential_y  7.315047e-06  2.975148e+01  5.401611e+01   \n",
      "GroupCentreOfPotential_z  4.030872e-06  2.714247e+01  4.805387e+01   \n",
      "NumOfSubhalos             1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "Group_M_Crit200           0.000000e+00  3.795990e+08  7.707009e+08   \n",
      "Group_R_Crit200           0.000000e+00  4.196827e+00  6.975633e+00   \n",
      "Group_M_Mean200           0.000000e+00  3.795990e+08  7.822040e+08   \n",
      "Group_R_Mean200           0.000000e+00  4.232388e+00  7.132559e+00   \n",
      "Group_M_TopHat200         0.000000e+00  4.026050e+08  8.167130e+08   \n",
      "Group_R_TopHat200         0.000000e+00  4.486248e+00  7.478053e+00   \n",
      "Group_M_Crit500           0.000000e+00  2.070540e+08  4.831260e+08   \n",
      "Group_R_Crit500           0.000000e+00  2.471770e+00  4.319969e+00   \n",
      "Group_M_Mean500           0.000000e+00  2.185570e+08  4.946290e+08   \n",
      "Group_R_Mean500           0.000000e+00  2.489134e+00  4.429305e+00   \n",
      "Group_M_Crit2500          0.000000e+00  0.000000e+00  1.265330e+08   \n",
      "Group_R_Crit2500          0.000000e+00  0.000000e+00  1.542554e+00   \n",
      "Group_R_Mean2500          0.000000e+00  0.000000e+00  1.587637e+00   \n",
      "Group_M_Mean2500          0.000000e+00  0.000000e+00  1.380360e+08   \n",
      "GalaxyID                  1.000000e+00  7.597679e+06  1.476928e+07   \n",
      "DescendantID             -1.000000e+00  7.597665e+06  1.476928e+07   \n",
      "LastProgID                1.000000e+00  7.597695e+06  1.476929e+07   \n",
      "TopLeafID                 1.000000e+00  7.597679e+06  1.476929e+07   \n",
      "GroupNumber               1.000000e+00  1.389720e+05  7.264840e+05   \n",
      "SubGroupNumber            0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "CentreOfMass_x            8.737245e-06  2.504911e+01  5.350601e+01   \n",
      "CentreOfMass_y            1.347567e-06  2.974966e+01  5.401327e+01   \n",
      "CentreOfMass_z            2.319093e-06  2.714315e+01  4.805469e+01   \n",
      "CentreOfPotential_x       5.741259e-06  2.504933e+01  5.350597e+01   \n",
      "CentreOfPotential_y       2.781347e-06  2.974986e+01  5.401319e+01   \n",
      "CentreOfPotential_z       4.030872e-06  2.714316e+01  4.805463e+01   \n",
      "Velocity_x               -1.921943e+03 -1.004202e+02  1.001791e+00   \n",
      "Velocity_y               -1.889517e+03 -9.138410e+01  3.338599e+00   \n",
      "Velocity_z               -1.985454e+03 -1.026157e+02  4.712578e-01   \n",
      "KineticEnergy             4.040274e+09  6.572192e+10  1.537140e+11   \n",
      "MechanicalEnergy         -1.534958e+19 -3.150803e+11 -9.064547e+10   \n",
      "TotalEnergy              -1.534958e+19 -3.150803e+11 -9.064547e+10   \n",
      "Vmax                      4.514020e+00  1.824998e+01  2.280985e+01   \n",
      "VmaxRadius                1.745217e-01  2.414129e+00  3.528735e+00   \n",
      "Mass                      2.300600e+08  4.026050e+08  6.211619e+08   \n",
      "MassType_DM               2.300600e+08  4.026050e+08  6.211619e+08   \n",
      "HalfMassProjRad_DM        1.972048e-01  1.898952e+00  2.603003e+00   \n",
      "HalfMassRad_DM            2.634832e-01  2.516514e+00  3.460878e+00   \n",
      "nodeIndex                 0.000000e+00  8.000100e+12  1.100680e+13   \n",
      "PosInFile                 0.000000e+00  4.880000e+03  1.065300e+04   \n",
      "FileNum                   0.000000e+00  2.100000e+01  5.500000e+01   \n",
      "Image_ID                 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
      "\n",
      "                                   75%           max     Skewness  \\\n",
      "GroupID                   1.500050e+13  1.900950e+13    -0.049538   \n",
      "SnapNum                   1.500000e+01  1.900000e+01    -0.049464   \n",
      "Redshift                  5.037237e+00  2.000002e+01     0.881842   \n",
      "RandomNumber              7.500763e-01  1.000000e+00    -0.001067   \n",
      "GroupMass                 5.107332e+09  1.431740e+14    17.513733   \n",
      "GroupCentreOfPotential_x  7.668488e+01  9.999999e+01    -0.147040   \n",
      "GroupCentreOfPotential_y  7.733022e+01  9.999999e+01    -0.166186   \n",
      "GroupCentreOfPotential_z  6.840163e+01  9.999999e+01     0.075855   \n",
      "NumOfSubhalos             2.000000e+00  4.424000e+03    15.856437   \n",
      "Group_M_Crit200           3.726972e+09  1.300364e+14    18.290518   \n",
      "Group_R_Crit200           1.264079e+01  7.273484e+02     5.643529   \n",
      "Group_M_Mean200           3.807493e+09  1.355612e+14    18.200108   \n",
      "Group_R_Mean200           1.322306e+01  8.008917e+02     5.805270   \n",
      "Group_M_TopHat200         3.980038e+09  1.353272e+14    18.054476   \n",
      "Group_R_TopHat200         1.365380e+01  7.976679e+02     5.707467   \n",
      "Group_M_Crit500           2.358115e+09  1.055352e+14    20.856383   \n",
      "Group_R_Crit500           8.166206e+00  4.999056e+02     5.734749   \n",
      "Group_M_Mean500           2.415630e+09  1.141353e+14    20.724151   \n",
      "Group_R_Mean500           8.558765e+00  5.571914e+02     5.930217   \n",
      "Group_M_Crit2500          8.167130e+08  4.136472e+13    23.116690   \n",
      "Group_R_Crit2500          3.494565e+00  2.139493e+02     5.563409   \n",
      "Group_R_Mean2500          3.673078e+00  2.508847e+02     5.855025   \n",
      "Group_M_Mean2500          8.512220e+08  5.209283e+13    25.051112   \n",
      "GalaxyID                  2.190046e+07  2.903905e+07    -0.035528   \n",
      "DescendantID              2.190046e+07  2.903905e+07    -0.050371   \n",
      "LastProgID                2.190046e+07  2.903906e+07    -0.035528   \n",
      "TopLeafID                 2.190046e+07  2.903906e+07    -0.035528   \n",
      "GroupNumber               1.628968e+06  3.102151e+06     0.650567   \n",
      "SubGroupNumber            0.000000e+00  4.423000e+03    20.620330   \n",
      "CentreOfMass_x            7.668815e+01  9.999999e+01    -0.147038   \n",
      "CentreOfMass_y            7.732618e+01  9.999997e+01    -0.166152   \n",
      "CentreOfMass_z            6.840122e+01  9.999999e+01     0.075845   \n",
      "CentreOfPotential_x       7.668816e+01  9.999999e+01    -0.147038   \n",
      "CentreOfPotential_y       7.732620e+01  9.999999e+01    -0.166152   \n",
      "CentreOfPotential_z       6.840125e+01  9.999999e+01     0.075846   \n",
      "Velocity_x                9.867443e+01  1.961115e+03     0.024259   \n",
      "Velocity_y                9.484699e+01  1.835720e+03    -0.153493   \n",
      "Velocity_z                1.041377e+02  2.187959e+03    -0.059887   \n",
      "KineticEnergy             5.306265e+11  6.452252e+19  1415.162505   \n",
      "MechanicalEnergy         -3.917565e+10  3.794829e+13 -1020.630077   \n",
      "TotalEnergy              -3.917565e+10  3.794829e+13 -1020.630077   \n",
      "Vmax                      2.964250e+01  9.183386e+02     5.255936   \n",
      "VmaxRadius                5.054914e+00  4.350563e+02    10.199067   \n",
      "Mass                      1.276833e+09  1.047125e+14   318.910193   \n",
      "MassType_DM               1.276833e+09  1.047125e+14   318.910193   \n",
      "HalfMassProjRad_DM        3.580783e+00  2.638247e+02    10.142411   \n",
      "HalfMassRad_DM            4.782349e+00  3.503752e+02    10.038993   \n",
      "nodeIndex                 1.500220e+13  1.901050e+13    -0.049567   \n",
      "PosInFile                 1.741600e+04  2.962400e+04     0.304657   \n",
      "FileNum                   9.100000e+01  1.270000e+02     0.155089   \n",
      "Image_ID                 -1.000000e+00 -1.000000e+00     0.000000   \n",
      "\n",
      "                              Kurtosis  \n",
      "GroupID                  -9.658902e-01  \n",
      "SnapNum                  -9.657985e-01  \n",
      "Redshift                  4.066718e-01  \n",
      "RandomNumber             -1.199266e+00  \n",
      "GroupMass                 3.887612e+02  \n",
      "GroupCentreOfPotential_x -1.236996e+00  \n",
      "GroupCentreOfPotential_y -1.159673e+00  \n",
      "GroupCentreOfPotential_z -9.331462e-01  \n",
      "NumOfSubhalos             3.201785e+02  \n",
      "Group_M_Crit200           4.457826e+02  \n",
      "Group_R_Crit200           4.169168e+01  \n",
      "Group_M_Mean200           4.370801e+02  \n",
      "Group_R_Mean200           4.424572e+01  \n",
      "Group_M_TopHat200         4.301523e+02  \n",
      "Group_R_TopHat200         4.265161e+01  \n",
      "Group_M_Crit500           6.060428e+02  \n",
      "Group_R_Crit500           4.375738e+01  \n",
      "Group_M_Mean500           5.921439e+02  \n",
      "Group_R_Mean500           4.693497e+01  \n",
      "Group_M_Crit2500          7.492486e+02  \n",
      "Group_R_Crit2500          4.170474e+01  \n",
      "Group_R_Mean2500          4.684125e+01  \n",
      "Group_M_Mean2500          8.808422e+02  \n",
      "GalaxyID                 -1.166340e+00  \n",
      "DescendantID             -1.147498e+00  \n",
      "LastProgID               -1.166340e+00  \n",
      "TopLeafID                -1.166340e+00  \n",
      "GroupNumber              -7.843759e-01  \n",
      "SubGroupNumber            5.785431e+02  \n",
      "CentreOfMass_x           -1.236994e+00  \n",
      "CentreOfMass_y           -1.159698e+00  \n",
      "CentreOfMass_z           -9.331356e-01  \n",
      "CentreOfPotential_x      -1.236993e+00  \n",
      "CentreOfPotential_y      -1.159697e+00  \n",
      "CentreOfPotential_z      -9.331357e-01  \n",
      "Velocity_x                1.993836e+00  \n",
      "Velocity_y                2.595438e+00  \n",
      "Velocity_z                2.323831e+00  \n",
      "KineticEnergy             3.045854e+06  \n",
      "MechanicalEnergy          1.465375e+06  \n",
      "TotalEnergy               1.465375e+06  \n",
      "Vmax                      6.511765e+01  \n",
      "VmaxRadius                3.447037e+02  \n",
      "Mass                      2.038103e+05  \n",
      "MassType_DM               2.038103e+05  \n",
      "HalfMassProjRad_DM        2.999946e+02  \n",
      "HalfMassRad_DM            2.911225e+02  \n",
      "nodeIndex                -9.658906e-01  \n",
      "PosInFile                -1.028833e+00  \n",
      "FileNum                  -1.267222e+00  \n",
      "Image_ID                  0.000000e+00  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prefix = 'merge'\n",
    "suffix =  ['L0100N1504']\n",
    "for i in range(len(suffix)):\n",
    "    filename = f'../../data/{prefix}_{suffix[i]}.feather'\n",
    "    dp = DataProcessor(data_path=filename)\n",
    "    dp.load_data()\n",
    "    dp.missing_values()\n",
    "    dp.basic_statistics()\n",
    "    dp.data_distribution()\n",
    "    dp.correlation_heatmap()\n",
    "    train_data, val_data = dp.preprocess_data(scaler_type=\"robust\", coverage=0.85)\n",
    "    dp.preprocessed_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>SnapNum</th>\n",
       "      <th>Redshift</th>\n",
       "      <th>RandomNumber</th>\n",
       "      <th>GalaxyID</th>\n",
       "      <th>DescendantID</th>\n",
       "      <th>LastProgID</th>\n",
       "      <th>TopLeafID</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>SubGroupNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>Velocity_z</th>\n",
       "      <th>KineticEnergy</th>\n",
       "      <th>MechanicalEnergy</th>\n",
       "      <th>TotalEnergy</th>\n",
       "      <th>Vmax</th>\n",
       "      <th>VmaxRadius</th>\n",
       "      <th>Mass</th>\n",
       "      <th>MassType_DM</th>\n",
       "      <th>HalfMassProjRad_DM</th>\n",
       "      <th>HalfMassRad_DM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.600450e+13</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.006850</td>\n",
       "      <td>0.286341</td>\n",
       "      <td>521691.0</td>\n",
       "      <td>521690.0</td>\n",
       "      <td>521692.0</td>\n",
       "      <td>521692.0</td>\n",
       "      <td>26958.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248569</td>\n",
       "      <td>-0.023669</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>-0.022474</td>\n",
       "      <td>-0.085861</td>\n",
       "      <td>-0.085861</td>\n",
       "      <td>-0.233358</td>\n",
       "      <td>-0.242843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.100980e+13</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.551370</td>\n",
       "      <td>0.961817</td>\n",
       "      <td>769339.0</td>\n",
       "      <td>769338.0</td>\n",
       "      <td>769358.0</td>\n",
       "      <td>769358.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024592</td>\n",
       "      <td>0.160463</td>\n",
       "      <td>-0.301631</td>\n",
       "      <td>-0.301631</td>\n",
       "      <td>0.167656</td>\n",
       "      <td>-0.259053</td>\n",
       "      <td>0.238621</td>\n",
       "      <td>0.238621</td>\n",
       "      <td>0.181598</td>\n",
       "      <td>0.176915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.021100e+12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.085618</td>\n",
       "      <td>0.745820</td>\n",
       "      <td>55630.0</td>\n",
       "      <td>44014.0</td>\n",
       "      <td>55637.0</td>\n",
       "      <td>55637.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.653960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161347</td>\n",
       "      <td>-0.086250</td>\n",
       "      <td>0.024818</td>\n",
       "      <td>0.024818</td>\n",
       "      <td>-0.113568</td>\n",
       "      <td>0.140645</td>\n",
       "      <td>-0.069746</td>\n",
       "      <td>-0.069746</td>\n",
       "      <td>0.082617</td>\n",
       "      <td>0.095744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.602170e+13</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.006850</td>\n",
       "      <td>0.178628</td>\n",
       "      <td>554995.0</td>\n",
       "      <td>554994.0</td>\n",
       "      <td>554997.0</td>\n",
       "      <td>554997.0</td>\n",
       "      <td>27995.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191987</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>0.062398</td>\n",
       "      <td>0.062398</td>\n",
       "      <td>-0.015111</td>\n",
       "      <td>-0.046603</td>\n",
       "      <td>-0.033633</td>\n",
       "      <td>-0.033633</td>\n",
       "      <td>-0.138668</td>\n",
       "      <td>-0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.700060e+13</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.898629</td>\n",
       "      <td>1007655.0</td>\n",
       "      <td>1007654.0</td>\n",
       "      <td>1007657.0</td>\n",
       "      <td>1007657.0</td>\n",
       "      <td>31522.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107055</td>\n",
       "      <td>-0.006099</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>-0.010767</td>\n",
       "      <td>-0.069218</td>\n",
       "      <td>-0.016149</td>\n",
       "      <td>-0.016149</td>\n",
       "      <td>-0.045225</td>\n",
       "      <td>-0.039401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989292</th>\n",
       "      <td>7.011800e+12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.869864</td>\n",
       "      <td>0.846827</td>\n",
       "      <td>110269.0</td>\n",
       "      <td>110268.0</td>\n",
       "      <td>110281.0</td>\n",
       "      <td>110281.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.584967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149101</td>\n",
       "      <td>-0.049523</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>-0.021734</td>\n",
       "      <td>-0.017178</td>\n",
       "      <td>-0.025604</td>\n",
       "      <td>-0.025604</td>\n",
       "      <td>-0.052263</td>\n",
       "      <td>-0.089542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989293</th>\n",
       "      <td>1.002420e+13</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.606165</td>\n",
       "      <td>0.834165</td>\n",
       "      <td>259179.0</td>\n",
       "      <td>259178.0</td>\n",
       "      <td>259187.0</td>\n",
       "      <td>259187.0</td>\n",
       "      <td>27103.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065746</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>-0.118938</td>\n",
       "      <td>0.045532</td>\n",
       "      <td>-0.038059</td>\n",
       "      <td>-0.038059</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>0.076985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989294</th>\n",
       "      <td>8.001100e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.797946</td>\n",
       "      <td>0.229255</td>\n",
       "      <td>131933.0</td>\n",
       "      <td>131932.0</td>\n",
       "      <td>131933.0</td>\n",
       "      <td>131933.0</td>\n",
       "      <td>20247.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011174</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>0.104640</td>\n",
       "      <td>-0.298039</td>\n",
       "      <td>-0.096150</td>\n",
       "      <td>-0.096150</td>\n",
       "      <td>-0.309351</td>\n",
       "      <td>-0.300594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989295</th>\n",
       "      <td>1.900620e+13</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.695206</td>\n",
       "      <td>0.395208</td>\n",
       "      <td>671156.0</td>\n",
       "      <td>671155.0</td>\n",
       "      <td>671156.0</td>\n",
       "      <td>671156.0</td>\n",
       "      <td>23249.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054240</td>\n",
       "      <td>0.028138</td>\n",
       "      <td>0.030698</td>\n",
       "      <td>0.030698</td>\n",
       "      <td>0.113055</td>\n",
       "      <td>-0.148162</td>\n",
       "      <td>-0.077641</td>\n",
       "      <td>-0.077641</td>\n",
       "      <td>-0.299303</td>\n",
       "      <td>-0.292034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989296</th>\n",
       "      <td>7.019900e+12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.869864</td>\n",
       "      <td>0.758966</td>\n",
       "      <td>121959.0</td>\n",
       "      <td>121958.0</td>\n",
       "      <td>122010.0</td>\n",
       "      <td>121979.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144073</td>\n",
       "      <td>0.075968</td>\n",
       "      <td>-0.372832</td>\n",
       "      <td>-0.372832</td>\n",
       "      <td>0.100403</td>\n",
       "      <td>-0.052175</td>\n",
       "      <td>0.070580</td>\n",
       "      <td>0.070580</td>\n",
       "      <td>-0.051223</td>\n",
       "      <td>-0.049176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989297 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             GroupID  SnapNum  Redshift  RandomNumber   GalaxyID  \\\n",
       "0       1.600450e+13     16.0  1.006850      0.286341   521691.0   \n",
       "1       2.100980e+13     21.0  0.551370      0.961817   769339.0   \n",
       "2       5.021100e+12      5.0  2.085618      0.745820    55630.0   \n",
       "3       1.602170e+13     16.0  1.006850      0.178628   554995.0   \n",
       "4       2.700060e+13     27.0  0.095891      0.898629  1007655.0   \n",
       "...              ...      ...       ...           ...        ...   \n",
       "989292  7.011800e+12      7.0  1.869864      0.846827   110269.0   \n",
       "989293  1.002420e+13     10.0  1.606165      0.834165   259179.0   \n",
       "989294  8.001100e+12      8.0  1.797946      0.229255   131933.0   \n",
       "989295  1.900620e+13     19.0  0.695206      0.395208   671156.0   \n",
       "989296  7.019900e+12      7.0  1.869864      0.758966   121959.0   \n",
       "\n",
       "        DescendantID  LastProgID  TopLeafID  GroupNumber  SubGroupNumber  ...  \\\n",
       "0           521690.0    521692.0   521692.0      26958.0        0.000000  ...   \n",
       "1           769338.0    769358.0   769358.0         19.0        3.367296  ...   \n",
       "2            44014.0     55637.0    55637.0          3.0        4.653960  ...   \n",
       "3           554994.0    554997.0   554997.0      27995.0        0.000000  ...   \n",
       "4          1007654.0   1007657.0  1007657.0      31522.0        0.000000  ...   \n",
       "...              ...         ...        ...          ...             ...  ...   \n",
       "989292      110268.0    110281.0   110281.0          4.0        4.584967  ...   \n",
       "989293      259178.0    259187.0   259187.0      27103.0        0.000000  ...   \n",
       "989294      131932.0    131933.0   131933.0      20247.0        0.000000  ...   \n",
       "989295      671155.0    671156.0   671156.0      23249.0        0.000000  ...   \n",
       "989296      121958.0    122010.0   121979.0          6.0        3.218876  ...   \n",
       "\n",
       "        Velocity_z  KineticEnergy  MechanicalEnergy  TotalEnergy      Vmax  \\\n",
       "0         0.248569      -0.023669          0.017343     0.017343  0.035503   \n",
       "1        -0.024592       0.160463         -0.301631    -0.301631  0.167656   \n",
       "2         0.161347      -0.086250          0.024818     0.024818 -0.113568   \n",
       "3         0.191987       0.021152          0.062398     0.062398 -0.015111   \n",
       "4         0.107055      -0.006099          0.016522     0.016522 -0.010767   \n",
       "...            ...            ...               ...          ...       ...   \n",
       "989292    0.149101      -0.049523          0.010524     0.010524 -0.021734   \n",
       "989293    0.065746      -0.081042          0.022439     0.022439 -0.118938   \n",
       "989294   -0.011174       0.009173          0.011692     0.011692  0.104640   \n",
       "989295   -0.054240       0.028138          0.030698     0.030698  0.113055   \n",
       "989296    0.144073       0.075968         -0.372832    -0.372832  0.100403   \n",
       "\n",
       "        VmaxRadius      Mass  MassType_DM  HalfMassProjRad_DM  HalfMassRad_DM  \n",
       "0        -0.022474 -0.085861    -0.085861           -0.233358       -0.242843  \n",
       "1        -0.259053  0.238621     0.238621            0.181598        0.176915  \n",
       "2         0.140645 -0.069746    -0.069746            0.082617        0.095744  \n",
       "3        -0.046603 -0.033633    -0.033633           -0.138668       -0.155800  \n",
       "4        -0.069218 -0.016149    -0.016149           -0.045225       -0.039401  \n",
       "...            ...       ...          ...                 ...             ...  \n",
       "989292   -0.017178 -0.025604    -0.025604           -0.052263       -0.089542  \n",
       "989293    0.045532 -0.038059    -0.038059            0.079832        0.076985  \n",
       "989294   -0.298039 -0.096150    -0.096150           -0.309351       -0.300594  \n",
       "989295   -0.148162 -0.077641    -0.077641           -0.299303       -0.292034  \n",
       "989296   -0.052175  0.070580     0.070580           -0.051223       -0.049176  \n",
       "\n",
       "[989297 rows x 51 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>SnapNum</th>\n",
       "      <th>Redshift</th>\n",
       "      <th>RandomNumber</th>\n",
       "      <th>GalaxyID</th>\n",
       "      <th>DescendantID</th>\n",
       "      <th>LastProgID</th>\n",
       "      <th>TopLeafID</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>SubGroupNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>Velocity_z</th>\n",
       "      <th>KineticEnergy</th>\n",
       "      <th>MechanicalEnergy</th>\n",
       "      <th>TotalEnergy</th>\n",
       "      <th>Vmax</th>\n",
       "      <th>VmaxRadius</th>\n",
       "      <th>Mass</th>\n",
       "      <th>MassType_DM</th>\n",
       "      <th>HalfMassProjRad_DM</th>\n",
       "      <th>HalfMassRad_DM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.300420e+13</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.246576</td>\n",
       "      <td>0.889542</td>\n",
       "      <td>369409.00</td>\n",
       "      <td>369408.00</td>\n",
       "      <td>369410.00</td>\n",
       "      <td>369410.00</td>\n",
       "      <td>35569.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236024</td>\n",
       "      <td>-0.204067</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>-0.227511</td>\n",
       "      <td>0.049183</td>\n",
       "      <td>-0.211853</td>\n",
       "      <td>-0.211853</td>\n",
       "      <td>-0.093204</td>\n",
       "      <td>-0.116673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.620111e+13</td>\n",
       "      <td>16.188377</td>\n",
       "      <td>1.213387</td>\n",
       "      <td>0.500460</td>\n",
       "      <td>1224255.79</td>\n",
       "      <td>1220855.79</td>\n",
       "      <td>1224259.74</td>\n",
       "      <td>1224259.74</td>\n",
       "      <td>37285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096425</td>\n",
       "      <td>-0.236849</td>\n",
       "      <td>0.059817</td>\n",
       "      <td>0.059817</td>\n",
       "      <td>-0.311868</td>\n",
       "      <td>0.333410</td>\n",
       "      <td>-0.105520</td>\n",
       "      <td>-0.105520</td>\n",
       "      <td>0.260575</td>\n",
       "      <td>0.242065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.400820e+13</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.174658</td>\n",
       "      <td>0.338938</td>\n",
       "      <td>428336.00</td>\n",
       "      <td>428335.00</td>\n",
       "      <td>428348.00</td>\n",
       "      <td>428345.00</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174425</td>\n",
       "      <td>0.195013</td>\n",
       "      <td>-0.582414</td>\n",
       "      <td>-0.582414</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.204440</td>\n",
       "      <td>0.204440</td>\n",
       "      <td>0.023075</td>\n",
       "      <td>0.034475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.601750e+13</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.006850</td>\n",
       "      <td>0.820254</td>\n",
       "      <td>546966.00</td>\n",
       "      <td>546965.00</td>\n",
       "      <td>546971.00</td>\n",
       "      <td>546971.00</td>\n",
       "      <td>11901.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193842</td>\n",
       "      <td>0.053084</td>\n",
       "      <td>-0.049739</td>\n",
       "      <td>-0.049739</td>\n",
       "      <td>-0.054049</td>\n",
       "      <td>0.467709</td>\n",
       "      <td>0.111081</td>\n",
       "      <td>0.111081</td>\n",
       "      <td>0.063972</td>\n",
       "      <td>0.115098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.620111e+13</td>\n",
       "      <td>16.188377</td>\n",
       "      <td>1.213387</td>\n",
       "      <td>0.500460</td>\n",
       "      <td>1130235.00</td>\n",
       "      <td>1130234.00</td>\n",
       "      <td>1130246.00</td>\n",
       "      <td>1130246.00</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504944</td>\n",
       "      <td>0.140580</td>\n",
       "      <td>-0.211823</td>\n",
       "      <td>-0.211823</td>\n",
       "      <td>0.076661</td>\n",
       "      <td>0.177330</td>\n",
       "      <td>0.180916</td>\n",
       "      <td>0.180916</td>\n",
       "      <td>0.091372</td>\n",
       "      <td>0.096988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247320</th>\n",
       "      <td>1.620111e+13</td>\n",
       "      <td>16.188377</td>\n",
       "      <td>1.213387</td>\n",
       "      <td>0.500460</td>\n",
       "      <td>1203642.00</td>\n",
       "      <td>1203641.00</td>\n",
       "      <td>1203644.00</td>\n",
       "      <td>1203644.00</td>\n",
       "      <td>36952.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215147</td>\n",
       "      <td>-0.198709</td>\n",
       "      <td>0.059952</td>\n",
       "      <td>0.059952</td>\n",
       "      <td>-0.311075</td>\n",
       "      <td>0.352225</td>\n",
       "      <td>-0.095216</td>\n",
       "      <td>-0.095216</td>\n",
       "      <td>0.251889</td>\n",
       "      <td>0.260286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247321</th>\n",
       "      <td>5.006000e+12</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.085618</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>41297.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>41299.00</td>\n",
       "      <td>41299.00</td>\n",
       "      <td>34230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071611</td>\n",
       "      <td>-0.082373</td>\n",
       "      <td>0.022851</td>\n",
       "      <td>0.022851</td>\n",
       "      <td>-0.119199</td>\n",
       "      <td>0.121107</td>\n",
       "      <td>-0.038142</td>\n",
       "      <td>-0.038142</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247322</th>\n",
       "      <td>6.002800e+12</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.941782</td>\n",
       "      <td>0.584804</td>\n",
       "      <td>63534.00</td>\n",
       "      <td>63533.00</td>\n",
       "      <td>63535.00</td>\n",
       "      <td>63535.00</td>\n",
       "      <td>17201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010384</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>-0.027674</td>\n",
       "      <td>-0.027674</td>\n",
       "      <td>0.080003</td>\n",
       "      <td>-0.381622</td>\n",
       "      <td>-0.008413</td>\n",
       "      <td>-0.008413</td>\n",
       "      <td>-0.162882</td>\n",
       "      <td>-0.198907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247323</th>\n",
       "      <td>2.102370e+13</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.551370</td>\n",
       "      <td>0.763274</td>\n",
       "      <td>793866.00</td>\n",
       "      <td>793865.00</td>\n",
       "      <td>793895.00</td>\n",
       "      <td>793887.00</td>\n",
       "      <td>2467.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046211</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>-0.691228</td>\n",
       "      <td>-0.691228</td>\n",
       "      <td>0.051547</td>\n",
       "      <td>0.027859</td>\n",
       "      <td>0.079149</td>\n",
       "      <td>0.079149</td>\n",
       "      <td>0.045699</td>\n",
       "      <td>0.044586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247324</th>\n",
       "      <td>1.900720e+13</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.695206</td>\n",
       "      <td>0.931895</td>\n",
       "      <td>672878.00</td>\n",
       "      <td>672877.00</td>\n",
       "      <td>672878.00</td>\n",
       "      <td>672878.00</td>\n",
       "      <td>6426.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037532</td>\n",
       "      <td>-0.063550</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>-0.279787</td>\n",
       "      <td>-0.177228</td>\n",
       "      <td>-0.177228</td>\n",
       "      <td>-0.374580</td>\n",
       "      <td>-0.383580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247325 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             GroupID    SnapNum  Redshift  RandomNumber    GalaxyID  \\\n",
       "0       1.300420e+13  13.000000  1.246576      0.889542   369409.00   \n",
       "1       1.620111e+13  16.188377  1.213387      0.500460  1224255.79   \n",
       "2       1.400820e+13  14.000000  1.174658      0.338938   428336.00   \n",
       "3       1.601750e+13  16.000000  1.006850      0.820254   546966.00   \n",
       "4       1.620111e+13  16.188377  1.213387      0.500460  1130235.00   \n",
       "...              ...        ...       ...           ...         ...   \n",
       "247320  1.620111e+13  16.188377  1.213387      0.500460  1203642.00   \n",
       "247321  5.006000e+12   5.000000  2.085618      0.011711    41297.00   \n",
       "247322  6.002800e+12   6.000000  1.941782      0.584804    63534.00   \n",
       "247323  2.102370e+13  21.000000  0.551370      0.763274   793866.00   \n",
       "247324  1.900720e+13  19.000000  0.695206      0.931895   672878.00   \n",
       "\n",
       "        DescendantID  LastProgID   TopLeafID  GroupNumber  SubGroupNumber  \\\n",
       "0          369408.00   369410.00   369410.00      35569.0             0.0   \n",
       "1         1220855.79  1224259.74  1224259.74      37285.0             0.0   \n",
       "2          428335.00   428348.00   428345.00       7287.0             0.0   \n",
       "3          546965.00   546971.00   546971.00      11901.0             0.0   \n",
       "4         1130234.00  1130246.00  1130246.00       9304.0             0.0   \n",
       "...              ...         ...         ...          ...             ...   \n",
       "247320    1203641.00  1203644.00  1203644.00      36952.0             0.0   \n",
       "247321         -1.00    41299.00    41299.00      34230.0             0.0   \n",
       "247322      63533.00    63535.00    63535.00      17201.0             0.0   \n",
       "247323     793865.00   793895.00   793887.00       2467.0             0.0   \n",
       "247324     672877.00   672878.00   672878.00       6426.0             0.0   \n",
       "\n",
       "        ...  Velocity_z  KineticEnergy  MechanicalEnergy  TotalEnergy  \\\n",
       "0       ...   -0.236024      -0.204067          0.092412     0.092412   \n",
       "1       ...   -0.096425      -0.236849          0.059817     0.059817   \n",
       "2       ...   -0.174425       0.195013         -0.582414    -0.582414   \n",
       "3       ...   -0.193842       0.053084         -0.049739    -0.049739   \n",
       "4       ...   -0.504944       0.140580         -0.211823    -0.211823   \n",
       "...     ...         ...            ...               ...          ...   \n",
       "247320  ...   -0.215147      -0.198709          0.059952     0.059952   \n",
       "247321  ...   -0.071611      -0.082373          0.022851     0.022851   \n",
       "247322  ...   -0.010384       0.053763         -0.027674    -0.027674   \n",
       "247323  ...    0.046211       0.064466         -0.691228    -0.691228   \n",
       "247324  ...    0.037532      -0.063550          0.027044     0.027044   \n",
       "\n",
       "            Vmax  VmaxRadius      Mass  MassType_DM  HalfMassProjRad_DM  \\\n",
       "0      -0.227511    0.049183 -0.211853    -0.211853           -0.093204   \n",
       "1      -0.311868    0.333410 -0.105520    -0.105520            0.260575   \n",
       "2       0.172523    0.001628  0.204440     0.204440            0.023075   \n",
       "3      -0.054049    0.467709  0.111081     0.111081            0.063972   \n",
       "4       0.076661    0.177330  0.180916     0.180916            0.091372   \n",
       "...          ...         ...       ...          ...                 ...   \n",
       "247320 -0.311075    0.352225 -0.095216    -0.095216            0.251889   \n",
       "247321 -0.119199    0.121107 -0.038142    -0.038142            0.103302   \n",
       "247322  0.080003   -0.381622 -0.008413    -0.008413           -0.162882   \n",
       "247323  0.051547    0.027859  0.079149     0.079149            0.045699   \n",
       "247324  0.109589   -0.279787 -0.177228    -0.177228           -0.374580   \n",
       "\n",
       "        HalfMassRad_DM  \n",
       "0            -0.116673  \n",
       "1             0.242065  \n",
       "2             0.034475  \n",
       "3             0.115098  \n",
       "4             0.096988  \n",
       "...                ...  \n",
       "247320        0.260286  \n",
       "247321        0.094600  \n",
       "247322       -0.198907  \n",
       "247323        0.044586  \n",
       "247324       -0.383580  \n",
       "\n",
       "[247325 rows x 51 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigma_clipping(df, column, coverage_percentage):\n",
    "    \"\"\"\n",
    "    Applies sigma clipping to a specified column in a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column (str): The column to apply sigma clipping on.\n",
    "        coverage_percentage (float): The desired coverage percentage (e.g., 95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with outliers removed based on sigma clipping.\n",
    "    \"\"\"\n",
    "    # Ensure coverage_percentage is valid\n",
    "    if not (0 < coverage_percentage < 100):\n",
    "        raise ValueError(\"Coverage percentage must be between 0 and 100.\")\n",
    "    \n",
    "    # Calculate the z-score threshold based on the coverage percentage\n",
    "    alpha = (100 - coverage_percentage) / 100\n",
    "    z_threshold = abs(np.percentile(np.random.Generate(0, 1, 1000000), [alpha * 50, 100 - alpha * 50]))[1]\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "\n",
    "    # Apply sigma clipping\n",
    "    df_clipped = df[(df[column] >= mean - z_threshold * std) & (df[column] <= mean + z_threshold * std)]\n",
    "\n",
    "    return df_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>SnapNum</th>\n",
       "      <th>Redshift</th>\n",
       "      <th>RandomNumber</th>\n",
       "      <th>GalaxyID</th>\n",
       "      <th>DescendantID</th>\n",
       "      <th>LastProgID</th>\n",
       "      <th>TopLeafID</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>SubGroupNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>Velocity_z</th>\n",
       "      <th>KineticEnergy</th>\n",
       "      <th>MechanicalEnergy</th>\n",
       "      <th>TotalEnergy</th>\n",
       "      <th>Vmax</th>\n",
       "      <th>VmaxRadius</th>\n",
       "      <th>Mass</th>\n",
       "      <th>MassType_DM</th>\n",
       "      <th>HalfMassProjRad_DM</th>\n",
       "      <th>HalfMassRad_DM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.900270e+13</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.695206</td>\n",
       "      <td>0.232667</td>\n",
       "      <td>664610.0</td>\n",
       "      <td>664609.0</td>\n",
       "      <td>664625.0</td>\n",
       "      <td>664625.0</td>\n",
       "      <td>9196.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096851</td>\n",
       "      <td>0.136929</td>\n",
       "      <td>-0.351693</td>\n",
       "      <td>-0.351693</td>\n",
       "      <td>0.135758</td>\n",
       "      <td>-0.193651</td>\n",
       "      <td>0.165770</td>\n",
       "      <td>0.165770</td>\n",
       "      <td>0.048835</td>\n",
       "      <td>0.065318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.900160e+13</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.695206</td>\n",
       "      <td>0.887910</td>\n",
       "      <td>662597.0</td>\n",
       "      <td>662596.0</td>\n",
       "      <td>662606.0</td>\n",
       "      <td>662606.0</td>\n",
       "      <td>10048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132286</td>\n",
       "      <td>0.157683</td>\n",
       "      <td>-0.379617</td>\n",
       "      <td>-0.379617</td>\n",
       "      <td>0.151565</td>\n",
       "      <td>-0.074694</td>\n",
       "      <td>0.151655</td>\n",
       "      <td>0.151655</td>\n",
       "      <td>-0.019520</td>\n",
       "      <td>-0.019656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.700200e+13</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.910959</td>\n",
       "      <td>0.673209</td>\n",
       "      <td>566495.0</td>\n",
       "      <td>566494.0</td>\n",
       "      <td>566505.0</td>\n",
       "      <td>566505.0</td>\n",
       "      <td>10873.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022154</td>\n",
       "      <td>0.127924</td>\n",
       "      <td>-0.207793</td>\n",
       "      <td>-0.207793</td>\n",
       "      <td>0.079525</td>\n",
       "      <td>0.057689</td>\n",
       "      <td>0.164824</td>\n",
       "      <td>0.164824</td>\n",
       "      <td>0.077088</td>\n",
       "      <td>0.070598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800180e+13</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.815069</td>\n",
       "      <td>0.743976</td>\n",
       "      <td>614978.0</td>\n",
       "      <td>614977.0</td>\n",
       "      <td>614982.0</td>\n",
       "      <td>614981.0</td>\n",
       "      <td>9211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171808</td>\n",
       "      <td>0.136128</td>\n",
       "      <td>-0.235490</td>\n",
       "      <td>-0.235490</td>\n",
       "      <td>0.147548</td>\n",
       "      <td>0.085389</td>\n",
       "      <td>0.105995</td>\n",
       "      <td>0.105995</td>\n",
       "      <td>-0.063311</td>\n",
       "      <td>-0.079067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.100190e+13</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.551370</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>755492.0</td>\n",
       "      <td>755491.0</td>\n",
       "      <td>755503.0</td>\n",
       "      <td>755502.0</td>\n",
       "      <td>7231.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111483</td>\n",
       "      <td>0.153675</td>\n",
       "      <td>-0.430258</td>\n",
       "      <td>-0.430258</td>\n",
       "      <td>0.131762</td>\n",
       "      <td>-0.010662</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.079877</td>\n",
       "      <td>0.071138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.900230e+13</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.695206</td>\n",
       "      <td>0.141115</td>\n",
       "      <td>663872.0</td>\n",
       "      <td>663871.0</td>\n",
       "      <td>663879.0</td>\n",
       "      <td>663879.0</td>\n",
       "      <td>9408.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101447</td>\n",
       "      <td>0.162445</td>\n",
       "      <td>-0.376567</td>\n",
       "      <td>-0.376567</td>\n",
       "      <td>0.145115</td>\n",
       "      <td>0.132346</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.018945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.600060e+13</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.006850</td>\n",
       "      <td>0.760488</td>\n",
       "      <td>514073.0</td>\n",
       "      <td>514072.0</td>\n",
       "      <td>514081.0</td>\n",
       "      <td>514079.0</td>\n",
       "      <td>5870.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.156709</td>\n",
       "      <td>-0.418019</td>\n",
       "      <td>-0.418019</td>\n",
       "      <td>0.145459</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.143580</td>\n",
       "      <td>0.143580</td>\n",
       "      <td>-0.010094</td>\n",
       "      <td>-0.018642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.600200e+13</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.006850</td>\n",
       "      <td>0.353033</td>\n",
       "      <td>516755.0</td>\n",
       "      <td>516754.0</td>\n",
       "      <td>516766.0</td>\n",
       "      <td>516766.0</td>\n",
       "      <td>11752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.147148</td>\n",
       "      <td>-0.292155</td>\n",
       "      <td>-0.292155</td>\n",
       "      <td>0.152886</td>\n",
       "      <td>-0.038810</td>\n",
       "      <td>0.158498</td>\n",
       "      <td>0.158498</td>\n",
       "      <td>-0.023865</td>\n",
       "      <td>-0.024661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.600140e+13</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.006850</td>\n",
       "      <td>0.297315</td>\n",
       "      <td>515585.0</td>\n",
       "      <td>515584.0</td>\n",
       "      <td>515594.0</td>\n",
       "      <td>515594.0</td>\n",
       "      <td>9912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007567</td>\n",
       "      <td>0.167311</td>\n",
       "      <td>-0.392245</td>\n",
       "      <td>-0.392245</td>\n",
       "      <td>0.163040</td>\n",
       "      <td>-0.159221</td>\n",
       "      <td>0.158739</td>\n",
       "      <td>0.158739</td>\n",
       "      <td>-0.033080</td>\n",
       "      <td>-0.021696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2.300140e+13</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.407534</td>\n",
       "      <td>0.647683</td>\n",
       "      <td>843524.0</td>\n",
       "      <td>843523.0</td>\n",
       "      <td>843538.0</td>\n",
       "      <td>843538.0</td>\n",
       "      <td>4946.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171888</td>\n",
       "      <td>0.130409</td>\n",
       "      <td>-0.427789</td>\n",
       "      <td>-0.427789</td>\n",
       "      <td>0.095691</td>\n",
       "      <td>0.102658</td>\n",
       "      <td>0.161455</td>\n",
       "      <td>0.161455</td>\n",
       "      <td>0.082719</td>\n",
       "      <td>0.079312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GroupID  SnapNum  Redshift  RandomNumber  GalaxyID  DescendantID  \\\n",
       "0    1.900270e+13     19.0  0.695206      0.232667  664610.0      664609.0   \n",
       "1    1.900160e+13     19.0  0.695206      0.887910  662597.0      662596.0   \n",
       "2    1.700200e+13     17.0  0.910959      0.673209  566495.0      566494.0   \n",
       "3    1.800180e+13     18.0  0.815069      0.743976  614978.0      614977.0   \n",
       "4    2.100190e+13     21.0  0.551370      0.206509  755492.0      755491.0   \n",
       "..            ...      ...       ...           ...       ...           ...   \n",
       "102  1.900230e+13     19.0  0.695206      0.141115  663872.0      663871.0   \n",
       "103  1.600060e+13     16.0  1.006850      0.760488  514073.0      514072.0   \n",
       "104  1.600200e+13     16.0  1.006850      0.353033  516755.0      516754.0   \n",
       "105  1.600140e+13     16.0  1.006850      0.297315  515585.0      515584.0   \n",
       "106  2.300140e+13     23.0  0.407534      0.647683  843524.0      843523.0   \n",
       "\n",
       "     LastProgID  TopLeafID  GroupNumber  SubGroupNumber  ...  Velocity_z  \\\n",
       "0      664625.0   664625.0       9196.0             0.0  ...    0.096851   \n",
       "1      662606.0   662606.0      10048.0             0.0  ...    0.132286   \n",
       "2      566505.0   566505.0      10873.0             0.0  ...   -0.022154   \n",
       "3      614982.0   614981.0       9211.0             0.0  ...   -0.171808   \n",
       "4      755503.0   755502.0       7231.0             0.0  ...   -0.111483   \n",
       "..          ...        ...          ...             ...  ...         ...   \n",
       "102    663879.0   663879.0       9408.0             0.0  ...   -0.101447   \n",
       "103    514081.0   514079.0       5870.0             0.0  ...    0.020623   \n",
       "104    516766.0   516766.0      11752.0             0.0  ...    0.104651   \n",
       "105    515594.0   515594.0       9912.0             0.0  ...   -0.007567   \n",
       "106    843538.0   843538.0       4946.0             0.0  ...    0.171888   \n",
       "\n",
       "     KineticEnergy  MechanicalEnergy  TotalEnergy      Vmax  VmaxRadius  \\\n",
       "0         0.136929         -0.351693    -0.351693  0.135758   -0.193651   \n",
       "1         0.157683         -0.379617    -0.379617  0.151565   -0.074694   \n",
       "2         0.127924         -0.207793    -0.207793  0.079525    0.057689   \n",
       "3         0.136128         -0.235490    -0.235490  0.147548    0.085389   \n",
       "4         0.153675         -0.430258    -0.430258  0.131762   -0.010662   \n",
       "..             ...               ...          ...       ...         ...   \n",
       "102       0.162445         -0.376567    -0.376567  0.145115    0.132346   \n",
       "103       0.156709         -0.418019    -0.418019  0.145459    0.016135   \n",
       "104       0.147148         -0.292155    -0.292155  0.152886   -0.038810   \n",
       "105       0.167311         -0.392245    -0.392245  0.163040   -0.159221   \n",
       "106       0.130409         -0.427789    -0.427789  0.095691    0.102658   \n",
       "\n",
       "         Mass  MassType_DM  HalfMassProjRad_DM  HalfMassRad_DM  \n",
       "0    0.165770     0.165770            0.048835        0.065318  \n",
       "1    0.151655     0.151655           -0.019520       -0.019656  \n",
       "2    0.164824     0.164824            0.077088        0.070598  \n",
       "3    0.105995     0.105995           -0.063311       -0.079067  \n",
       "4    0.174811     0.174811            0.079877        0.071138  \n",
       "..        ...          ...                 ...             ...  \n",
       "102  0.163675     0.163675            0.017922        0.018945  \n",
       "103  0.143580     0.143580           -0.010094       -0.018642  \n",
       "104  0.158498     0.158498           -0.023865       -0.024661  \n",
       "105  0.158739     0.158739           -0.033080       -0.021696  \n",
       "106  0.161455     0.161455            0.082719        0.079312  \n",
       "\n",
       "[107 rows x 51 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clip = train_data\n",
    "train_data_clip = sigma_clipping(train_data_clip, train_data.columns, 85)\n",
    "train_data_clip.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import DataProcessor,  DMDataset\n",
    "from model_train import ModelTrainer\n",
    "from model_hyperparameter_optimizer import RNNHyperParameterOptimizer\n",
    "from rnn_model import RNNDMHaloMapper, LSTMDMHaloMapper, GRUDMHaloMapper\n",
    "from redshift_analyzer import RedshiftAnalyzer\n",
    "from model_visualization import ModelVisualizer\n",
    "import torch\n",
    "import torch.optim as tco\n",
    "import torch.utils.data as tcud\n",
    "import torch.nn as tcn\n",
    "\n",
    "class DMHaloMapper:\n",
    "    def __init__(self, data=None, device=None):\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dp = DataProcessor(data_path=data)\n",
    "        self.mt = ModelTrainer()\n",
    "        self.dm = DMDataset()\n",
    "        self.hp = None\n",
    "        self.mv = ModelVisualizer()\n",
    "        self.rnn = RNNDMHaloMapper()\n",
    "        self.lstm = LSTMDMHaloMapper()\n",
    "        self.gru = GRUDMHaloMapper()\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.features = None\n",
    "        self.targets = None\n",
    "        self.redshift_values = None\n",
    "        self.unique_redshifts = None\n",
    "        self.best_hyperparameter = None\n",
    "\n",
    "\n",
    "    def data_process(self):\n",
    "        dp.load_data()\n",
    "        dp.missing_values()\n",
    "        dp.basic_statistics()\n",
    "        dp.data_distribution()\n",
    "        dp.correlation_heatmap()\n",
    "        self.train_data, self.val_data = dp.preprocess_data(scaler_type=\"robust\", coverage=0.85)\n",
    "        dp.preprocessed_distribution()\n",
    "    \n",
    "    def data_to_tensor(self):\n",
    "        features = [\n",
    "            'Redshift',\n",
    "            'NumOfSubhalos',\n",
    "            'Velocity_x',\n",
    "            'Velocity_y',\n",
    "            'Velocity_z',\n",
    "            'MassType_DM',\n",
    "            'CentreOfPotential_x',\n",
    "            'CentreOfPotential_y',\n",
    "            'CentreOfPotential_z',\n",
    "            'GroupCentreOfPotential_x',\n",
    "            'GroupCentreOfPotential_y',\n",
    "            'GroupCentreOfPotential_z',\n",
    "            'Vmax',\n",
    "            'VmaxRadius',\n",
    "            'Group_R_Crit200'\n",
    "        ]\n",
    "        \n",
    "        targets = ['Group_M_Crit200']\n",
    "        \n",
    "        self.X = self.train_data[features]\n",
    "        self.y = self.train_data[targets].values.reshape(-1, 1)\n",
    "        \n",
    "        self.features, self.targets = self.dm(self.X, self.y)\n",
    "    \n",
    "    def process_redshift(self):\n",
    "        self.ra = RedshiftAnalyzer(self.train_data, self.features)\n",
    "        self.redshift_values, self.unique_redshifts = self.ra.process_redshift_data()\n",
    "    \n",
    "    def discover_hyperparameter(self):\n",
    "        print(f\"Starting discover_hyperparameter for {len(self.models_to_optimize)} models...\")\n",
    "        self.models_to_optimize = [self.rnn, self.lstm, self.gru]\n",
    "        \n",
    "        optimization_results = []\n",
    "        \n",
    "        self.hp = RNNHyperParameterOptimizer(\n",
    "            X=self.X,\n",
    "            y=self.y,\n",
    "            base_model_class=RNNDMHaloMapper,  # You can change this to LSTMDMHaloMapper or GRUDMHaloMapper\n",
    "            device=self.device,\n",
    "            num_features=self.X.shape[1],\n",
    "            pop_size=20,\n",
    "            maxiter=100,\n",
    "            cross_val_folds=13\n",
    "        )\n",
    "        \n",
    "        # Run optimization\n",
    "        optimization_result = self.hp.optimize()\n",
    "        \n",
    "        if optimization_result:\n",
    "            self.best_hyperparameters = optimization_result['best_params']\n",
    "            print(\"Best hyperparameters found:\")\n",
    "            for param, value in self.best_hyperparameters.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        else:\n",
    "            print(\"Hyperparameter optimization failed. Using default hyperparameters.\")\n",
    "            self.best_hyperparameters = {\n",
    "                'hidden_size': 256,\n",
    "                'num_layers': 2,\n",
    "                'dropout_rate': 0.2,\n",
    "                'learning_rate': 0.001,\n",
    "                'batch_size': 64,\n",
    "                'weight_decay': 0.0001,\n",
    "                'num_epochs': 10\n",
    "            }\n",
    "        \n",
    "    def train_rnn_model(self):\n",
    "        \"\"\"\n",
    "        Train the RNN, LSTM, and GRU models using the best hyperparameters.\n",
    "        \"\"\"\n",
    "        if not self.best_hyperparameters:\n",
    "            print(\"No hyperparameters found. Running discover_hyperparameter first.\")\n",
    "            self.discover_hyperparameter()\n",
    "\n",
    "        input_size = self.X.shape[1]\n",
    "        output_size = 1\n",
    "\n",
    "        # Define models with the best hyperparameters\n",
    "        models = {\n",
    "            'RNN': RNNDMHaloMapper(\n",
    "                input_size=input_size,\n",
    "                hidden_size=self.best_hyperparameters['hidden_size'],\n",
    "                output_size=output_size,\n",
    "                num_layers=self.best_hyperparameters['num_layers'],\n",
    "                dropout_rate=self.best_hyperparameters['dropout_rate'],\n",
    "                learning_rate=self.best_hyperparameters['learning_rate'],\n",
    "                batch_size=self.best_hyperparameters['batch_size'],\n",
    "                weight_decay=self.best_hyperparameters['weight_decay'],\n",
    "                num_epochs=self.best_hyperparameters['num_epochs']\n",
    "            ),\n",
    "            'LSTM': LSTMDMHaloMapper(\n",
    "                input_size=input_size,\n",
    "                hidden_size=self.best_hyperparameters['hidden_size'],\n",
    "                output_size=output_size,\n",
    "                num_layers=self.best_hyperparameters['num_layers'],\n",
    "                dropout_rate=self.best_hyperparameters['dropout_rate'],\n",
    "                learning_rate=self.best_hyperparameters['learning_rate'],\n",
    "                batch_size=self.best_hyperparameters['batch_size'],\n",
    "                weight_decay=self.best_hyperparameters['weight_decay'],\n",
    "                num_epochs=self.best_hyperparameters['num_epochs']\n",
    "            ),\n",
    "            'GRU': GRUDMHaloMapper(\n",
    "                input_size=input_size,\n",
    "                hidden_size=self.best_hyperparameters['hidden_size'],\n",
    "                output_size=output_size,\n",
    "                num_layers=self.best_hyperparameters['num_layers'],\n",
    "                dropout_rate=self.best_hyperparameters['dropout_rate'],\n",
    "                learning_rate=self.best_hyperparameters['learning_rate'],\n",
    "                batch_size=self.best_hyperparameters['batch_size'],\n",
    "                weight_decay=self.best_hyperparameters['weight_decay'],\n",
    "                num_epochs=self.best_hyperparameters['num_epochs']\n",
    "            )\n",
    "        }\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, test_size=0.25, random_state=42)\n",
    "\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        \n",
    "        train_dataset = DMDataset(X_train, y_train)\n",
    "        val_dataset = DMDataset(X_val, y_val)\n",
    "\n",
    "        train_loader = DMDataset.data_loader(train_dataset, batch_size=self.best_hyperparameters['batch_size'])\n",
    "        val_loader = DMDataset.data_loader(val_dataset, batch_size=self.best_hyperparameters['batch_size'])\n",
    "        '''train_loader = tcud.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.best_hyperparameters['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )\n",
    "\n",
    "        val_loader = tcud.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.best_hyperparameters['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if self.device.type == 'cuda' else False\n",
    "        )'''\n",
    "\n",
    "        model_results = {}\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name} model with optimized hyperparameters...\")\n",
    "            optimizer = tco.Adam(model.parameters(), lr=self.best_hyperparameters['learning_rate'], weight_decay=self.best_hyperparameters['weight_decay'])\n",
    "            criterion = tcn.MSELoss()\n",
    "\n",
    "            history, model_dir = self.mt.train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                num_epochs=self.best_hyperparameters['num_epochs'],\n",
    "                device=self.device,\n",
    "                model_name=name,\n",
    "                save_dir='./best_model'\n",
    "            )\n",
    "\n",
    "            model_results[name] = {\n",
    "                'history': history,\n",
    "                'model_dir': model_dir\n",
    "            }\n",
    "\n",
    "        return model_results\n",
    "    \n",
    "    def process_model(self):\n",
    "        \"\"\"\n",
    "        Process the validation dataset, make predictions, and store results for visualization.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'models'):\n",
    "            print(\"No trained models found. Run train_rnn_model first.\")\n",
    "            return\n",
    "\n",
    "        # Load validation data\n",
    "        X_val_tensor = tc.tensor(self.X_val.values, dtype=tc.float32)\n",
    "        self.y_val = self.y[self.X_val.index]\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        self.val_predictions = {}\n",
    "        for name, model in self.models.items():\n",
    "            model.eval()\n",
    "            with tc.no_grad():\n",
    "                outputs = model(X_val_tensor.to(self.device))\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                self.val_predictions[name] = outputs.cpu().numpy()\n",
    "\n",
    "        print(\"Validation dataset processed and predictions stored.\")\n",
    "    \n",
    "    def model_visualization(self):\n",
    "        \"\"\"\n",
    "        Visualize the model's predictions, including dark matter distribution, triaxial model, and redshift evolution.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'val_predictions'):\n",
    "            print(\"No validation predictions found. Run process_model first.\")\n",
    "            return\n",
    "\n",
    "        # Extract spatial coordinates for visualization\n",
    "        X_val_tensor = tc.tensor(self.X_val.values, dtype=tc.float32)\n",
    "        column_indices = [self.X.columns.get_loc(f) for f in \n",
    "            [\n",
    "                'GroupCentreOfPotential_x',\n",
    "                'GroupCentreOfPotential_y',\n",
    "                'GroupCentreOfPotential_z'\n",
    "            ]\n",
    "        ]\n",
    "        spatial_coords = X_val_tensor[:, column_indices].numpy()\n",
    "\n",
    "        # Initialize ModelVisualizer\n",
    "        self.mv = ModelVisualizer(\n",
    "            predictions=self.val_predictions['RNN'],  # Use RNN predictions as an example\n",
    "            true_values=self.y_val,\n",
    "            spatial_coords=spatial_coords,\n",
    "            redshifts=self.redshift_values[self.X_val.index],\n",
    "            save_dir='./visualizations'\n",
    "        )\n",
    "\n",
    "        # Generate and save visualizations\n",
    "        self.mv.visualize_dm_distribution(a=1.0, b=0.8, c=0.6)\n",
    "\n",
    "    \n",
    "    def run_process(self):\n",
    "        self.data_process()\n",
    "        self.data_to_tensor()\n",
    "        self.process_redshift()\n",
    "        self.discover_hyperparameter()\n",
    "        self.train_rnn_model()\n",
    "        self.model_visualization()\n",
    "        #self.process_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNHyperparameterOptimizer:\n",
    "    def __init__(self, X, y, base_model_class, device, num_features, \n",
    "                 pop_size=20, maxiter=10, cross_val_folds=3):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.base_model_class = base_model_class\n",
    "        self.device = device\n",
    "        self.num_features = num_features\n",
    "        self.pop_size = pop_size\n",
    "        self.maxiter = maxiter\n",
    "        self.cross_val_folds = cross_val_folds\n",
    "        \n",
    "        # Define parameter bounds\n",
    "        self.bounds = [\n",
    "            (32, 512),      # hidden_size\n",
    "            (1, 4),         # num_layers\n",
    "            (0.0, 0.5),     # dropout_rate\n",
    "            (0.0001, 0.1),  # learning_rate\n",
    "            (16, 128),      # batch_size\n",
    "            (0.0, 0.1)      # weight_decay\n",
    "            (1, 100)        # number of epochs\n",
    "        ]\n",
    "    \n",
    "    def create_model(self, params):\n",
    "        \"\"\"Create model with given hyperparameters\"\"\"\n",
    "        hidden_size = int(params[0])\n",
    "        num_layers = int(params[1])\n",
    "        dropout_rate = params[2]\n",
    "        learning_rate = params[3]\n",
    "        batch_size = int(params[4])\n",
    "        weight_decay = params[5]\n",
    "        num_epochs = int(params[6])\n",
    "        \n",
    "        return self.base_model_class(\n",
    "            input_size=self.num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=1,\n",
    "            num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            weight_decay=weight_decay,\n",
    "            num_epochs=num_epochs,\n",
    "            nonlinearity='tanh'\n",
    "        )\n",
    "    \n",
    "    def objective_function(self, params):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "        try:\n",
    "            # Extract parameters\n",
    "            hidden_size = int(params[0])\n",
    "            num_layers = int(params[1])\n",
    "            dropout_rate = params[2]\n",
    "            learning_rate = params[3]\n",
    "            batch_size = int(params[4])\n",
    "            weight_decay = params[5]\n",
    "            \n",
    "            # Initialize cross-validation\n",
    "            kf = KFold(n_splits=self.cross_val_folds, shuffle=True, random_state=42)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(self.X):\n",
    "                # Split data\n",
    "                X_train, X_val = self.X.iloc[train_idx], self.X.iloc[val_idx]\n",
    "                y_train, y_val = self.y[train_idx], self.y[val_idx]\n",
    "                \n",
    "                # Create datasets and dataloaders\n",
    "                train_dataset = DMDataset(X_train, y_train)\n",
    "                val_dataset = DMDataset(X_val, y_val)\n",
    "                \n",
    "                train_loader = tcud.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=True if self.device.type == 'cuda' else False\n",
    "                )\n",
    "                \n",
    "                val_loader = tcud.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=True if self.device.type == 'cuda' else False\n",
    "                )\n",
    "                \n",
    "                # Create and train model\n",
    "                model = self.create_model(params).to(self.device)\n",
    "                optimizer = tco.Adam(model.parameters(), \n",
    "                                    lr=learning_rate, \n",
    "                                    weight_decay=weight_decay)\n",
    "                criterion = tcn.MSELoss()\n",
    "                \n",
    "                # Quick training loop\n",
    "                model.train()\n",
    "                for epoch in range(5):  # Reduced epochs for optimization\n",
    "                    for batch_features, batch_targets in train_loader:\n",
    "                        batch_features = batch_features.to(self.device)\n",
    "                        batch_targets = batch_targets.to(self.device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(batch_features)\n",
    "                        loss = criterion(outputs, batch_targets)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Evaluate\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with tc.no_grad():\n",
    "                    for batch_features, batch_targets in val_loader:\n",
    "                        batch_features = batch_features.to(self.device)\n",
    "                        batch_targets = batch_targets.to(self.device)\n",
    "                        outputs = model(batch_features)\n",
    "                        val_loss += criterion(outputs, batch_targets).item()\n",
    "                \n",
    "                cv_scores.append(val_loss / len(val_loader))\n",
    "            \n",
    "            mean_score = np.mean(cv_scores)\n",
    "            print(f\"Trial completed - Score: {mean_score:.4f}, Params: {params}\")\n",
    "            return mean_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"Run differential evolution\"\"\"\n",
    "        try:\n",
    "            result = differential_evolution(\n",
    "                func=self.objective_function,\n",
    "                bounds=self.bounds,\n",
    "                maxiter=self.maxiter,\n",
    "                popsize=self.pop_size,\n",
    "                mutation=(0.5, 1.0),\n",
    "                recombination=0.7,\n",
    "                seed=42,\n",
    "                disp=True,\n",
    "                workers=-1\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            best_params = {\n",
    "                'hidden_size': int(result.x[0]),\n",
    "                'num_layers': int(result.x[1]),\n",
    "                'dropout_rate': result.x[2],\n",
    "                'learning_rate': result.x[3],\n",
    "                'batch_size': int(result.x[4]),\n",
    "                'weight_decay': result.x[5],\n",
    "                'num_epochs': int(result.x[6])\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'best_params': best_params,\n",
    "                'best_score': result.fun,\n",
    "                'convergence': result.success,\n",
    "                'iterations': result.nit,\n",
    "                'optimization_result': result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in optimization: {e}\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name, save_dir):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    metrics_calculator = CustomMetrics()\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_metrics': [],\n",
    "        'val_metrics': []\n",
    "    }\n",
    "    \n",
    "    # Create directories for saving results\n",
    "    model_dir = os.path.join(save_dir, f\"{model_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "    train_history_path = os.path.join(model_dir, \"train_history.npy\")\n",
    "    model_comparison_path = os.path.join(model_dir, \"model_comparison_results.npy\")\n",
    "    plot_path = os.path.join(model_dir, \"dmhalo_mapper.html\")\n",
    "\n",
    "    # Wrap the epochs loop in tqdm\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        epoch_train_metrics =   {\n",
    "                                    'mse': 0, \n",
    "                                    'rmse': 0, \n",
    "                                    'mae': 0, \n",
    "                                    'r2': 0,\n",
    "                                    'gaussian_nll': 0,\n",
    "                                    'poisson_nll': 0\n",
    "                                }\n",
    "\n",
    "        # Wrap the training batches loop in tqdm\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            try: \n",
    "                batch_features= batch_features.to(device)\n",
    "                batch_targets = batch_targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_features)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                \n",
    "                tcn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "                # Calculate training metrics for this batch\n",
    "                batch_metrics = metrics_calculator.calculate_all_metrics(batch_targets, outputs)\n",
    "                for metric in batch_metrics:\n",
    "                    epoch_train_metrics[metric] += batch_metrics[metric]\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in training batch: {e}\")\n",
    "                continue        \n",
    "\n",
    "        # Average the training metrics\n",
    "        num_batches = len(train_loader)\n",
    "        avg_train_loss = train_loss / num_batches\n",
    "        avg_train_metrics = {\n",
    "            metric: value / num_batches \n",
    "            for metric, value in epoch_train_metrics.items()\n",
    "        }\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        epoch_val_metrics =   {\n",
    "                                    'mse': 0, \n",
    "                                    'rmse': 0, \n",
    "                                    'mae': 0, \n",
    "                                    'r2': 0,\n",
    "                                    'gaussian_nll': 0, \n",
    "                                    'poisson_nll': 0\n",
    "                                }\n",
    "        \n",
    "        with tc.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                try:\n",
    "                    batch_features= batch_features.to(device)\n",
    "                    batch_targets = batch_targets.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = criterion(outputs, batch_targets)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                    # Calculate validation metrics for this batch\n",
    "                    batch_metrics = metrics_calculator.calculate_all_metrics(batch_targets, outputs)\n",
    "                    for metric in batch_metrics:\n",
    "                        epoch_val_metrics[metric] += batch_metrics[metric]\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "        # Average the validation metrics\n",
    "        num_val_batches = len(val_loader)\n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        avg_val_metrics = {\n",
    "            metric: value / num_val_batches \n",
    "            for metric, value in epoch_val_metrics.items()\n",
    "        }\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            tc.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_metrics'].append(avg_train_metrics)\n",
    "        history['val_metrics'].append(avg_val_metrics)\n",
    "        \n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(\"\\nTraining Metrics:\")\n",
    "            for metric, value in avg_train_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\", end=\"  \")\n",
    "            print(\"\\nValidation Metrics:\")\n",
    "            for metric, value in avg_val_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\", end=\"  \")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "    # Save training history\n",
    "    np.save(train_history_path, history)\n",
    "\n",
    "    print(f\"Model {model} training complete. Best model saved to {best_model_path}\")\n",
    "    return history, model_dir\n",
    "\n",
    "def visualize_dm_distribution(predictions, true_values, spatial_coords, redshifts=None, save_path=None, \n",
    "                            a=1.0, b=0.8, c=0.6):  # Add axis ratios for triaxiality\n",
    "    try:\n",
    "        # Ensure all inputs are numpy arrays and properly shaped\n",
    "        predictions = np.array(predictions).reshape(-1)\n",
    "        true_values = np.array(true_values).reshape(-1)\n",
    "        spatial_coords = np.array(spatial_coords)\n",
    "        \n",
    "        if redshifts is not None:\n",
    "            redshifts = np.array(redshifts)\n",
    "            unique_redshifts = np.sort(np.unique(redshifts))[::-1]  # Sort in descending order\n",
    "            \n",
    "        if spatial_coords.shape[1] != 3:\n",
    "            raise ValueError(\"spatial_coords must have shape (n_samples, 3)\")\n",
    "\n",
    "        # Create subplots: 3D scatter, triaxial view, and animation controls\n",
    "        fig = sp.make_subplots(\n",
    "            rows=1, cols=3,\n",
    "            specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "            subplot_titles=[\n",
    "                \"Dark Matter Distribution\",\n",
    "                \"Triaxial Model\",\n",
    "                \"Evolution View\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create frames for animation if redshifts are provided\n",
    "        frames = []\n",
    "        if redshifts is not None:\n",
    "            for z in unique_redshifts:\n",
    "                mask = (redshifts == z)\n",
    "                \n",
    "                frame_data = [\n",
    "                    # 3D scatter for DM distribution\n",
    "                    go.Scatter3d(\n",
    "                        x=spatial_coords[mask, 0],\n",
    "                        y=spatial_coords[mask, 1],\n",
    "                        z=spatial_coords[mask, 2],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=3,\n",
    "                            color=predictions[mask],\n",
    "                            colorscale='Viridis',\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        name=f'z={z:.2f}'\n",
    "                    ),\n",
    "                    # Triaxial model stays constant\n",
    "                    *create_triaxial_surfaces(a, b, c),\n",
    "                    # Evolution view\n",
    "                    go.Scatter3d(\n",
    "                        x=spatial_coords[mask, 0],\n",
    "                        y=spatial_coords[mask, 1],\n",
    "                        z=spatial_coords[mask, 2],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=3,\n",
    "                            color=predictions[mask],\n",
    "                            colorscale='Inferno',\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        name=f'Evolution z={z:.2f}'\n",
    "                    )\n",
    "                ]\n",
    "                \n",
    "                frames.append(go.Frame(\n",
    "                    data=frame_data,\n",
    "                    name=f'z={z:.2f}'\n",
    "                ))\n",
    "\n",
    "        # Initial state\n",
    "        scatter3d = go.Scatter3d(\n",
    "            x=spatial_coords[:, 0],\n",
    "            y=spatial_coords[:, 1],\n",
    "            z=spatial_coords[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=predictions,\n",
    "                colorscale='Viridis',\n",
    "                opacity=0.8,\n",
    "                colorbar=dict(title=\"Predicted Mass\")\n",
    "            ),\n",
    "            name='Dark Matter Halos'\n",
    "        )\n",
    "        fig.add_trace(scatter3d, row=1, col=1)\n",
    "\n",
    "        # Add triaxial model\n",
    "        triaxial_surfaces = create_triaxial_surfaces(a, b, c)\n",
    "        for surface in triaxial_surfaces:\n",
    "            fig.add_trace(surface, row=1, col=2)\n",
    "\n",
    "        # Add evolution view\n",
    "        evolution_scatter = go.Scatter3d(\n",
    "            x=spatial_coords[:, 0],\n",
    "            y=spatial_coords[:, 1],\n",
    "            z=spatial_coords[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=predictions,\n",
    "                colorscale='Inferno',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            name='Evolution View'\n",
    "        )\n",
    "        fig.add_trace(evolution_scatter, row=1, col=3)\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Dark Matter Halo Distribution with Triaxial Model and Evolution',\n",
    "            width=1800,\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            updatemenus=[{\n",
    "                'buttons': [\n",
    "                    {\n",
    "                        'args': [None, {'frame': {'duration': 50, 'redraw': True},\n",
    "                                      'fromcurrent': True}],\n",
    "                        'label': 'Play',\n",
    "                        'method': 'animate'\n",
    "                    },\n",
    "                    {\n",
    "                        'args': [[None], {'frame': {'duration': 0, 'redraw': True},\n",
    "                                        'mode': 'immediate',\n",
    "                                        'transition': {'duration': 0}}],\n",
    "                        'label': 'Pause',\n",
    "                        'method': 'animate'\n",
    "                    }\n",
    "                ],\n",
    "                'type': 'buttons',\n",
    "                'showactive': False,\n",
    "                'x': 0.1,\n",
    "                'y': 1.1,\n",
    "                'xanchor': 'right',\n",
    "                'yanchor': 'top'\n",
    "            }],\n",
    "            sliders=[{\n",
    "                'currentvalue': {'prefix': 'Redshift: '},\n",
    "                'steps': [\n",
    "                    {\n",
    "                        'args': [[f'z={z:.2f}'], {'frame': {'duration': 50, 'redraw': True},\n",
    "                                                'mode': 'immediate'}],\n",
    "                        'label': f'{z:.2f}',\n",
    "                        'method': 'animate'\n",
    "                    }\n",
    "                    for z in unique_redshifts\n",
    "                ]\n",
    "            }] if redshifts is not None else None\n",
    "        )\n",
    "\n",
    "        # Add frames to the figure\n",
    "        fig.frames = frames\n",
    "\n",
    "        # Save to file if specified\n",
    "        if save_path:\n",
    "            fig.write_html(save_path)\n",
    "            print(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "            '''# Create and save GIF\n",
    "            if redshifts is not None:\n",
    "                gif_path = save_path.replace('.html', '.gif')\n",
    "                create_evolution_gif(spatial_coords, predictions, redshifts, gif_path)\n",
    "'''\n",
    "        return fig\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_triaxial_surfaces(a, b, c, n_points=50):\n",
    "    \"\"\"Create triaxial model surfaces.\"\"\"\n",
    "    phi = np.linspace(0, 2*np.pi, n_points)\n",
    "    theta = np.linspace(-np.pi/2, np.pi/2, n_points)\n",
    "    phi, theta = np.meshgrid(phi, theta)\n",
    "\n",
    "    surfaces = []\n",
    "    \n",
    "    # Create outer shell\n",
    "    x_outer = a * np.cos(theta) * np.cos(phi)\n",
    "    y_outer = b * np.cos(theta) * np.sin(phi)\n",
    "    z_outer = c * np.sin(theta)\n",
    "    \n",
    "    surfaces.append(go.Surface(\n",
    "        x=x_outer, y=y_outer, z=z_outer,\n",
    "        opacity=0.3,\n",
    "        colorscale='Blues',\n",
    "        showscale=False,\n",
    "        name='Outer Shell'\n",
    "    ))\n",
    "    \n",
    "    # Create inner shell\n",
    "    x_inner = 0.7*a * np.cos(theta) * np.cos(phi)\n",
    "    y_inner = 0.7*b * np.cos(theta) * np.sin(phi)\n",
    "    z_inner = 0.7*c * np.sin(theta)\n",
    "    \n",
    "    surfaces.append(go.Surface(\n",
    "        x=x_inner, y=y_inner, z=z_inner,\n",
    "        opacity=0.3,\n",
    "        colorscale='Reds',\n",
    "        showscale=False,\n",
    "        name='Inner Shell'\n",
    "    ))\n",
    "    \n",
    "    return surfaces\n",
    "\n",
    "def create_evolution_gif(spatial_coords, predictions, redshifts, save_path):\n",
    "    \"\"\"Create a GIF showing the evolution of dark matter halos with redshift.\"\"\"\n",
    "    import imageio\n",
    "    \n",
    "    unique_redshifts = np.sort(np.unique(redshifts))[::-1]\n",
    "    frames = []\n",
    "    \n",
    "    for z in unique_redshifts:\n",
    "        mask = (redshifts == z)\n",
    "        \n",
    "        # Create static figure for this redshift\n",
    "        fig = go.Figure(data=[\n",
    "            go.Scatter3d(\n",
    "                x=spatial_coords[mask, 0],\n",
    "                y=spatial_coords[mask, 1],\n",
    "                z=spatial_coords[mask, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=3,\n",
    "                    color=predictions[mask],\n",
    "                    colorscale='Viridis',\n",
    "                    opacity=0.8\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Dark Matter Distribution at z={z:.2f}',\n",
    "            width=800,\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        # Convert to image\n",
    "        img_bytes = fig.to_image(format=\"png\")\n",
    "        frames.append(imageio.imread(img_bytes))\n",
    "    \n",
    "    # Save as GIF\n",
    "    imageio.mimsave(save_path, frames, duration=50)\n",
    "    print(f\"Evolution GIF saved to {save_path}\")\n",
    "    \n",
    "def process_redshift_data(data, selected_features):\n",
    "    \"\"\"\n",
    "    Process redshift data from the EAGLE simulation dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data : DataFrame\n",
    "        The input EAGLE simulation data\n",
    "    selected_features : list\n",
    "        List of selected features including 'Redshift'\n",
    "        \n",
    "    Returns:\n",
    "    numpy.ndarray\n",
    "        Processed redshift values\n",
    "    \"\"\"\n",
    "    if 'Redshift' not in selected_features:\n",
    "        raise ValueError(\"'Redshift' must be included in selected_features\")\n",
    "    \n",
    "    # Extract redshift values\n",
    "    redshift_values = data['Redshift'].values\n",
    "    \n",
    "    # Sort unique redshift values in descending order\n",
    "    unique_redshifts = np.sort(np.unique(redshift_values))[::-1]\n",
    "    \n",
    "    print(f\"Redshift range: {unique_redshifts.min():.2f} to {unique_redshifts.max():.2f}\")\n",
    "    print(f\"Number of unique redshift values: {len(unique_redshifts)}\")\n",
    "    \n",
    "    return redshift_values, unique_redshifts\n",
    "\n",
    "def analyze_redshift_evolution(predictions, true_values, redshifts, save_dir, model_name):\n",
    "    \"\"\"\n",
    "    Analyze how predictions vary with redshift.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    true_values : array-like\n",
    "        True values\n",
    "    redshifts : array-like\n",
    "        Corresponding redshift values\n",
    "    save_dir : str\n",
    "        Directory to save analysis results\n",
    "    model_name : str\n",
    "        Name of the model being analyzed\n",
    "    \"\"\"\n",
    "    unique_z = np.sort(np.unique(redshifts))\n",
    "    metrics_by_z = []\n",
    "    \n",
    "    for z in unique_z:\n",
    "        mask = (redshifts == z)\n",
    "        z_metrics = CustomMetrics.calculate_all_metrics(\n",
    "            true_values[mask],\n",
    "            predictions[mask]\n",
    "        )\n",
    "        z_metrics['redshift'] = z\n",
    "        metrics_by_z.append(z_metrics)\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    metrics_df = pd.DataFrame(metrics_by_z)\n",
    "    metrics_df.to_csv(os.path.join(save_dir, f'{model_name}_redshift_evolution.csv'))\n",
    "    \n",
    "    # Create evolution plots\n",
    "    fig = go.Figure()\n",
    "    for metric in ['mse', 'rmse', 'mae', 'r2']:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=metrics_df['redshift'],\n",
    "            y=metrics_df[metric],\n",
    "            mode='lines+markers',\n",
    "            name=metric.upper()\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{model_name} Metrics Evolution with Redshift',\n",
    "        xaxis_title='Redshift',\n",
    "        yaxis_title='Metric Value',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.write_html(os.path.join(save_dir, f'{model_name}_redshift_evolution.html'))\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    selected_features = [\n",
    "        'Redshift',\n",
    "        'NumOfSubhalos',\n",
    "        'Velocity_x',\n",
    "        'Velocity_y',\n",
    "        'Velocity_z',\n",
    "        'MassType_DM',\n",
    "        'Mass',\n",
    "        'CentreOfPotential_x',\n",
    "        'CentreOfPotential_y',\n",
    "        'CentreOfPotential_z',\n",
    "        'GroupCentreOfPotential_x',\n",
    "        'GroupCentreOfPotential_y',\n",
    "        'GroupCentreOfPotential_z',\n",
    "        'Vmax',\n",
    "        'VmaxRadius',\n",
    "        'Group_R_Crit200'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        #Model hyperparameter\n",
    "        input_size = len(selected_features)\n",
    "        hidden_size = 256\n",
    "        output_size = 1\n",
    "        learning_rate = 0.001\n",
    "        batch_size = 64\n",
    "        num_epochs = 10\n",
    "        weight_decay = 0.001\n",
    "        epsilon = 1e-08\n",
    "    \n",
    "        device = tc.device('cuda' if tc.cuda.is_available() else 'cpu')\n",
    "        print(f'Using device: {device}')\n",
    "        \n",
    "        save_dir = './best_model'\n",
    "    \n",
    "        X = train_set_w_obs[selected_features]\n",
    "        y = train_set_w_obs['Group_M_Crit200'].values.reshape(-1, 1)\n",
    "\n",
    "        # Process redshift data\n",
    "        redshift_values, unique_redshifts = process_redshift_data(train_set_w_obs, selected_features)\n",
    "\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "        # Get corresponding redshift values for validation set\n",
    "        val_indices = X_val.index\n",
    "        redshift_val = redshift_values[val_indices]\n",
    "\n",
    "    \n",
    "        train_dataset = DMDataset(X_train, y_train)\n",
    "        val_dataset = DMDataset(X_val, y_val)\n",
    "    \n",
    "        # Create data loaders\n",
    "        train_loader = tcud.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if device.type == 'cuda' else False\n",
    "        )\n",
    "        \n",
    "        val_loader = tcud.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if device.type == 'cuda' else False\n",
    "        )\n",
    "    \n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'RNN': RNNDMHaloMapper(input_size, hidden_size, output_size),\n",
    "            'LSTM': LSTMDMHaloMapper(input_size, hidden_size, output_size),\n",
    "            'GRU': GRUDMHaloMapper(input_size, hidden_size, output_size)\n",
    "        }\n",
    "        \n",
    "        '''models = {\n",
    "            'RNN': RNNDMHaloMapper(input_size, hidden_size, output_size)\n",
    "        }'''\n",
    "        \n",
    "        model_results = {}\n",
    "    \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name} model:\")\n",
    "            optimizer = tco.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            \n",
    "            criterion = tcn.MSELoss()\n",
    "        \n",
    "            # Pass tqdm into train_model if it has an internal loop\n",
    "            history, model_dir = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, name, save_dir)\n",
    "\n",
    "\n",
    "            #sanitized_model_name = re.sub(r'[<>:\"/\\\\|?*\\n]', '_', str(model))\n",
    "\n",
    "            # Create directories for saving results\n",
    "            #model_dir = os.path.join(save_dir, f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "            #os.makedirs(model_dir, exist_ok=True)\n",
    "            # Paths\n",
    "            best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "            train_history_path = os.path.join(model_dir, f\"training_history_{name}.csv\")\n",
    "            model_comparison_path = os.path.join(save_dir, \"model_comparison_results.csv\")\n",
    "            plot_path = os.path.join(model_dir, f\"dmhalo_distribution_{name}.html\")\n",
    "        \n",
    "            # Load best model weights\n",
    "            model.load_state_dict(tc.load(best_model_path))\n",
    "            model.eval()\n",
    "            \n",
    "            # Make predictions on validation set\n",
    "            with tc.no_grad():\n",
    "                val_predictions = []\n",
    "                for batch_features, _ in val_loader:\n",
    "                    batch_features = batch_features.to(device)\n",
    "                    outputs = model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    val_predictions.extend(outputs.cpu().numpy())\n",
    "                \n",
    "                val_predictions = np.array(val_predictions)\n",
    "            \n",
    "            if isinstance(X_val, pd.DataFrame):\n",
    "                # Convert DataFrame to tensor\n",
    "                X_val_tensor = tc.tensor(X_val.values, dtype=tc.float32)\n",
    "            else:\n",
    "                # Directly convert to tensor if it's already NumPy or similar\n",
    "                X_val_tensor = tc.tensor(X_val, dtype=tc.float32)\n",
    "\n",
    "            # Map feature names to indices\n",
    "            column_indices = [selected_features.index(f) for f in \n",
    "                [\n",
    "                    'GroupCentreOfPotential_x',\n",
    "                    'GroupCentreOfPotential_y',\n",
    "                    'GroupCentreOfPotential_z'\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            # Slice the tensor\n",
    "            spatial_coords = X_val_tensor[:, column_indices]\n",
    "                        \n",
    "            # Create visualization\n",
    "            fig = visualize_dm_distribution(\n",
    "                predictions=val_predictions,\n",
    "                true_values=y_val,\n",
    "                spatial_coords=spatial_coords,\n",
    "                redshifts=redshift_val,  # Add redshift values here\n",
    "                save_path=plot_path\n",
    "                #a=1.0, b=0.8, c=0.6  # Triaxial parameters\n",
    "            )\n",
    "            \n",
    "            '''# Analyze triaxial fit\n",
    "            fit_stats = analyze_triaxial_fit(\n",
    "                    spatial_coords=spatial_coords,\n",
    "                    predictions=val_predictions,\n",
    "                    save_dir=model_dir\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nTriaxial Fit Statistics for {name}:\")\n",
    "            print(f\"Axis Ratios (b/a, c/a): {fit_stats['axis_ratios'][1]:.3f}, {fit_stats['axis_ratios'][2]:.3f}\")\n",
    "            print(f\"Percentage of halos inside model: {fit_stats['percent_inside']:.2f}%\")'''\n",
    "            \n",
    "            # Additional analysis specific to redshift evolution\n",
    "            analyze_redshift_evolution(\n",
    "                predictions=val_predictions,\n",
    "                true_values=y_val,\n",
    "                redshifts=redshift_val,\n",
    "                save_dir=model_dir,\n",
    "                model_name=name\n",
    "            )\n",
    "            \n",
    "            # Calculate final metrics\n",
    "            metrics = CustomMetrics.calculate_all_metrics(\n",
    "                tc.tensor(y_val),\n",
    "                tc.tensor(val_predictions)\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            model_results[name] = {\n",
    "                'history': history,\n",
    "                'metrics': metrics,\n",
    "                'predictions': val_predictions\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nFinal metrics for {name} model:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "            # Compare models and save results\n",
    "            results_df = pd.DataFrame({\n",
    "                            name: {\n",
    "                                'MSE': results.get('metrics', {}).get('mse', np.nan),\n",
    "                                #'MSE': model_results['metrics']['mse']\n",
    "                                \n",
    "                                'RMSE': results.get('metrics', {}).get('rmse', np.nan),\n",
    "                                #'RMSE': model_results['metrics']['rmse']\n",
    "                                \n",
    "                                'MAE': results.get('metrics', {}).get('mae', np.nan),\n",
    "                                #'MAE': model_results['metrics']['mae']\n",
    "\n",
    "                                'R2': results.get('metrics', {}).get('r2', np.nan),\n",
    "                                #'R2': model_results['metrics']['r2']\n",
    "\n",
    "                                'GaussianNLL': results.get('metrics', {}).get('gaussian_nll', np.nan),\n",
    "                                #'GaussianNLL': model_results['metrics']['gaussian_nll']\n",
    "\n",
    "                                'PoissonNLL': results.get('metrics', {}).get('poisson_nll', np.nan),\n",
    "                                #'PoissonNLL': model_results['metrics']['poisson_nll']\n",
    "\n",
    "                            }\n",
    "                            for name, results in model_results.items()\n",
    "                        })\n",
    "\n",
    "            \n",
    "            results_df.to_csv(model_comparison_path)\n",
    "            print(f\"\\nModel comparison results saved to {model_comparison_path}\")\n",
    "\n",
    "            \n",
    "            # Save training histories\n",
    "            for name, results in model_results.items():\n",
    "                history_df = pd.DataFrame({\n",
    "                    'train_loss': results['history']['train_loss'],\n",
    "                    'val_loss': results['history']['val_loss']\n",
    "                })\n",
    "                history_df.to_csv(train_history_path)\n",
    "                print(f\"Training history for {name} saved to {train_history_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        raise\n",
    "            \n",
    "        '''visualize_dm_distribution(\n",
    "            predictions=model(X_val_tensor).detach().cpu().numpy(),\n",
    "            true_values=y_val,\n",
    "            save_path=f'../plots/dmhalo_vis/{name}_halo_distribution.html'\n",
    "        )'''\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requqira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
