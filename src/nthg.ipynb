{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10de0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284d3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_436946/3252503702.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_fof = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.csv', skiprows=28)\n",
      "/tmp/ipykernel_436946/3252503702.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_subhalo = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.csv', skiprows=37)\n"
     ]
    }
   ],
   "source": [
    "data_fof = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.csv', skiprows=28)\n",
    "data_subhalo = pd.read_csv('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.csv', skiprows=37)  \n",
    "data_subhalo['GalaxyID'] = data_subhalo['GalaxyID'].astype(str)\n",
    "data_fof['GroupID'] = data_subhalo['GalaxyID'].astype(str)\n",
    "\n",
    "data_subhalo.to_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.feather')\n",
    "data_fof.to_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fof = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.feather')  \n",
    "data_fof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subhalo = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.feather')  \n",
    "data_subhalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c752d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_fof \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_FoF.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data_subhalo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mgpfs/home/sarmantara/dmmapper/data/L0100N1504_Subhalo.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m data_fof\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/requqira/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "same_features = set(data_fof.columns).intersection(set(data_subhalo.columns))\n",
    "same_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fof_unique = data_fof.drop(columns=same_features)\n",
    "fof_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2da83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subhalo_unique = data_subhalo.drop(columns=same_features)\n",
    "subhalo_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3db738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1: Index(['GroupID', 'SnapNum', 'Redshift', 'RandomNumber', 'GroupMass',\n",
      "       'GroupCentreOfPotential_x', 'GroupCentreOfPotential_y',\n",
      "       'GroupCentreOfPotential_z', 'NumOfSubhalos', 'Group_M_Crit200',\n",
      "       'Group_R_Crit200', 'Group_M_Mean200', 'Group_R_Mean200',\n",
      "       'Group_M_TopHat200', 'Group_R_TopHat200', 'Group_M_Crit500',\n",
      "       'Group_R_Crit500', 'Group_M_Mean500', 'Group_R_Mean500',\n",
      "       'Group_M_Crit2500', 'Group_R_Crit2500', 'Group_R_Mean2500',\n",
      "       'Group_M_Mean2500', 'GalaxyID', 'DescendantID', 'LastProgID',\n",
      "       'TopLeafID', 'GroupNumber', 'SubGroupNumber', 'CentreOfMass_x',\n",
      "       'CentreOfMass_y', 'CentreOfMass_z', 'CentreOfPotential_x',\n",
      "       'CentreOfPotential_y', 'CentreOfPotential_z', 'Velocity_x',\n",
      "       'Velocity_y', 'Velocity_z', 'KineticEnergy', 'MechanicalEnergy',\n",
      "       'TotalEnergy', 'Vmax', 'VmaxRadius', 'Mass', 'MassType_DM',\n",
      "       'HalfMassProjRad_DM', 'HalfMassRad_DM', 'nodeIndex', 'PosInFile',\n",
      "       'FileNum', 'Image_ID'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1236622 entries, 0 to 1236621\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Non-Null Count    Dtype  \n",
      "---  ------                    --------------    -----  \n",
      " 0   GroupID                   1084535 non-null  float64\n",
      " 1   SnapNum                   1084535 non-null  float64\n",
      " 2   Redshift                  1084535 non-null  float64\n",
      " 3   RandomNumber              1084535 non-null  float64\n",
      " 4   GroupMass                 1084535 non-null  float64\n",
      " 5   GroupCentreOfPotential_x  1084535 non-null  float64\n",
      " 6   GroupCentreOfPotential_y  1084535 non-null  float64\n",
      " 7   GroupCentreOfPotential_z  1084535 non-null  float64\n",
      " 8   NumOfSubhalos             1084535 non-null  float64\n",
      " 9   Group_M_Crit200           1084535 non-null  float64\n",
      " 10  Group_R_Crit200           1084535 non-null  float64\n",
      " 11  Group_M_Mean200           1084535 non-null  float64\n",
      " 12  Group_R_Mean200           1084535 non-null  float64\n",
      " 13  Group_M_TopHat200         1084535 non-null  float64\n",
      " 14  Group_R_TopHat200         1084535 non-null  float64\n",
      " 15  Group_M_Crit500           1084535 non-null  float64\n",
      " 16  Group_R_Crit500           1084535 non-null  float64\n",
      " 17  Group_M_Mean500           1084535 non-null  float64\n",
      " 18  Group_R_Mean500           1084535 non-null  float64\n",
      " 19  Group_M_Crit2500          1084535 non-null  float64\n",
      " 20  Group_R_Crit2500          1084535 non-null  float64\n",
      " 21  Group_R_Mean2500          1084535 non-null  float64\n",
      " 22  Group_M_Mean2500          1084535 non-null  float64\n",
      " 23  GalaxyID                  1236622 non-null  int64  \n",
      " 24  DescendantID              1236622 non-null  int64  \n",
      " 25  LastProgID                1236622 non-null  int64  \n",
      " 26  TopLeafID                 1236622 non-null  int64  \n",
      " 27  GroupNumber               1236622 non-null  int64  \n",
      " 28  SubGroupNumber            1236622 non-null  int64  \n",
      " 29  CentreOfMass_x            1236622 non-null  float64\n",
      " 30  CentreOfMass_y            1236622 non-null  float64\n",
      " 31  CentreOfMass_z            1236622 non-null  float64\n",
      " 32  CentreOfPotential_x       1236622 non-null  float64\n",
      " 33  CentreOfPotential_y       1236622 non-null  float64\n",
      " 34  CentreOfPotential_z       1236622 non-null  float64\n",
      " 35  Velocity_x                1236622 non-null  float64\n",
      " 36  Velocity_y                1236622 non-null  float64\n",
      " 37  Velocity_z                1236622 non-null  float64\n",
      " 38  KineticEnergy             1236622 non-null  float64\n",
      " 39  MechanicalEnergy          1236622 non-null  float64\n",
      " 40  TotalEnergy               1236622 non-null  float64\n",
      " 41  Vmax                      1236622 non-null  float64\n",
      " 42  VmaxRadius                1236622 non-null  float64\n",
      " 43  Mass                      1236622 non-null  float64\n",
      " 44  MassType_DM               1236622 non-null  float64\n",
      " 45  HalfMassProjRad_DM        1236622 non-null  float64\n",
      " 46  HalfMassRad_DM            1236622 non-null  float64\n",
      " 47  nodeIndex                 1236622 non-null  int64  \n",
      " 48  PosInFile                 1236622 non-null  int64  \n",
      " 49  FileNum                   1236622 non-null  int64  \n",
      " 50  Image_ID                  1236622 non-null  int64  \n",
      "dtypes: float64(41), int64(10)\n",
      "memory usage: 481.2 MB\n",
      "None\n",
      "\n",
      "Data 2: Index(['GroupID', 'SnapNum', 'Redshift', 'RandomNumber', 'GroupMass',\n",
      "       'GroupCentreOfPotential_x', 'GroupCentreOfPotential_y',\n",
      "       'GroupCentreOfPotential_z', 'NumOfSubhalos', 'Group_M_Crit200',\n",
      "       'Group_R_Crit200', 'Group_M_Mean200', 'Group_R_Mean200',\n",
      "       'Group_M_TopHat200', 'Group_R_TopHat200', 'Group_M_Crit500',\n",
      "       'Group_R_Crit500', 'Group_M_Mean500', 'Group_R_Mean500',\n",
      "       'Group_M_Crit2500', 'Group_R_Crit2500', 'Group_R_Mean2500',\n",
      "       'Group_M_Mean2500', 'GalaxyID', 'DescendantID', 'LastProgID',\n",
      "       'TopLeafID', 'GroupNumber', 'SubGroupNumber', 'CentreOfMass_x',\n",
      "       'CentreOfMass_y', 'CentreOfMass_z', 'CentreOfPotential_x',\n",
      "       'CentreOfPotential_y', 'CentreOfPotential_z', 'Velocity_x',\n",
      "       'Velocity_y', 'Velocity_z', 'KineticEnergy', 'MechanicalEnergy',\n",
      "       'TotalEnergy', 'Vmax', 'VmaxRadius', 'Mass', 'MassType_DM',\n",
      "       'HalfMassProjRad_DM', 'HalfMassRad_DM', 'nodeIndex', 'PosInFile',\n",
      "       'FileNum', 'Image_ID'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8058645 entries, 0 to 8058644\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   GroupID                   int64  \n",
      " 1   SnapNum                   int64  \n",
      " 2   Redshift                  float64\n",
      " 3   RandomNumber              float64\n",
      " 4   GroupMass                 float64\n",
      " 5   GroupCentreOfPotential_x  float64\n",
      " 6   GroupCentreOfPotential_y  float64\n",
      " 7   GroupCentreOfPotential_z  float64\n",
      " 8   NumOfSubhalos             int64  \n",
      " 9   Group_M_Crit200           float64\n",
      " 10  Group_R_Crit200           float64\n",
      " 11  Group_M_Mean200           float64\n",
      " 12  Group_R_Mean200           float64\n",
      " 13  Group_M_TopHat200         float64\n",
      " 14  Group_R_TopHat200         float64\n",
      " 15  Group_M_Crit500           float64\n",
      " 16  Group_R_Crit500           float64\n",
      " 17  Group_M_Mean500           float64\n",
      " 18  Group_R_Mean500           float64\n",
      " 19  Group_M_Crit2500          float64\n",
      " 20  Group_R_Crit2500          float64\n",
      " 21  Group_R_Mean2500          float64\n",
      " 22  Group_M_Mean2500          float64\n",
      " 23  GalaxyID                  float64\n",
      " 24  DescendantID              float64\n",
      " 25  LastProgID                float64\n",
      " 26  TopLeafID                 float64\n",
      " 27  GroupNumber               float64\n",
      " 28  SubGroupNumber            float64\n",
      " 29  CentreOfMass_x            float64\n",
      " 30  CentreOfMass_y            float64\n",
      " 31  CentreOfMass_z            float64\n",
      " 32  CentreOfPotential_x       float64\n",
      " 33  CentreOfPotential_y       float64\n",
      " 34  CentreOfPotential_z       float64\n",
      " 35  Velocity_x                float64\n",
      " 36  Velocity_y                float64\n",
      " 37  Velocity_z                float64\n",
      " 38  KineticEnergy             float64\n",
      " 39  MechanicalEnergy          float64\n",
      " 40  TotalEnergy               float64\n",
      " 41  Vmax                      float64\n",
      " 42  VmaxRadius                float64\n",
      " 43  Mass                      float64\n",
      " 44  MassType_DM               float64\n",
      " 45  HalfMassProjRad_DM        float64\n",
      " 46  HalfMassRad_DM            float64\n",
      " 47  nodeIndex                 float64\n",
      " 48  PosInFile                 float64\n",
      " 49  FileNum                   float64\n",
      " 50  Image_ID                  float64\n",
      "dtypes: float64(48), int64(3)\n",
      "memory usage: 3.1 GB\n",
      "None\n",
      "\n",
      "Data 3: Index(['GroupID', 'SnapNum', 'Redshift', 'RandomNumber', 'GroupMass',\n",
      "       'GroupCentreOfPotential_x', 'GroupCentreOfPotential_y',\n",
      "       'GroupCentreOfPotential_z', 'NumOfSubhalos', 'Group_M_Crit200',\n",
      "       'Group_R_Crit200', 'Group_M_Mean200', 'Group_R_Mean200',\n",
      "       'Group_M_TopHat200', 'Group_R_TopHat200', 'Group_M_Crit500',\n",
      "       'Group_R_Crit500', 'Group_M_Mean500', 'Group_R_Mean500',\n",
      "       'Group_M_Crit2500', 'Group_R_Crit2500', 'Group_R_Mean2500',\n",
      "       'Group_M_Mean2500', 'GalaxyID', 'DescendantID', 'LastProgID',\n",
      "       'TopLeafID', 'GroupNumber', 'SubGroupNumber', 'CentreOfMass_x',\n",
      "       'CentreOfMass_y', 'CentreOfMass_z', 'CentreOfPotential_x',\n",
      "       'CentreOfPotential_y', 'CentreOfPotential_z', 'Velocity_x',\n",
      "       'Velocity_y', 'Velocity_z', 'KineticEnergy', 'MechanicalEnergy',\n",
      "       'TotalEnergy', 'Vmax', 'VmaxRadius', 'Mass', 'MassType_DM',\n",
      "       'HalfMassProjRad_DM', 'HalfMassRad_DM', 'nodeIndex', 'PosInFile',\n",
      "       'FileNum', 'Image_ID'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22109109 entries, 0 to 22109108\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   GroupID                   int64  \n",
      " 1   SnapNum                   float64\n",
      " 2   Redshift                  float64\n",
      " 3   RandomNumber              float64\n",
      " 4   GroupMass                 float64\n",
      " 5   GroupCentreOfPotential_x  float64\n",
      " 6   GroupCentreOfPotential_y  float64\n",
      " 7   GroupCentreOfPotential_z  float64\n",
      " 8   NumOfSubhalos             float64\n",
      " 9   Group_M_Crit200           float64\n",
      " 10  Group_R_Crit200           float64\n",
      " 11  Group_M_Mean200           float64\n",
      " 12  Group_R_Mean200           float64\n",
      " 13  Group_M_TopHat200         float64\n",
      " 14  Group_R_TopHat200         float64\n",
      " 15  Group_M_Crit500           float64\n",
      " 16  Group_R_Crit500           float64\n",
      " 17  Group_M_Mean500           float64\n",
      " 18  Group_R_Mean500           float64\n",
      " 19  Group_M_Crit2500          float64\n",
      " 20  Group_R_Crit2500          float64\n",
      " 21  Group_R_Mean2500          float64\n",
      " 22  Group_M_Mean2500          float64\n",
      " 23  GalaxyID                  int64  \n",
      " 24  DescendantID              float64\n",
      " 25  LastProgID                float64\n",
      " 26  TopLeafID                 float64\n",
      " 27  GroupNumber               float64\n",
      " 28  SubGroupNumber            float64\n",
      " 29  CentreOfMass_x            float64\n",
      " 30  CentreOfMass_y            float64\n",
      " 31  CentreOfMass_z            float64\n",
      " 32  CentreOfPotential_x       float64\n",
      " 33  CentreOfPotential_y       float64\n",
      " 34  CentreOfPotential_z       float64\n",
      " 35  Velocity_x                float64\n",
      " 36  Velocity_y                float64\n",
      " 37  Velocity_z                float64\n",
      " 38  KineticEnergy             float64\n",
      " 39  MechanicalEnergy          float64\n",
      " 40  TotalEnergy               float64\n",
      " 41  Vmax                      float64\n",
      " 42  VmaxRadius                float64\n",
      " 43  Mass                      float64\n",
      " 44  MassType_DM               float64\n",
      " 45  HalfMassProjRad_DM        float64\n",
      " 46  HalfMassRad_DM            float64\n",
      " 47  nodeIndex                 float64\n",
      " 48  PosInFile                 float64\n",
      " 49  FileNum                   float64\n",
      " 50  Image_ID                  float64\n",
      "dtypes: float64(49), int64(2)\n",
      "memory usage: 8.4 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_1 = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/merge_L0025N0376.feather')\n",
    "data_2 = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/merge_L0025N0752.feather')  \n",
    "data_3 = pd.read_feather('/mgpfs/home/sarmantara/dmmapper/data/merge_L0100N1504.feather')\n",
    "\n",
    "print(\"Data 1:\", data_1.columns)\n",
    "print(data_1.info())\n",
    "print(\"\\nData 2:\", data_2.columns)\n",
    "print(data_2.info())\n",
    "print(\"\\nData 3:\", data_3.columns)\n",
    "print(data_3.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns with all NaNs:\", [col for col in numeric_cols if self.data[col].isna().all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4121e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimized PyTorch Training Code\n",
    "- Improved data loading\n",
    "- Enhanced model architecture\n",
    "- Optimized training loop\n",
    "\"\"\"\n",
    "\n",
    "################ MODEL OPTIMIZATION ################\n",
    "\n",
    "class DMHaloMatterRNN(tcn.Module):\n",
    "    def __init__(self, \n",
    "                input_size=None, \n",
    "                hidden_size=None, \n",
    "                output_size=None, \n",
    "                num_layers=2, \n",
    "                dropout_rate=0.2,\n",
    "                activation='gelu', \n",
    "                bidirectional=False, \n",
    "                feature_dim=None,\n",
    "                use_layer_norm=False, \n",
    "                return_hidden=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size if input_size is not None else 64\n",
    "        self.hidden_size = hidden_size if hidden_size is not None else 128\n",
    "        self.output_size = output_size if output_size is not None else 1\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.feature_dim = feature_dim or min(self.hidden_size, self.input_size)\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.return_hidden = return_hidden\n",
    "        \n",
    "        # Get activation function\n",
    "        self.activation = self.__get_activation_layer(activation)\n",
    "        \n",
    "        # Pre-compute bidirectional factor\n",
    "        self.output_factor = 2 if self.bidirectional else 1\n",
    "        \n",
    "        # Streamlined feature extraction\n",
    "        self.feature_layer = tcn.Linear(self.input_size, self.feature_dim)\n",
    "        \n",
    "        # Optional batch normalization - don't create if not needed\n",
    "        self.batch_norm = tcn.BatchNorm1d(self.feature_dim)\n",
    "        \n",
    "        # Simplified output projection\n",
    "        intermediate_size = max(4, self.hidden_size // 2)\n",
    "        self.output_proj1 = tcn.Linear(self.hidden_size * self.output_factor, intermediate_size)\n",
    "        self.output_proj2 = tcn.Linear(intermediate_size, self.output_size)\n",
    "        \n",
    "        # Optional layer normalization\n",
    "        if self.use_layer_norm:\n",
    "            self.norm_after_rnn = tcn.LayerNorm(self.hidden_size * self.output_factor)\n",
    "    \n",
    "    def __get_activation_layer(self, activation):\n",
    "        activation_name = activation.lower()\n",
    "        \n",
    "        # Simplified activation map with most common activations\n",
    "        activation_map = {\n",
    "            'relu': tcn.ReLU(inplace=True),\n",
    "            'leaky_relu': tcn.LeakyReLU(inplace=True),\n",
    "            'gelu': tcn.GELU(),\n",
    "            'swish': tcn.SiLU(inplace=True),\n",
    "            'silu': tcn.SiLU(inplace=True),\n",
    "        }\n",
    "        \n",
    "        return activation_map.get(activation_name, tcn.GELU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass (not implemented in base class).\"\"\"\n",
    "        raise NotImplementedError(\"Forward is Not Implemented\")\n",
    "    \n",
    "    @tc.jit.ignore\n",
    "    def process_input(self, x):\n",
    "        \"\"\"Optimized input processing\"\"\"\n",
    "        # Handle 2D (single step) vs 3D (sequence) input\n",
    "        is_sequence = x.dim() == 3\n",
    "        \n",
    "        if is_sequence:\n",
    "            # Batch processing for sequences\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            x = x.reshape(-1, self.input_size)\n",
    "            x = self.feature_layer(x)\n",
    "            x = self.activation(x)\n",
    "            \n",
    "            # Apply batch norm efficiently\n",
    "            x = self.batch_norm(x)\n",
    "            \n",
    "            # Return to sequence form\n",
    "            return x.reshape(batch_size, seq_len, self.feature_dim)\n",
    "        else:\n",
    "            # Process single step\n",
    "            x = self.feature_layer(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.batch_norm(x)\n",
    "            return x.unsqueeze(1)  # Add sequence dimension\n",
    "    \n",
    "    def apply_weight_init(self):\n",
    "        \"\"\"Optimized weight initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, tcn.Linear):\n",
    "                tcn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    tcn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, tcn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        tcn.init.xavier_uniform_(param)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        tcn.init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        tcn.init.zeros_(param)\n",
    "\n",
    "\n",
    "class GRUDMHaloMapper(DMHaloMatterRNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create GRU layer\n",
    "        self.gru = tcn.GRU(\n",
    "            self.feature_dim,\n",
    "            self.hidden_size,\n",
    "            self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_rate if self.num_layers > 1 else 0,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # Apply optimized weight initialization\n",
    "        self.apply_weight_init()\n",
    "        \n",
    "        # Enable cudnn benchmarking for optimal kernels\n",
    "        if tc.cuda.is_available():\n",
    "            tc.backends.cudnn.benchmark = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input\n",
    "        x = self.process_input(x)\n",
    "        \n",
    "        # Run through GRU\n",
    "        features, h_n = self.gru(x)\n",
    "        \n",
    "        # Get only the last output efficiently\n",
    "        features = features[:, -1]\n",
    "        \n",
    "        # Apply layer normalization if needed\n",
    "        if self.use_layer_norm:\n",
    "            features = self.norm_after_rnn(features)\n",
    "        \n",
    "        # Apply functional dropout during training\n",
    "        if self.training:\n",
    "            features = F.dropout(features, p=self.dropout_rate, training=True, inplace=True)\n",
    "        \n",
    "        # Simplified output projection\n",
    "        features = self.output_proj1(features)\n",
    "        features = self.activation(features)\n",
    "        \n",
    "        # Final output projection\n",
    "        output = self.output_proj2(features)\n",
    "        \n",
    "        return (output, h_n) if self.return_hidden else output\n",
    "\n",
    "\n",
    "################ TRAINER OPTIMIZATION ################\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model=None, criterion=None, optimizer=None, patience=10, device=None, save_dir=None):\n",
    "        # Model setup\n",
    "        self.model = model if model is not None else RNNDMHaloMapper()\n",
    "        self.device = device if device else ('cuda' if tc.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Criterion and optimizer with better defaults\n",
    "        self.criterion = criterion or tc.nn.MSELoss()\n",
    "        self.optimizer = optimizer or tc.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=0.001, \n",
    "            weight_decay=0.0001,\n",
    "            eps=1e-8\n",
    "        )\n",
    "\n",
    "        # Save and tracking\n",
    "        self.save_dir = save_dir\n",
    "        self.metrics_calculator = CustomMetrics()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_metrics': [],\n",
    "            'val_metrics': []\n",
    "        }\n",
    "\n",
    "        # Configure for performance\n",
    "        self.__configure_for_performance()\n",
    "\n",
    "    def __configure_for_performance(self):\n",
    "        \"\"\"Configure settings for optimal performance\"\"\"\n",
    "        # Data loaders\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "\n",
    "        # Mixed precision settings\n",
    "        self.use_amp = self.device == 'cuda'\n",
    "        if self.use_amp:\n",
    "            self.scaler = GradScaler()\n",
    "        \n",
    "        # Optimize settings for device\n",
    "        if self.device == 'cuda':\n",
    "            # Set optimal CUDA settings\n",
    "            tc.backends.cudnn.benchmark = True\n",
    "            tc.backends.cudnn.deterministic = False\n",
    "            # Increase batch size for GPU\n",
    "            self.log_interval = 25\n",
    "            self.accumulation_steps = 1\n",
    "        else:\n",
    "            # CPU settings\n",
    "            self.log_interval = 100\n",
    "            self.accumulation_steps = 1\n",
    "        \n",
    "        # Use JIT compilation for CPU if possible\n",
    "        if self.device == 'cpu' and hasattr(tc, 'compile'):\n",
    "            try:\n",
    "                self.model = tc.compile(self.model)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def __optimize_batch_parameters(self):\n",
    "        \"\"\"Optimize batch processing parameters\"\"\"\n",
    "        # Auto-tune batching parameters based on the device and model\n",
    "        if self.device == 'cuda':\n",
    "            # Find optimal batch size based on model size\n",
    "            param_count = sum(p.numel() for p in self.model.parameters())\n",
    "            gpu_mem = tc.cuda.get_device_properties(0).total_memory\n",
    "            \n",
    "            if param_count < 1e6:  # Small model\n",
    "                self.log_interval = 10\n",
    "            elif param_count < 1e7:  # Medium model\n",
    "                self.log_interval = 25\n",
    "            else:  # Large model\n",
    "                self.log_interval = 50\n",
    "                \n",
    "            # Calculate optimal number of workers based on CPU cores\n",
    "            self.train_loader.num_workers = min(8, os.cpu_count() or 4)\n",
    "            \n",
    "            # Set pinned memory for faster data transfer\n",
    "            if hasattr(self.train_loader, 'pin_memory'):\n",
    "                self.train_loader.pin_memory = True\n",
    "                \n",
    "        else:  # CPU optimizations\n",
    "            self.log_interval = 100\n",
    "            if hasattr(self.train_loader, 'num_workers'):\n",
    "                self.train_loader.num_workers = 0  # Avoid overhead on CPU\n",
    "\n",
    "    def __train_epoch(self):\n",
    "        \"\"\"Optimized training epoch\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Simplified metrics tracking\n",
    "        epoch_metrics = {\n",
    "            'mse': 0, 'rmse': 0, 'mae': 0, 'r2': 0\n",
    "        }\n",
    "        \n",
    "        # Gradient tracking\n",
    "        grad_norms = []\n",
    "        \n",
    "        # Streamlined progress bar\n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        \n",
    "        # Track if metrics need update\n",
    "        metrics_update_counter = 0\n",
    "        \n",
    "        for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "            try:\n",
    "                # Move data to device efficiently\n",
    "                batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision training\n",
    "                if self.use_amp:\n",
    "                    with autocast(device_type=self.device):\n",
    "                        outputs = self.model(batch_features)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        loss = self.criterion(outputs, batch_targets)\n",
    "                        loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Scale gradients\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights with scaler\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)  # More efficient\n",
    "                else:\n",
    "                    # Standard CPU training\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Optional gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Accumulate loss for reporting (unscaled)\n",
    "                train_loss += loss.item() * self.accumulation_steps\n",
    "                \n",
    "                # Update metrics less frequently\n",
    "                if batch_idx % self.log_interval == 0:\n",
    "                    metrics_update_counter += 1\n",
    "                    with tc.no_grad():\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets.detach(), outputs.detach()\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    if batch_idx > 0:\n",
    "                        pbar.set_postfix({'loss': f\"{loss.item() * self.accumulation_steps:.5f}\"})\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                logger.error(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = train_loss / len(self.train_loader)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics, grad_norms\n",
    "\n",
    "    def __validate_epoch(self):\n",
    "        \"\"\"Optimized validation epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Simplified metrics tracking\n",
    "        epoch_metrics = {\n",
    "            'mse': 0, 'rmse': 0, 'mae': 0, 'r2': 0\n",
    "        }\n",
    "        \n",
    "        # Metrics update tracking\n",
    "        metrics_update_counter = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "        \n",
    "        with tc.no_grad():\n",
    "            for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    # Move data to device efficiently\n",
    "                    batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                    batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                    \n",
    "                    # Forward pass (simpler without autocast for validation)\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    \n",
    "                    # Track loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    # Update metrics less frequently\n",
    "                    if batch_idx % self.log_interval == 0:\n",
    "                        metrics_update_counter += 1\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets, outputs\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        if batch_idx > 0:\n",
    "                            pbar.set_postfix({'loss': f\"{loss.item():.5f}\"})\n",
    "                \n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = val_loss / max(1, val_batches)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics\n",
    "\n",
    "    def __create_save_directory(self, model_name):\n",
    "        \"\"\"Create directory for saving model artifacts\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(self.save_dir, f\"{model_name}_{timestamp}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        best_model_path = os.path.join(model_dir, f\"best_{model_name}.pt\")\n",
    "        train_history_path = os.path.join(model_dir, f\"{model_name}_history.npy\")\n",
    "        train_history_file = os.path.join(model_dir, f\"{model_name}_history.csv\")\n",
    "        \n",
    "        return {\n",
    "            'model_dir': model_dir,\n",
    "            'best_model_path': best_model_path,\n",
    "            'train_history_path': train_history_path,\n",
    "            'train_history_file': train_history_file\n",
    "        }\n",
    "\n",
    "    def __save_model_checkpoint(self, path):\n",
    "        \"\"\"Save model checkpoint efficiently\"\"\"\n",
    "        # Use torch.save with _use_new_zipfile_serialization=True for faster saving\n",
    "        tc.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path, _use_new_zipfile_serialization=True)\n",
    "\n",
    "    def __save_training_progress(self, epoch, train_loss, val_loss, history_file):\n",
    "        \"\"\"Save training progress to CSV file\"\"\"\n",
    "        if epoch == 0:\n",
    "            with open(history_file, 'w') as f:\n",
    "                f.write('epoch,train_loss,val_loss\\n')\n",
    "        \n",
    "        with open(history_file, 'a') as f:\n",
    "            f.write(f'{epoch},{train_loss},{val_loss}\\n')\n",
    "\n",
    "    def __log_metrics(self, train_metrics, val_metrics):\n",
    "        \"\"\"Log metrics efficiently\"\"\"\n",
    "        # Only log essential metrics\n",
    "        essential_metrics = ['mse', 'rmse', 'r2']\n",
    "        \n",
    "        train_line = \"  \".join([f\"{m}: {train_metrics[m]:.5E}\" for m in essential_metrics if m in train_metrics])\n",
    "        val_line = \"  \".join([f\"{m}: {val_metrics[m]:.5E}\" for m in essential_metrics if m in val_metrics])\n",
    "        \n",
    "        logger.info(f\"Train: {train_line}\")\n",
    "        logger.info(f\"Valid: {val_line}\")\n",
    "\n",
    "    def set_loaders(self, X, y):\n",
    "        \"\"\"Set up optimized data loaders\"\"\"\n",
    "        # Create dataset and split\n",
    "        dataset = TensorDataset(tc.FloatTensor(X), tc.FloatTensor(y))\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create optimized data loaders\n",
    "        num_workers = min(8, os.cpu_count() or 4) if self.device == 'cuda' else 0\n",
    "        pin_memory = self.device == 'cuda'\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=512, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=1024,  # Larger batches for validation\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "\n",
    "    def train(self, num_epochs, model_name, train_loader=None, val_loader=None, X=None, y=None):\n",
    "        \"\"\"Optimized training function\"\"\"\n",
    "        # Set up data loaders\n",
    "        if train_loader and val_loader:\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "        else:\n",
    "            self.set_loaders(X, y)\n",
    "        \n",
    "        # Create save directory\n",
    "        save_paths = self.__create_save_directory(model_name)\n",
    "        \n",
    "        # Initialize learning rate scheduler with better defaults\n",
    "        scheduler = tc.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Slightly more patience\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training variables\n",
    "        grad_norms = []\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Optimize batch parameters\n",
    "        self.__optimize_batch_parameters()\n",
    "        \n",
    "        # Track time and throughput\n",
    "        start_time = time.time()\n",
    "        samples_processed = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{num_epochs}] | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_metrics, epoch_grad_norms = self.__train_epoch()\n",
    "            grad_norms.extend(epoch_grad_norms)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_metrics = self.__validate_epoch()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Check for LR changes\n",
    "            new_lr = self.optimizer.param_groups[0]['lr']\n",
    "            if new_lr != current_lr:\n",
    "                print(f\"Learning rate updated: {current_lr:.2e} → {new_lr:.2e}\")\n",
    "                current_lr = new_lr\n",
    "            \n",
    "            # Save training progress\n",
    "            self.__save_training_progress(epoch, train_loss, val_loss, save_paths['train_history_file'])\n",
    "            \n",
    "            # Track gradient statistics\n",
    "            if grad_norms:\n",
    "                avg_grad_norm = sum(grad_norms[-min(len(grad_norms), len(self.train_loader)):]) / min(len(grad_norms), len(self.train_loader))\n",
    "                self.history.setdefault('grad_norms', []).append(avg_grad_norm)\n",
    "            \n",
    "            # Track samples for throughput calculation\n",
    "            samples_processed += len(self.train_loader) * self.train_loader.batch_size\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.__save_model_checkpoint(save_paths['best_model_path'])\n",
    "                print(f\"New best model saved (Val Loss: {val_loss:.5E})\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_metrics'].append(train_metrics)\n",
    "            self.history['val_metrics'].append(val_metrics)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            samples_per_second = len(self.train_loader) * self.train_loader.batch_size / epoch_time\n",
    "            \n",
    "            # Print summary\n",
    "            epoch_summary = (\n",
    "                f\"Train Loss: {train_loss:.5E}, Val Loss: {val_loss:.5E}, \"\n",
    "                f\"Throughput: {samples_per_second:.1f} it/s, \"\n",
    "                f\"Time: {epoch_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "            print(epoch_summary)\n",
    "            \n",
    "            # Log metrics occasionally\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.__log_metrics(train_metrics, val_metrics)\n",
    "        \n",
    "        # Final performance summary\n",
    "        total_time = time.time() - start_time\n",
    "        avg_throughput = samples_processed / total_time\n",
    "        print(f\"Training complete. Total time: {total_time:.2f}s, Avg throughput: {avg_throughput:.1f} it/s\")\n",
    "        \n",
    "        # Save final history\n",
    "        np.save(save_paths['train_history_path'], self.history)\n",
    "        \n",
    "        return self.history, save_paths['model_dir']\n",
    "\n",
    "\n",
    "################ HYPERPARAMETER OPTIMIZATION ################\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    \"\"\"Optimized hyperparameters based on target hardware\"\"\"\n",
    "    # Detect device capabilities\n",
    "    device = 'cuda' if tc.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Get system info\n",
    "    cpu_count = os.cpu_count() or 4\n",
    "    \n",
    "    # Optimize based on hardware\n",
    "    if device == 'cuda':\n",
    "        # GPU configuration\n",
    "        gpu_mem = tc.cuda.get_device_properties(0).total_memory\n",
    "        gpu_mem_gb = gpu_mem / (1024**3)\n",
    "        \n",
    "        # Scale batch size based on GPU memory\n",
    "        if gpu_mem_gb > 16:  # High-end GPU\n",
    "            batch_size = 2048\n",
    "            hidden_size = 128\n",
    "        elif gpu_mem_gb > 8:  # Mid-range GPU\n",
    "            batch_size = 1024\n",
    "            hidden_size = 64\n",
    "        else:  # Entry-level GPU\n",
    "            batch_size = 512\n",
    "            hidden_size = 32\n",
    "        \n",
    "        # Workers based on CPU cores, with limit for GPU data transfer\n",
    "        num_workers = min(8, cpu_count)\n",
    "    else:\n",
    "        # CPU configuration\n",
    "        batch_size = 256  # Smaller batches for CPU\n",
    "        hidden_size = 32  # Smaller model for CPU\n",
    "        num_workers = 0  # Avoid worker overhead on CPU\n",
    "    \n",
    "    # Return optimized config\n",
    "    return {\n",
    "        'MODEL_TYPE': 'GRU',\n",
    "        'BATCH_SIZE': batch_size,\n",
    "        'HIDDEN_SIZE': hidden_size,\n",
    "        'NUM_WORKERS': num_workers,\n",
    "        'DEVICE': device,\n",
    "        'NUM_LAYERS': 1,  # Simpler model is often faster\n",
    "        'DROPOUT_RATE': 0.1,  # Lower dropout for speed\n",
    "        'ACTIVATION': 'swish',\n",
    "        'USE_LAYER_NORM': False,  # Skip layer norm for speed\n",
    "        'LEARNING_RATE': 0.001,\n",
    "        'WEIGHT_DECAY': 0.001,\n",
    "        'OPTIMIZER': 'adamw'  # AdamW often works better than Adam\n",
    "    }\n",
    "\n",
    "\n",
    "################ USAGE EXAMPLE ################\n",
    "\n",
    "def train_optimized_model():\n",
    "    \"\"\"Example of using the optimized code\"\"\"\n",
    "    # Get optimized hyperparameters\n",
    "    config = optimize_hyperparameters()\n",
    "    \n",
    "    # Initialize model with optimized configuration\n",
    "    model = GRUDMHaloMapper(\n",
    "        input_size=X_train.shape[1],\n",
    "        hidden_size=config['HIDDEN_SIZE'],\n",
    "        output_size=1,\n",
    "        num_layers=config['NUM_LAYERS'],\n",
    "        dropout_rate=config['DROPOUT_RATE'],\n",
    "        activation=config['ACTIVATION'],\n",
    "        bidirectional=False,\n",
    "        use_layer_norm=config['USE_LAYER_NORM'],\n",
    "        return_hidden=True\n",
    "    ).to(config['DEVICE'])\n",
    "    \n",
    "    # Create optimizer - AdamW usually better than Adam\n",
    "    optimizer = tc.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['LEARNING_RATE'],\n",
    "        weight_decay=config['WEIGHT_DECAY'],\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        criterion=tc.nn.MSELoss(),\n",
    "        optimizer=optimizer,\n",
    "        device=config['DEVICE'],\n",
    "        save_dir=path_manager.model_path\n",
    "    )\n",
    "    \n",
    "    # Create data loaders with optimized batch size\n",
    "    train_dataset = TensorDataset(tc.FloatTensor(X_train), tc.FloatTensor(y_train))\n",
    "    val_dataset = TensorDataset(tc.FloatTensor(X_val), tc.FloatTensor(y_val))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['BATCH_SIZE'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['NUM_WORKERS'],\n",
    "        pin_memory=config['DEVICE'] == 'cuda',\n",
    "        persistent_workers=config['NUM_WORKERS'] > 0,\n",
    "        prefetch_factor=2 if config['NUM_WORKERS'] > 0 else None\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['BATCH_SIZE'] * 2,  # Larger batches for validation\n",
    "        shuffle=False,\n",
    "        num_workers=config['NUM_WORKERS'],\n",
    "        pin_memory=config['DEVICE'] == 'cuda',\n",
    "        persistent_workers=config['NUM_WORKERS'] > 0,\n",
    "        prefetch_factor=2 if config['NUM_WORKERS'] > 0 else None\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history, model_dir = trainer.train(\n",
    "        num_epochs=30,\n",
    "        model_name=config['MODEL_TYPE'],\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader\n",
    "    )\n",
    "    \n",
    "    return model, history, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeeb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        metrics_update_counter = 0\n",
    "        \n",
    "        for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "            try:\n",
    "                # Move data to device efficiently\n",
    "                batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision training\n",
    "                if self.use_amp:\n",
    "                    with autocast(device_type=self.device):\n",
    "                        outputs = self.model(batch_features)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        loss = self.criterion(outputs, batch_targets)\n",
    "                        loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Scale gradients\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights with scaler\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)  # More efficient\n",
    "                else:\n",
    "                    # Standard CPU training\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    loss = loss / self.accumulation_steps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Step optimizer at accumulation boundary\n",
    "                    if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                        # Optional gradient clipping\n",
    "                        if batch_idx % self.log_interval == 0:\n",
    "                            grad_norm = tc.nn.utils.clip_grad_norm_(\n",
    "                                self.model.parameters(), max_norm=1.0\n",
    "                            )\n",
    "                            grad_norms.append(grad_norm.item())\n",
    "                        \n",
    "                        # Update weights\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Accumulate loss for reporting (unscaled)\n",
    "                train_loss += loss.item() * self.accumulation_steps\n",
    "                \n",
    "                # Update metrics less frequently\n",
    "                if batch_idx % self.log_interval == 0:\n",
    "                    metrics_update_counter += 1\n",
    "                    with tc.no_grad():\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets.detach(), outputs.detach()\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    if batch_idx > 0:\n",
    "                        pbar.set_postfix({'loss': f\"{loss.item() * self.accumulation_steps:.5f}\"})\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                logger.error(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = train_loss / len(self.train_loader)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics, grad_norms\n",
    "\n",
    "    def __validate_epoch(self):\n",
    "        \"\"\"Optimized validation epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Simplified metrics tracking\n",
    "        epoch_metrics = {\n",
    "            'mse': 0, 'rmse': 0, 'mae': 0, 'r2': 0\n",
    "        }\n",
    "        \n",
    "        # Metrics update tracking\n",
    "        metrics_update_counter = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "        \n",
    "        with tc.no_grad():\n",
    "            for batch_idx, (batch_features, batch_targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    # Move data to device efficiently\n",
    "                    batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                    batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                    \n",
    "                    # Forward pass (simpler without autocast for validation)\n",
    "                    outputs = self.model(batch_features)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = self.criterion(outputs, batch_targets)\n",
    "                    \n",
    "                    # Track loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    # Update metrics less frequently\n",
    "                    if batch_idx % self.log_interval == 0:\n",
    "                        metrics_update_counter += 1\n",
    "                        batch_metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "                            batch_targets, outputs\n",
    "                        )\n",
    "                        for metric, value in batch_metrics.items():\n",
    "                            if metric in epoch_metrics:\n",
    "                                epoch_metrics[metric] += value\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        if batch_idx > 0:\n",
    "                            pbar.set_postfix({'loss': f\"{loss.item():.5f}\"})\n",
    "                \n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        num_eval_steps = max(1, metrics_update_counter)\n",
    "        avg_loss = val_loss / max(1, val_batches)\n",
    "        avg_metrics = {metric: value / num_eval_steps for metric, value in epoch_metrics.items()}\n",
    "        \n",
    "        return avg_loss, avg_metrics\n",
    "\n",
    "    def __create_save_directory(self, model_name):\n",
    "        \"\"\"Create directory for saving model artifacts\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(self.save_dir, f\"{model_name}_{timestamp}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        best_model_path = os.path.join(model_dir, f\"best_{model_name}.pt\")\n",
    "        train_history_path = os.path.join(model_dir, f\"{model_name}_history.npy\")\n",
    "        train_history_file = os.path.join(model_dir, f\"{model_name}_history.csv\")\n",
    "        \n",
    "        return {\n",
    "            'model_dir': model_dir,\n",
    "            'best_model_path': best_model_path,\n",
    "            'train_history_path': train_history_path,\n",
    "            'train_history_file': train_history_file\n",
    "        }\n",
    "\n",
    "    def __save_model_checkpoint(self, path):\n",
    "        \"\"\"Save model checkpoint efficiently\"\"\"\n",
    "        # Use torch.save with _use_new_zipfile_serialization=True for faster saving\n",
    "        tc.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path, _use_new_zipfile_serialization=True)\n",
    "\n",
    "    def __save_training_progress(self, epoch, train_loss, val_loss, history_file):\n",
    "        \"\"\"Save training progress to CSV file\"\"\"\n",
    "        if epoch == 0:\n",
    "            with open(history_file, 'w') as f:\n",
    "                f.write('epoch,train_loss,val_loss\\n')\n",
    "        \n",
    "        with open(history_file, 'a') as f:\n",
    "            f.write(f'{epoch},{train_loss},{val_loss}\\n')\n",
    "\n",
    "    def __log_metrics(self, train_metrics, val_metrics):\n",
    "        \"\"\"Log metrics efficiently\"\"\"\n",
    "        # Only log essential metrics\n",
    "        essential_metrics = ['mse', 'rmse', 'r2']\n",
    "        \n",
    "        train_line = \"  \".join([f\"{m}: {train_metrics[m]:.5E}\" for m in essential_metrics if m in train_metrics])\n",
    "        val_line = \"  \".join([f\"{m}: {val_metrics[m]:.5E}\" for m in essential_metrics if m in val_metrics])\n",
    "        \n",
    "        logger.info(f\"Train: {train_line}\")\n",
    "        logger.info(f\"Valid: {val_line}\")\n",
    "\n",
    "    def set_loaders(self, X, y):\n",
    "        \"\"\"Set up optimized data loaders\"\"\"\n",
    "        # Create dataset and split\n",
    "        dataset = TensorDataset(tc.FloatTensor(X), tc.FloatTensor(y))\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create optimized data loaders\n",
    "        num_workers = min(8, os.cpu_count() or 4) if self.device == 'cuda' else 0\n",
    "        pin_memory = self.device == 'cuda'\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=512, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=1024,  # Larger batches for validation\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=num_workers > 0,\n",
    "            prefetch_factor=2 if num_workers > 0 else None\n",
    "        )\n",
    "\n",
    "    def train(self, num_epochs, model_name, train_loader=None, val_loader=None, X=None, y=None):\n",
    "        \"\"\"Optimized training function\"\"\"\n",
    "        # Set up data loaders\n",
    "        if train_loader and val_loader:\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "        else:\n",
    "            self.set_loaders(X, y)\n",
    "        \n",
    "        # Create save directory\n",
    "        save_paths = self.__create_save_directory(model_name)\n",
    "        \n",
    "        # Initialize learning rate scheduler with better defaults\n",
    "        scheduler = tc.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Slightly more patience\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training variables\n",
    "        grad_norms = []\n",
    "        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Optimize batch parameters\n",
    "        self.__optimize_batch_parameters()\n",
    "        \n",
    "        # Track time and throughput\n",
    "        start_time = time.time()\n",
    "        samples_processed = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{num_epochs}] | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_metrics, epoch_grad_norms = self.__train_epoch()\n",
    "            grad_norms.extend(epoch_grad_norms)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_metrics = self.__validate_epoch()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Check for LR changes\n",
    "            new_lr = self.optimizer.param_groups[0]['lr']\n",
    "            if new_lr != current_lr:\n",
    "                print(f\"Learning rate updated: {current_lr:.2e} → {new_lr:.2e}\")\n",
    "                current_lr = new_lr\n",
    "            \n",
    "            # Save training progress\n",
    "            self.__save_training_progress(epoch, train_loss, val_loss, save_paths['train_history_file'])\n",
    "            \n",
    "            # Track gradient statistics\n",
    "            if grad_norms:\n",
    "                avg_grad_norm = sum(grad_norms[-min(len(grad_norms), len(self.train_loader)):]) / min(len(grad_norms), len(self.train_loader))\n",
    "                self.history.setdefault('grad_norms', []).append(avg_grad_norm)\n",
    "            \n",
    "            # Track samples for throughput calculation\n",
    "            samples_processed += len(self.train_loader) * self.train_loader.batch_size\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.__save_model_checkpoint(save_paths['best_model_path'])\n",
    "                print(f\"New best model saved (Val Loss: {val_loss:.5E})\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_metrics'].append(train_metrics)\n",
    "            self.history['val_metrics'].append(val_metrics)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            samples_per_second = len(self.train_loader) * self.train_loader.batch_size / epoch_time\n",
    "            \n",
    "            # Print summary\n",
    "            epoch_summary = (\n",
    "                f\"Train Loss: {train_loss:.5E}, Val Loss: {val_loss:.5E}, \"\n",
    "                f\"Throughput: {samples_per_second:.1f} it/s, \"\n",
    "                f\"Time: {epoch_time:.2f}s\"\n",
    "            )\n",
    "            \n",
    "            print(epoch_summary)\n",
    "            \n",
    "            # Log metrics occasionally\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.__log_metrics(train_metrics, val_metrics)\n",
    "        \n",
    "        # Final performance summary\n",
    "        total_time = time.time() - start_time\n",
    "        avg_throughput = samples_processed / total_time\n",
    "        print(f\"Training complete. Total time: {total_time:.2f}s, Avg throughput: {avg_throughput:.1f} it/s\")\n",
    "        \n",
    "        # Save final history\n",
    "        np.save(save_paths['train_history_path'], self.history)\n",
    "        \n",
    "        return self.history, save_paths['model_dir']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requqira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
